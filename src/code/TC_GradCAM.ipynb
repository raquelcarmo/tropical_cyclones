{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TC_GradCAM.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/raquelcarmo/tropical_cyclones/blob/import-py-files/src/code/TC_GradCAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","metadata":{"id":"dj7JZ5lPCZvm"},"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AyL3l3Z1TBHj"},"source":["%cd /content/drive/My Drive/ESRIN_PhiLab/Tropical_Cyclones/tropical_cyclones/src/code\n","%ls\n","\n","import imp \n","# import helper.py\n","helper = imp.new_module('helper_functions')\n","exec(open(\"./helper_functions.py\").read(), helper.__dict__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3d95OmlLCDSj"},"source":["# getting in the directory \n","%cd /content/drive/My Drive/ESRIN_PhiLab/Tropical_Cyclones/data\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a8tbWAaHayf0"},"source":["# general imports\n","import random\n","import glob\n","import os\n","import sys\n","sys.stdout.flush()\n","import pandas as pd\n","import numpy as np\n","import cv2\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","import math\n","import imageio\n","import os.path\n","from PIL import Image\n","from mpl_toolkits.axes_grid1 import make_axes_locatable\n","from scipy import ndimage\n","from google.colab.patches import cv2_imshow\n","import random\n","from shapely.geometry import Point\n","import re\n","import pickle\n","import scipy\n","from sklearn.model_selection import StratifiedKFold\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n","import datetime\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.data import Dataset\n","from tensorflow.keras import Input\n","from tensorflow.keras.applications import resnet50, mobilenet_v2\n","from tensorflow.keras.applications.resnet50 import ResNet50\n","from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import concatenate, Dense, GlobalAveragePooling2D\n","from tensorflow.keras.optimizers import SGD, Adam\n","from tensorflow.keras.metrics import CategoricalAccuracy, Precision, Recall, TruePositives, FalsePositives, TrueNegatives, FalseNegatives\n","\n","np.set_printoptions(precision=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ArR-yXNqH0mI"},"source":["class DataProcessor():\n","  def __init__(self, model = \"ResNet\", min_height = 200, min_width = 200, normalise = True, rotate = False, \n","               plot_light = False, plot_extensive = False, show_prints = False):\n","\n","    self.model = model\n","    self.min_height = min_height\n","    self.min_width = min_width\n","    self.normalise = normalise\n","    self.rotate = rotate\n","    self.plot_light = plot_light\n","    self.plot_extensive = plot_extensive\n","    self.show_prints = show_prints\n","    self.img_name = None\n","\n","\n","  def preprocess_pipeline(self, image_path, bbox):\n","    ''' Defines the preprocess pipeline '''\n","    #image_path = image_path.numpy().decode('utf-8')\n","    #bbox = bbox.numpy()\n","    if self.show_prints:\n","      print(\"[START]: preprocess_pipeline\")\n","      print(image_path, bbox)\n","    self.img_name = os.path.basename(image_path)[:-4]\n","    #filelist = glob.glob(\"SAR_swath_masks/\" + image_path.split('/')[1] + '/' + self.img_name + \"*.npy\")\n","    #mask = np.load(filelist[0]) if filelist != [] else None\n","\n","    # read the image path\n","    im = cv2.imread(image_path).astype(np.float32) # loads images as BGR in float32\n","    image = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)   # BGR -> RGB\n","\n","    if self.plot_extensive:\n","      print(\"Before:\", image.shape)\n","      cv2_imshow(image)\n","\n","    if self.rotate:\n","      # perform rotation to the images\n","      image, bbox_rotated = self.rotation(image_path, bbox)\n","    else:\n","      # look for bounding box\n","      bbox_rotated = None if (bbox == 0).all() else bbox\n","\n","    # perform padding and then cropping to the images\n","    padded_image, bbox_padded = self.padding(image, bbox_rotated)\n","    sized_image = self.random_crop(padded_image, bbox_padded)\n","\n","    if self.normalise:\n","      # perform normalisation to the images\n","      #output_image = self.normalisation(sized_image, z_norm = False)\n","      if self.model == \"ResNet\":\n","        norm_image = resnet50.preprocess_input(sized_image)\n","      elif self.model == \"Mobile\":\n","        norm_image = mobilenet_v2.preprocess_input(sized_image)\n","      else:\n","        sys.exit(\"Incert valid network model. Options: Mobile or ResNet (case sensitive)\")\n","    else:\n","      norm_image = None\n","    # force land values to be zero\n","    #output_image[mask == 1] = 0\n","\n","    if self.show_prints:\n","      print(\"[END]: preprocess_pipeline\")\n","    # return output_image (not normalised) and normalised_output_image\n","    return tf.transpose(sized_image, [1,0,2]), tf.transpose(norm_image, [1,0,2])\n","\n","\n","  def padding(self, image, bbox):\n","    '''Takes an input image and returns a padded version of it with\n","    the required dimensions to reach self.min_height, self.min_width'''\n","    h, w = image.shape[:2]\n","    if h >= self.min_height and w >= self.min_width:\n","      return image, bbox\n","      \n","    new_values = None\n","    h1 = (self.min_height - h)//2 if (self.min_height - h)//2 > 0 else 0\n","    w1 = (self.min_width - w)//2 if (self.min_width - w)//2 > 0 else 0\n","    padded_img = cv2.copyMakeBorder(image, \n","                                    top = h1+1 if (self.min_height - (h+ 2*h1)) == 1 else h1, \n","                                    bottom = h1, \n","                                    left = w1+1 if (self.min_width - (w+ 2*w1)) == 1 else w1, \n","                                    right = w1, \n","                                    borderType = cv2.BORDER_CONSTANT,\n","                                    value = [0.0, 0.0, 0.0])\n","    if bbox is not None:\n","      x_left = (int)(bbox[0])\n","      y_top = (int)(bbox[1])\n","      bb_width = (int)(bbox[2]) \n","      bb_height = (int)(bbox[3])\n","\n","      # translate the x_left and y_top values according to the pad\n","      x_left += w1+1 if (self.min_width - (w+ 2*w1)) == 1 else w1\n","      y_top += h1+1 if (self.min_height - (h+ 2*h1)) == 1 else h1\n","      new_values = np.array([x_left, y_top, bb_width, bb_height])\n","\n","    if self.plot_extensive:\n","      print(\"After padding:\", padded_img.shape)\n","      cv2_imshow(padded_img)\n","    return padded_img, new_values\n","\n","\n","  def normalisation(self, image, z_norm = True, **kwargs):\n","    '''Takes an input image and returns a normalised version of it'''\n","    if z_norm:\n","      # values z transformation\n","      #im = image.astype('float')\n","      im[im == 0] = np.nan\n","\n","      if np.isnan(im).all():\n","        #print(\"Problematic img: \", self.img_name, \" shape:\", im.shape)\n","        return image\n","\n","      mean = np.nanmean(im)\n","      sd = np.nanstd(im)\n","      n_im = (im - mean) / sd\n","      n_im = np.nan_to_num(n_im)\n","    else:\n","      # values normalised between 0 and 1\n","      n_im = image/np.max(image)\n","\n","    if self.plot_extensive:\n","      print('After normalising: range ' + str(round(np.nanmin(n_im), 3)) + ' to ' + str(round(np.nanmax(n_im), 3)))\n","      print(n_im.shape)\n","      cv2_imshow(n_im)\n","    return n_im\n","\n","\n","  def rotation(self, image_path, bbox):\n","    '''Takes an input image and returns a rotated version of it'''\n","    # read the image path\n","    im = cv2.imread(image_path) # loads image as BGR in uint8\n","    image = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)   # BGR -> RGB\n","    h, w = image.shape[:2]\n","    new_values = None\n","\n","    # convert to gray image\n","    img_gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n","\n","    # use threshold to find contours of relevant gray image\n","    ret, thresh = cv2.threshold(img_gray, 127, 255, cv2.THRESH_TRUNC)\n","    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","    cnt = contours[0]\n","\n","    # rectangle detected around the relevant image\n","    rect = cv2.minAreaRect(cnt)   # rect = ((center_x,center_y), (width,height), angle)\n","    box = cv2.boxPoints(rect)\n","    box = np.int0(box)\n","\n","    # get width and height of the detected rectangle\n","    rect_width = int(rect[1][0])\n","    rect_height = int(rect[1][1])\n","    \n","    src_pts = box.astype(\"float32\")\n","    if rect_height < rect_width:\n","      # coordinate of the points in box points after the rectangle has been straightened\n","      dst_pts = np.array([[rect_height-1, rect_width-1],\n","                          [0, rect_width-1],\n","                          [0, 0],\n","                          [rect_height-1, 0]], dtype=\"float32\")\n","      \n","      # the perspective transformation matrix\n","      M = cv2.getPerspectiveTransform(src_pts, dst_pts)\n","\n","      # directly warp the rotated rectangle to get the straightened rectangle\n","      warped = cv2.warpPerspective(image, M, (rect_height, rect_width))\n","\n","    else:\n","      # coordinate of the points in box points after the rectangle has been straightened\n","      dst_pts = np.array([[0, rect_height-1],\n","                          [0, 0],\n","                          [rect_width-1, 0],\n","                          [rect_width-1, rect_height-1]], dtype=\"float32\")\n","\n","      # the perspective transformation matrix\n","      M = cv2.getPerspectiveTransform(src_pts, dst_pts)\n","\n","      # directly warp the rotated rectangle to get the straightened rectangle\n","      warped = cv2.warpPerspective(image, M, (rect_width, rect_height))\n","    warped = warped.astype(np.float32)\n","\n","    # look for bounding box\n","    if not (bbox == 0).all():\n","      # read dimensions of bbox_eye\n","      cX = (int)(bbox[0])\n","      cY = (int)(bbox[1])\n","      bb_width = (int)(bbox[2])\n","      bb_height = (int)(bbox[3])\n","\n","      # recompute center of bbox according to transformation matrix\n","      new_cX = (M[0][0]*cX + M[0][1]*cY + M[0][2]) / ((M[2][0]*cX + M[2][1]*cY + M[2][2]))\n","      new_cY = (M[1][0]*cX + M[1][1]*cY + M[1][2]) / ((M[2][0]*cX + M[2][1]*cY + M[2][2]))\n","      new_values = np.array([new_cX, new_cY, bb_width, bb_height])\n","\n","      if self.plot_extensive:\n","        fig, (ax1, ax2) = plt.subplots(1, 2)\n","        ax1.imshow(image)\n","        ax1.add_patch(matplotlib.patches.Rectangle((cX, cY), bb_width, bb_height, color ='green', fc='none'))\n","        ax1.set(title = \"Original image with bbox (green)\")\n","        ax2.imshow(warped)\n","        ax2.add_patch(matplotlib.patches.Rectangle((new_cX, new_cY), bb_width, bb_height, color ='green', fc='none'))\n","        ax2.set(title = \"Rotated with bbox (green)\")\n","        fig.tight_layout()\n","        plt.show()\n","    \n","    if self.plot_extensive:\n","      print(\"After rotating:\", warped.shape)\n","      cv2_imshow(warped)\n","    return warped, new_values\n","\n","\n","  def random_crop(self, image, new_bbox_values):\n","    '''Takes an input image and returns a cropped version of it'''\n","    height, width = image.shape[:2]\n","    if height == self.min_height and width == self.min_width:\n","      #print(\"No crop needed\")\n","      return image\n","\n","    if new_bbox_values is None:\n","      # there is no eye, select random crop with dimensions (MIN_HEIGHT, MIN_WIDTH)\n","      box_to_crop = self.select_crop(image, eye = False)\n","      (x, y, w, h) = box_to_crop\n","\n","    else:\n","      # read dimensions of bbox_eye\n","      x_left = (int)(new_bbox_values[0])\n","      y_top = (int)(new_bbox_values[1])\n","      bb_width = (int)(new_bbox_values[2]) \n","      bb_height = (int)(new_bbox_values[3])\n","      bbox_eye = (x_left, y_top, bb_width, bb_height)\n","\n","      # select random crop with dimensions (MIN_HEIGHT, MIN_WIDTH) containing eye in bbox_eye\n","      box_to_crop = self.select_crop(image, bbox_eye, eye = True)\n","      (x, y, w, h) = box_to_crop\n","\n","      # recompute and normalize the x1 and y1 according to new dimensions of crop\n","      new_x_left = (x_left - x)\n","      new_y_top = (y_top - y)\n","      new_values = np.array([new_x_left, new_y_top, bb_width, bb_height])\n","    \n","    # crop image with the random crop\n","    img_cropped = image[int(y):int(y+h), int(x):int(x+w)]\n","\n","    if self.plot_extensive:\n","      print(\"After cropping:\", img_cropped.shape)\n","      cv2_imshow(img_cropped)\n","    assert img_cropped.shape[0] == self.min_height\n","    assert img_cropped.shape[1] == self.min_width\n","    return img_cropped\n","\n","\n","  #######################################  \n","  ######      HELPER FUNCTIONS      #####\n","  #######################################\n","  def define_search_box(self, img):\n","    \"\"\" Defines the box where to search for a random point. This box \n","    is computed to prevent additional padding in case the random point\n","    would appear next to the borders of the image. \"\"\"\n","    (h1, w1) = img.shape[:2]\n","    x = self.min_width/2\n","    y = self.min_height/2\n","    w = w1 - self.min_width\n","    h = h1 - self.min_height\n","    return (x, y, w, h)\n","\n","\n","  def pick_random_pt(self, bbox):\n","    \"\"\" Collects a random point from inside the box. \"\"\"\n","    (x_left, y_top, w, h) = bbox\n","    x_right = x_left + w\n","    y_bottom = y_top + h\n","    pnt = Point(random.uniform(x_left, x_right), random.uniform(y_top, y_bottom))\n","    return pnt\n","\n","\n","  def compute_intersection(self, bb1, bb2):\n","    \"\"\" Calculates the overlap of two bounding boxes. \"\"\"\n","    (x1, y1, w1, h1) = bb1\n","    (x2, y2, w2, h2) = bb2 \n","\n","    # determine the coordinates of the intersection rectangle\n","    x_left = max(x1, x2)\n","    y_top = max(y1, y2)\n","    x_right = min(x1+w1, x2+w2)\n","    y_bottom = min(y1+h1, y2+h2)\n","    intersection_rect = (x_left, y_top, x_right-x_left, y_bottom-y_top)\n","\n","    if x_right < x_left or y_bottom < y_top:\n","      overlap = 0.0\n","      return overlap, intersection_rect\n","\n","    # compute the intersection area\n","    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n","\n","    # compute the area of both AABBs\n","    bb1_area = w1 * h1\n","    bb2_area = w2 * h2\n","\n","    # compute the overlap\n","    overlap = intersection_area / bb1_area\n","    return overlap, intersection_rect\n","\n","\n","  def select_crop(self, im, bbox1 = None, eye = True):\n","    \"\"\" Selects the random crop with the dimensions (MIN_WIDTH, MIN_HEIGHT), \n","    knowing it has to contain the bounding box around the eye (bbox1). If \n","    there is no eye in the image, the crop is totally random. \"\"\"\n","\n","    # define box where to search for a random point\n","    search_box = self.define_search_box(im)\n","    (x, y, w, h) = search_box\n","\n","    if eye:\n","      # get dimensions from the bounding box around the eye\n","      (x1, y1, w1, h1) = bbox1\n","\n","      (height, width) = im.shape[:2]\n","      box_img = (0, 0, width, height)\n","      max_overlap,_ = self.compute_intersection(bbox1, box_img)\n","\n","      # only stop if bounding boxes are completely overlapped\n","      overlap = 0\n","      max_overlap_rounded = (math.floor(max_overlap*100)/100) - 0.005\n","      while overlap < max_overlap_rounded:\n","        # pick random point from inside search box\n","        pnt = self.pick_random_pt(search_box)\n","\n","        # define a box centered in pnt with the dimensions (MIN_WIDTH, MIN_HEIGHT)\n","        bbox2 = (pnt.x - self.min_width/2, pnt.y - self.min_height/2, self.min_width, self.min_height)\n","        (x2, y2, w2, h2) = bbox2\n","\n","        # compute the overlap between bounding boxes\n","        overlap, intersection_rect = self.compute_intersection(bbox1, bbox2)\n","    else:\n","      # pick random point from inside search box\n","      pnt = self.pick_random_pt(search_box)\n","\n","      # define a box centered in pnt with the dimensions (MIN_WIDTH, MIN_HEIGHT)\n","      bbox2 = (pnt.x - self.min_width/2, pnt.y - self.min_height/2, self.min_width, self.min_height)\n","      (x2, y2, w2, h2) = bbox2\n","\n","    if self.plot_light:\n","      fig = plt.figure() \n","      ax = fig.add_subplot(111) \n","      plt.imshow(im)\n","      plt.title(\"Original image, search box (green) and crop (blue)\")\n","      ax.add_patch(matplotlib.patches.Rectangle((x, y), w, h, color ='green', fc='none'))\n","      if eye:\n","        ax.add_patch(matplotlib.patches.Rectangle((x1, y1), w1, h1, color ='red', fc='none'))\n","      ax.add_patch(matplotlib.patches.Rectangle((x2, y2), w2, h2, color ='blue', fc='none'))\n","      ax.autoscale()\n","      plt.show()  \n","    return bbox2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mNuQlrZaKCjK"},"source":["def get_img_array(img_path, size):\n","    # `img` is a PIL image of size 299x299\n","    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n","    # `array` is a float32 Numpy array of shape (299, 299, 3)\n","    array = keras.preprocessing.image.img_to_array(img)\n","    # We add a dimension to transform our array into a \"batch\" of size (1, 299, 299, 3)\n","    array = np.expand_dims(array, axis=0)\n","    return array\n","\n","\n","def make_gradcam_heatmap(img_array, model, last_conv_layer_name, classifier_layer_names):\n","    # First, we create a model that maps the input image to the activations of the last conv layer\n","    last_conv_layer = model.get_layer(last_conv_layer_name)\n","    last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)\n","\n","    # Second, we create a model that maps the activations of the last conv layer to the final class predictions\n","    classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n","    x = classifier_input\n","    for layer_name in classifier_layer_names:\n","        x = model.get_layer(layer_name)(x)\n","    classifier_model = keras.Model(classifier_input, x)\n","\n","    # Then, we compute the gradient of the top predicted class for our input image\n","    # with respect to the activations of the last conv layer\n","    with tf.GradientTape() as tape:\n","        # Compute activations of the last conv layer and make the tape watch it\n","        last_conv_layer_output = last_conv_layer_model(img_array)\n","        tape.watch(last_conv_layer_output)\n","        # Compute class predictions\n","        preds = classifier_model(last_conv_layer_output)\n","        top_pred_index = tf.argmax(preds[0])\n","        top_class_channel = preds[:, top_pred_index]\n","\n","    # This is the gradient of the top predicted class with regard to\n","    # the output feature map of the last conv layer\n","    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n","\n","    # This is a vector where each entry is the mean intensity of the gradient\n","    # over a specific feature map channel\n","    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n","\n","    # We multiply each channel in the feature map array\n","    # by \"how important this channel is\" with regard to the top predicted class\n","    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n","    pooled_grads = pooled_grads.numpy()\n","    for i in range(pooled_grads.shape[-1]):\n","        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n","\n","    # The channel-wise mean of the resulting feature map is our heatmap of class activation\n","    heatmap = np.mean(last_conv_layer_output, axis=-1)\n","\n","    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n","    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n","    return heatmap"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u98-zRy8uPst"},"source":["Load previously saved model."]},{"cell_type":"code","metadata":{"id":"ES-VLu14RD2d"},"source":["main_dir = \"SAR_swath_images_WS+WS+WS/\"\n","dir = main_dir + \"classification_results/categorization/ResNet_Numeric-F_BatchSize-8_lr-0.0001_Epochs-25_Folds-5_Norm-T_Aug-T_FineTune-T_from165/\"\n","#dir = main_dir + \"classification_results/identification/ResNet_Numeric-F_BatchSize-8_lr-0.0001_Epochs-20_Folds-5_Norm-T/\"\n","#save_path = dir + \"Gradcam_heatmaps\"\n","#os.makedirs(save_path, exist_ok=True)\n","\n","# load previously saved model\n","model = tf.keras.models.load_model(dir + \"best_model_fine_tuned_4.h5\", compile = False)\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mj3Bq4NqYEKz"},"source":["# source: https://stackoverflow.com/questions/50283844/in-keras-how-to-get-the-layer-name-associated-with-a-model-object-contained-i\n","last_conv_layer_name = model.get_layer('resnet50').layers[-1].name\n","last_conv_layer = model.get_layer('resnet50').get_layer(last_conv_layer_name)\n","\n","print(\"last_conv_layer_name:\", last_conv_layer_name)\n","print(\"last_conv_layer:\", last_conv_layer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rwSqTJxwfm2B"},"source":["last_conv_layer.output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jV85inIWe025"},"source":["model.get_layer('resnet50').summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1QrOlIQwhhBJ"},"source":["model.inputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MjFVVeIokkl2"},"source":["model.get_layer(\"resnet50\").inputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"enp8jEjVcskL"},"source":["from tensorflow import keras\n","last_conv_layer_model = keras.Model(model.get_layer(\"resnet50\").inputs, last_conv_layer.output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"roojlKM5iIoh"},"source":["classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n","x = classifier_input\n","classifier_layer_names = [\n","    \"global_average_pooling2d\",\n","    \"dense\"]\n","for layer_name in classifier_layer_names:\n","    x = model.get_layer(layer_name)(x)\n","classifier_model = keras.Model(classifier_input, x)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aVNn9rqcL47a"},"source":["model.get_layer('mobilenetv2_1.00_224').layers[-1].name\n","[layer.name for layer in model.layers[-2:]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C6gNBlB1Gsdf"},"source":["last_conv_layer_name = \"conv5_block3_out\"\n","\n","# check the names with model.summary(), sometimes recreating the model multiple times for testing the names can change\n","classifier_layer_names = [\n","    \"global_average_pooling2d\",\n","    \"dense\"\n","#    \"global_average_pooling2d_1\",\n","#    \"dense_1\"\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nkJlSAA1G0ri"},"source":["Actual GradCam testing"]},{"cell_type":"code","metadata":{"id":"HbDZQa2jDwZg"},"source":["\"\"\"\n","MIN_HEIGHT = 700\n","MIN_WIDTH = 400\n","BATCH_SIZE = 1\n","NETWORK = \"ResNet\"\n","\n","# Preprocess image\n","df = pd.read_csv(main_dir + \"/csv/full_dataset.csv\", converters={'bbox_shape': eval}).dropna()\n","images, labels, bboxes = helper.load_from_df(df)\n","\n","# create an instance of the DataProcessor\n","processor = DataProcessor(model = NETWORK,\n","                          min_height = MIN_HEIGHT,\n","                          min_width = MIN_WIDTH)\n","\n","processed_image = np.array([processor.preprocess_pipeline(images[2], bboxes[2])])\n","\n","# Print what the top predicted class is\n","preds = model.predict(processed_image)\n","#print(\"Predicted:\", decode_predictions(preds, top=1)[0])\n","print(preds)\n","print(labels[2])\n","# Generate class activation heatmap\n","heatmap = make_gradcam_heatmap(\n","    processed_image, model, last_conv_layer_name, classifier_layer_names\n",")\n","\n","# Display heatmap\n","plt.matshow(heatmap)\n","plt.show()\n","sys.exit(\"Stop\")\n","############################################\n","###  SUPERIMPOSED GARDCAM VISUALIZATION  ###\n","############################################\n","\n","# Load the original image\n","img = keras.preprocessing.image.load_img(images[0])\n","img = keras.preprocessing.image.img_to_array(img)\n","\n","# Rescale heatmap to a range 0-255\n","heatmap = np.uint8(255 * heatmap)\n","\n","# Use jet colormap to colorize heatmap\n","jet = cm.get_cmap(\"jet\")\n","\n","# Use RGB values of the colormap\n","jet_colors = jet(np.arange(256))[:, :3]\n","jet_heatmap = jet_colors[heatmap]\n","\n","# Create an image with RGB colorized heatmap\n","jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n","jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n","jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n","\n","# Superimpose the heatmap on original image\n","superimposed_img = jet_heatmap * 0.4 + img\n","superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n","\n","# Save the superimposed image\n","#superimposed_img.save(save_path)\n","\n","# Display Grad CAM\n","display(Image(save_path))\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hFJsZMlNKDah"},"source":["MIN_HEIGHT = 700\n","MIN_WIDTH = 400\n","BATCH_SIZE = 1\n","NETWORK = \"ResNet\"\n","categorization = False\n","\n","# Preprocess image\n","csv_path = main_dir + \"/csv/full_dataset.csv\"\n","images, labels, bboxes = helper.load_data(csv_path, with_cat = categorization)\n","\n","# create an instance of the DataProcessor\n","processor = DataProcessor(model = NETWORK,\n","                          min_height = MIN_HEIGHT,\n","                          min_width = MIN_WIDTH)\n","\n","count = -1\n","for image, bbox, label in zip(images, bboxes, labels):\n","    count += 1\n","    orig_image, processed_image = processor.preprocess_pipeline(image, bbox)\n","    processed_image = np.array([processed_image])\n","\n","    # Print what the top predicted class is\n","    preds = model.predict(processed_image)\n","    #print(\"Predicted:\", decode_predictions(preds, top=1)[0])\n","    predictions = np.round(preds[0],2) if categorization else preds[0]\n","    print(\"Prediction: {}, True label: {}\".format(predictions, label))\n","\n","    # Generate class activation heatmap\n","    heatmap = make_gradcam_heatmap(\n","        processed_image, model, last_conv_layer_name, classifier_layer_names\n","    )\n","\n","    # Display heatmap\n","    #plt.matshow(heatmap)\n","    #plt.show()\n","    #plt.imshow(orig_image[:,:,1])\n","    #plt.show()\n","    #plt.imshow(orig_image[:,:,2])\n","    #plt.show()\n","\n","    ############################################\n","    ###  SUPERIMPOSED GARDCAM VISUALIZATION  ###\n","    ############################################\n","    # Load the original image\n","    #plt.imshow(orig_image)\n","    #plt.title(\"Original Image\")\n","    #plt.show()\n","    #img = keras.preprocessing.image.load_img(image)\n","    #img = keras.preprocessing.image.img_to_array(img)\n","\n","    # Rescale heatmap to a range 0-255\n","    heatmap = np.uint8(255 * heatmap)\n","\n","    # Use jet colormap to colorize heatmap\n","    jet = cm.get_cmap(\"jet\")\n","\n","    # Use RGB values of the colormap\n","    jet_colors = jet(np.arange(256))[:, :3]\n","    jet_heatmap = jet_colors[heatmap]\n","\n","    # Create an image with RGB colorized heatmap\n","    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n","    jet_heatmap = jet_heatmap.resize((orig_image.shape[1], orig_image.shape[0]))\n","    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n","\n","    # Superimpose the heatmap on original image\n","    vv_channel = np.dstack((orig_image[:,:,2], orig_image[:,:,2], orig_image[:,:,2]))\n","    vh_channel = np.dstack((orig_image[:,:,1], orig_image[:,:,1], orig_image[:,:,1]))\n","    superimposed_img = jet_heatmap * 0.4 + vv_channel\n","    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n","\n","    # Save the superimposed image\n","    #superimposed_img.save(save_path)\n","\n","    # Display Grad CAM\n","    #display(Image(save_path))\n","    #plt.imshow(superimposed_img)\n","    #plt.title(\"Superimposed Image\")\n","    #plt.show()\n","\n","    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize = (15,9.2))\n","    font_size = 18\n","    ax1.imshow(vv_channel)    # plot vv channel\n","    ax1.set_title(\"Original Image (VV) - label: {}\".format(label), fontsize = font_size)\n","\n","    #ax2.imshow(np.squeeze(processed_image, axis=0))\n","    #ax2.set(title = \"Normalised Image\")\n","    ax2.imshow(vh_channel)  # plot vh channel\n","    ax2.set_title(\"Original Image (VH)\", fontsize = font_size)\n","\n","    #ax3.matshow(heatmap)\n","    #ax3.set(title = \"Heatmap - prediction: {}\".format(predictions))\n","    heat_plot = ax3.imshow(jet_heatmap, cmap='jet')\n","    ax3.set_title(\"Heatmap - prediction: {}\".format(predictions), fontsize = font_size)\n","    divider = make_axes_locatable(ax3)\n","    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n","    plt.colorbar(heat_plot, cax=cax)\n","\n","    ax4.imshow(superimposed_img)\n","    ax4.set_title(\"Superimposed Image\", fontsize = font_size)\n","    fig.tight_layout()\n","    plt.show()\n","    \n","    #fig, (ax1, ax2, ax3, ax4) = plt.subplots(1,4, figsize = (15,12))\n","    #ax1.imshow(vv_channel)  # plot vv channel\n","    #ax1.set(title = \"Original Image (VV) - label: {}\".format(label))\n","\n","    #ax2.imshow(vh_channel)  # plot vh channel\n","    #ax2.set(title = \"Original Image (VH)\")\n","\n","    #heat_plot = ax3.imshow(jet_heatmap, cmap='jet')\n","    #ax3.set(title = \"Heatmap - prediction: {}\".format(predictions))\n","    #divider = make_axes_locatable(ax3)\n","    #cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n","    #plt.colorbar(heat_plot, cax=cax)\n","\n","    #ax4.imshow(superimposed_img)\n","    #ax4.set(title = \"Superimposed Image\")\n","    #fig.tight_layout()\n","    #plt.show()\n","\n","    fig.savefig(\"{}/id_resnet_gradcam.jpg\".format(save_path))\n","    #fig.savefig(\"{}/superimposed_{}.jpg\".format(save_path, count))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kq36IoxN0JO4"},"source":["save_path = dir + \"gradcam_heatmaps_{}\".format(fold_var)\n","os.makedirs(save_path, exist_ok=True)\n","\n","def grad_cam(model, NETWORK, val_dataset, val_norm_dataset, save_path):\n","    cnt = 0\n","    if NETWORK == \"ResNet\":\n","        # check the names with model.summary()\n","        last_conv_layer_name = \"conv5_block3_out\"\n","        \n","        classifier_layer_names = [\n","            \"global_average_pooling2d\",\n","            \"dense\"\n","            ]\n","            \n","    else:  #MobileNetV2\n","        last_conv_layer_name = \"out_relu\" # have to check this again\n","        \n","        classifier_layer_names = [\n","            \"global_average_pooling2d_1\",\n","            \"dense_1\"\n","            ]\n","\n","    for orig_image, label, processed_image, processed_label in zip(val_dataset, val_norm_dataset):\n","        # label and norm_label should be the same\n","\n","        pred = model.predict(processed_image)\n","        prediction = np.round(pred[0],2) #if categorization else pred[0]\n","\n","        heatmap = make_gradcam_heatmap(\n","            processed_image, model, last_conv_layer_name, classifier_layer_names\n","            )\n","\n","        # Rescale heatmap to a range 0-255\n","        heatmap = np.uint8(255 * heatmap)\n","\n","        # Use jet colormap to colorize heatmap\n","        jet = cm.get_cmap(\"jet\")\n","\n","        # Use RGB values of the colormap\n","        jet_colors = jet(np.arange(256))[:, :3]\n","        jet_heatmap = jet_colors[heatmap]\n","\n","        # Create an image with RGB colorized heatmap\n","        jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n","        jet_heatmap = jet_heatmap.resize((orig_image.shape[1], orig_image.shape[0]))\n","        jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n","\n","        # Superimpose the heatmap on original image\n","        vv_channel = np.dstack((orig_image[:,:,0], orig_image[:,:,0], orig_image[:,:,0]))\n","        vh_channel = np.dstack((orig_image[:,:,1], orig_image[:,:,1], orig_image[:,:,1]))\n","        superimposed_img = jet_heatmap * 0.4 + vv_channel\n","        superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n","\n","        # Display Grad CAM\n","        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize = (15,9.2))\n","        font_size = 18\n","        ax1.imshow(vv_channel)    # plot vv channel\n","        ax1.set_title(\"Original Image (VV) - label: {}\".format(label), fontsize = font_size)\n","        ax2.imshow(vh_channel)  # plot vh channel\n","        ax2.set_title(\"Original Image (VH)\", fontsize = font_size)\n","        heat_plot = ax3.imshow(jet_heatmap, cmap='jet')\n","        ax3.set_title(\"Heatmap - prediction: {}\".format(prediction), fontsize = font_size)\n","        divider = make_axes_locatable(ax3)\n","        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n","        plt.colorbar(heat_plot, cax=cax)\n","        ax4.imshow(superimposed_img)\n","        ax4.set_title(\"Superimposed Image\", fontsize = font_size)\n","        fig.tight_layout()\n","        #plt.show()\n","\n","        fig.savefig(\"{}/heatmap_{}.jpg\".format(save_path, cnt), bbox_inches='tight')\n","        cnt += 1\n","\n","    return"],"execution_count":null,"outputs":[]}]}