{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"TC_Category_Classification.ipynb","provenance":[],"collapsed_sections":["sUEMhiDjZ9bi","GPP7a2XY3Atf","_D-HnbVhEYPS","9ht1Z-KOohzD"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/raquelcarmo/tropical_cyclones/blob/import-py-files/src/code/TC_Category_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"DDvPT6mlexeA"},"source":["#Tropical Cyclones Categorization\n","\n","Script to train Deep Learning models to categorize TCs based on their topology patterns."]},{"cell_type":"markdown","metadata":{"id":"EG-uvLsngAPX"},"source":["##Imports and configurations"]},{"cell_type":"code","metadata":{"id":"Jnxs1L1PWjeL"},"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JNUcV31BIHhF"},"source":["%cd /content/drive/My Drive/ESRIN_PhiLab/Tropical_Cyclones/tropical_cyclones/src/code\n","%ls\n","\n","import imp \n","# import helper.py\n","helper = imp.new_module('helper_functions')\n","exec(open(\"./helper_functions.py\").read(), helper.__dict__)\n","# import models.py\n","models = imp.new_module('models')\n","exec(open(\"./models.py\").read(), models.__dict__)\n","# import data_processor.py\n","dp = imp.new_module('data_processor')\n","exec(open(\"./data_processor.py\").read(), dp.__dict__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4p-mSrzKWqUb"},"source":["# insert your desired path to work on\n","%cd /content/drive/My Drive/ESRIN_PhiLab/Tropical_Cyclones/data\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PZu5fkuXWxdC"},"source":["# general imports\n","import random\n","import glob\n","import os\n","import sys\n","sys.stdout.flush()\n","import pandas as pd\n","import numpy as np\n","import cv2\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","import math\n","import imageio\n","import os.path\n","import time\n","from PIL import Image\n","from mpl_toolkits.axes_grid1 import make_axes_locatable\n","from scipy import ndimage\n","from google.colab.patches import cv2_imshow\n","import random\n","from shapely.geometry import Point\n","import re\n","import pickle\n","import scipy\n","from sklearn.model_selection import StratifiedKFold\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n","import datetime\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","import tensorflow_datasets as tfds\n","from tensorflow.data import Dataset\n","from tensorflow.keras import Input\n","from tensorflow.keras.applications import resnet50, mobilenet_v2, vgg16\n","from tensorflow.keras.applications.resnet50 import ResNet50\n","from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n","from tensorflow.keras.applications.vgg16 import VGG16\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import concatenate, Dense, GlobalAveragePooling2D, Dropout\n","from tensorflow.keras.optimizers import SGD, Adam\n","from tensorflow.keras.metrics import CategoricalAccuracy, TopKCategoricalAccuracy, Precision, Recall, TruePositives, FalsePositives, TrueNegatives, FalseNegatives\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","np.set_printoptions(precision=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sUEMhiDjZ9bi"},"source":["## 1.Train on data according to csv split into train, val and test sets\n","\n","Prepare the tf.data.Dataset instances to be fed to the model."]},{"cell_type":"code","metadata":{"id":"ySpk4sOVemVz"},"source":["##################################\n","###          SETTINGS         ####\n","##################################\n","main_dir = \"SAR_swath_images_VV+VH+WS\"\n","NETWORK = \"ResNet\"     # options: [\"Mobile\", \"ResNet\"]\n","MIN_HEIGHT = 288\n","MIN_WIDTH = 288\n","NUM_VARS = False\n","NORMALISE = True\n","ROTATE = False\n","BATCH_SIZE = 8\n","BUFFER_SIZE = 100\n","EPOCHS = 20\n","LEARNING_RATE = 0.0001\n","##################################\n","\n","# load data\n","train_images, train_labels, train_bbox = helper.load_data(\"{}/csv/training.csv\".format(main_dir))\n","val_images, val_labels, val_bbox = helper.load_data(\"{}/csv/val.csv\".format(main_dir))\n","test_images, test_labels, test_bbox = helper.load_data(\"{}/csv/test.csv\".format(main_dir))\n","class_weights = helper.compute_class_weights(\"{}/csv/full_dataset.csv\".format(main_dir))\n","\n","\n","# create an instance of the DataProcessor\n","processor = dp.DataProcessor(model = NETWORK,\n","                             min_height = MIN_HEIGHT,\n","                             min_width = MIN_WIDTH,\n","                             normalise = NORMALISE,           # perform normalisation\n","                             rotate = ROTATE,                 # perform rotation\n","                             plot_light = False,              # plot only select_crop() images\n","                             plot_extensive = False,          # plot extensively all images\n","                             show_prints = False)\n","\n","\n","# generate datasets\n","train_dataset = helper.prepare_dataset(processor, train_images, train_labels, train_bbox)\n","val_dataset = helper.prepare_dataset(processor, val_images, val_labels, val_bbox)\n","test_dataset = helper.prepare_dataset(processor, test_images, test_labels, test_bbox)\n","#for image, label in train_dataset:\n","#  plt.imshow(image)\n","#  plt.show()\n","#  print(\"FINAL - image: {}, label: {}\".format(image.shape, label))\n","#  print(\"FINAL - image: {}, max: {}, min: {}, label: {}\".format(image.shape, np.max(image), np.min(image),  label))\n","\n","train_dataset, val_dataset = helper.z_norm(train_dataset, val_dataset)\n","#cnt = 0\n","#for image, label in train_dataset:\n","#  cnt +=1\n","#  plt.imshow(image)\n","#  plt.show()\n","\n","# configure for performance\n","train_dataset = helper.configure_for_performance(train_dataset, BUFFER_SIZE, BATCH_SIZE, shuffle = True, augment = True)\n","val_dataset = helper.configure_for_performance(val_dataset, BUFFER_SIZE, BATCH_SIZE)\n","test_dataset = helper.configure_for_performance(test_dataset, BUFFER_SIZE, BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H9-tkgGrid0-"},"source":["Script to perform end-to-end training in the model."]},{"cell_type":"code","metadata":{"id":"w0eTiiKghBZg"},"source":["# directory to save results\n","dir = '{}_Numeric-{}_BatchSize-{}_Epochs-{}_Norm-{}'.format(NETWORK, str(NUM_VARS)[0], str(BATCH_SIZE), str(EPOCHS), str(NORMALISE)[0])\n","save_dir = 'classification_results/categorization/' + dir + '/'\n","os.makedirs(save_dir, exist_ok=True)\n","\n","# MAKE MODEL\n","if NETWORK == \"Mobile\":\n","    model = models.make_MobileNet(MIN_WIDTH, MIN_HEIGHT, LEARNING_RATE)\n","elif NETWORK == \"ResNet\":\n","    model = models.make_ResNet(MIN_WIDTH, MIN_HEIGHT, LEARNING_RATE)\n","else:\n","    sys.exit(\"Incert valid network model. Options: Mobile or ResNet (case sensitive)\")\n","\n","# CREATE CALLBACKS\n","callbacks = [\n","    EarlyStopping(patience = 7, verbose = 1),\n","    ReduceLROnPlateau(factor = 0.1, patience = 5, min_lr = 0.00001, verbose = 1),\n","    ModelCheckpoint(save_dir + \"best_model.h5\", verbose = 1, save_best_only = True),\n","    TensorBoard(log_dir = save_dir + \"logs/\" + datetime.datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"))\n","]\n","\n","# TRAIN THE NETWORK\n","H = model.fit(\n","    train_dataset,\n","    steps_per_epoch = len(train_dataset),\n","    validation_data = val_dataset,\n","    validation_steps = len(val_dataset),\n","    epochs = EPOCHS,\n","    callbacks = callbacks,\n","    verbose = 1,\n","    class_weight = class_weights,\n","    shuffle = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z0wRibZ1YUh-"},"source":["# Load the TensorBoard notebook extension\n","#%load_ext tensorboard\n","%reload_ext tensorboard\n","%tensorboard --logdir classification_results/categorization/ResNet_Numeric-F_BatchSize-8_Epochs-20_Norm-T/logs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oBRY3vfnhF-D"},"source":["# EVALUATE MODEL\n","print(\"Loaded best weights of the training\")\n","model.load_weights(save_dir + \"best_model.h5\")\n","\n","results = model.evaluate(test_dataset, \n","                         steps = len(test_dataset), \n","                         verbose = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kGg9tMhEhKut"},"source":["# MAKE PREDICTIONS\n","# Retrieve a batch of images from the test set\n","predictions = model.predict(test_dataset.)\n","\n","predictions = tf.where(predictions < 0.5, 0, 1)\n","\n","print('Predictions:\\n', predictions.numpy())\n","print('Labels:\\n')\n","for label in test_labels_dataset:\n","    print(label)\n","#class_names = {0: \"No eye\", 1: \"Eye\"}\n","#plt.figure(figsize=(10, 10))\n","#for i in range(9):\n","#  ax = plt.subplot(3, 3, i + 1)\n","#  plt.imshow(image_batch[i].astype(\"uint8\"))\n","#  plt.title(class_names[predictions[i]])\n","#  plt.axis(\"off\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-V1lTSzbDUyS"},"source":["## 2.Train on data using the Stratified K Fold"]},{"cell_type":"code","metadata":{"id":"NxvI67fsr2zD"},"source":["# STRATIFIED CROSS VALIDATION\n","def fold_cross_validation(main_dir, NETWORK, MIN_HEIGHT, MIN_WIDTH, EYE_ONLY, NUM_VARS, NORMALISE, NORM_MODE, ROTATE, N_CROPS, \n","                          CROP_MODE, AUGMENT, BATCH_SIZE, BUFFER_SIZE, EPOCHS, LEARNING_RATE, SPLIT_NUMBER, DROPOUT, DROP_RATE):\n","\n","    full_dataset_path = \"{}/csv/full_dataset.csv\".format(main_dir)\n","    df = pd.read_csv(full_dataset_path, converters={'bbox_shape': eval}).dropna()\n","    print(\"Dataset dimension: {}\".format(len(df)))\n","\n","    # COMPUTE CLASS WEIGHTS\n","    # if CROP_MODE == weighted (proporional to presence of classes), helper.compute_class_weights()\n","    # will no longer represent the true distribution of classes in the dataset\n","    class_weights = helper.compute_class_weights(full_dataset_path, eye_only = EYE_ONLY) if CROP_MODE != \"weighted\" else None\n","\n","    # extract non-categorical labels for StratifiedKFold\n","    #Y = df[\"label\"]\n","    Y = np.zeros(len(df), dtype=int)\n","    for index, row in df.iterrows():\n","        if df[\"label\"][index] != 0:\n","            cat = df[\"image\"][index].split('/')[1]\n","            Y[index] = int(cat[-1])\n","    \n","    if EYE_ONLY:\n","        assert isinstance(Y, np.ndarray)\n","        idx = np.where(Y == 0)[0].tolist()\n","        Y = np.delete(Y, idx)\n","        Y -= 1\n","        df.drop(idx, inplace = True)\n","        df.reset_index(drop=True, inplace=True)\n","        #print(df)\n","        print(\"New dimensions, Y: {} and df: {}\".format(len(Y), len(df)))\n","\n","    VALIDATION_ACCURACY = []\n","    VALIDATION_TOP2_ACCURACY = []\n","    VALIDATION_LOSS = []\n","    VALIDATION_TP = []\n","    VALIDATION_FP = []\n","    VALIDATION_TN = []\n","    VALIDATION_FN = []\n","    VALIDATION_PRECISION = []\n","    VALIDATION_RECALL = []\n","\n","    # directory to save results\n","    dir = '{}_nu-{}_bs-{}_{}x{}_lr-{}_ep-{}_sp-{}_no-{}{}_cr-{}{}_ag-{}_drp-{}{}'.format(\n","        NETWORK, str(NUM_VARS)[0], BATCH_SIZE, MIN_HEIGHT, MIN_WIDTH, LEARNING_RATE, EPOCHS, SPLIT_NUMBER, \n","        NORM_MODE[0], str(NORMALISE)[0], CROP_MODE[0], N_CROPS, str(AUGMENT)[0], str(DROPOUT)[0], DROP_RATE)\n","    save_dir = main_dir + '/classification_results/categorization/test_eye_only/' + dir + '/'\n","\n","\n","    # create an instance of the DataProcessor\n","    processor = dp.DataProcessor(model = NETWORK,\n","                                min_height = MIN_HEIGHT,\n","                                min_width = MIN_WIDTH,\n","                                normalise = NORMALISE,           # perform normalisation [DEPRECATED]\n","                                rotate = ROTATE,                 # perform rotation\n","                                plot_light = False,              # plot only select_crop() images\n","                                plot_extensive = False,          # plot extensively all images\n","                                show_prints = False)\n","\n","\n","    print(\"Entering in k fold cross validation...\")\n","    stratified_k_fold = StratifiedKFold(n_splits = SPLIT_NUMBER, shuffle = False)\n","    fold_var = 1\n","\n","    for train_index, val_index in stratified_k_fold.split(np.zeros(len(df)), Y):\n","        training_data = df.iloc[train_index]\n","        validation_data = df.iloc[val_index]\n","\n","        # LOAD DATA\n","        train_images, train_labels, train_bbox = helper.load_data(df = training_data, eye_only = EYE_ONLY)\n","        #print(\"Train:\", len(train_images), len(train_labels), len(train_bbox))\n","        val_images, val_labels, val_bbox = helper.load_data(df = validation_data, eye_only = EYE_ONLY)\n","        #print(\"Validation:\", len(val_images), len(val_labels), len(val_bbox))\n","\n","        # GENERATE DATASETS\n","        #train_dataset = helper.prepare_dataset(processor, train_images, train_labels, train_bbox)\n","        #val_dataset = helper.prepare_dataset(processor, val_images, val_labels, val_bbox)\n","        train_dataset = helper.create_dataset(processor, train_images, train_labels, train_bbox, N_CROPS, CROP_MODE, MIN_HEIGHT, MIN_WIDTH)\n","        val_dataset = helper.create_dataset(processor, val_images, val_labels, val_bbox, 1, CROP_MODE, MIN_HEIGHT, MIN_WIDTH)\n","        # since we increased dataset size with more crops, val_labels also increased\n","        #val_labels = val_dataset.map(lambda x, y: y)\n","\n","        # PERFORM NORMALISATION\n","        if NORMALISE:\n","            train_norm_dataset, val_norm_dataset = helper.normalisation(train_dataset, val_dataset, mode = NORM_MODE, model = NETWORK)\n","        else:\n","            train_norm_dataset = train_dataset\n","            val_norm_dataset = val_dataset\n","        unbatched_val_norm_dataset = val_norm_dataset\n","\n","        # CONFIGURE FOR PERFORMANCE\n","        SQUARED = True if MIN_HEIGHT == MIN_WIDTH else False\n","        train_norm_dataset = helper.configure_for_performance(train_norm_dataset, BUFFER_SIZE, BATCH_SIZE, shuffle = True, augment = AUGMENT, squared_input = SQUARED)\n","        val_norm_dataset = helper.configure_for_performance(val_norm_dataset, BUFFER_SIZE, BATCH_SIZE)\n","\n","        # CREATE NEW MODEL\n","        model = models.make_cnn(NETWORK, MIN_WIDTH, MIN_HEIGHT, LEARNING_RATE, eye_only = EYE_ONLY, dropout = DROPOUT, rate = DROP_RATE)\n","        #if NETWORK == \"Mobile\":\n","        #    model = models.make_MobileNet(MIN_WIDTH, MIN_HEIGHT, LEARNING_RATE, eye_only = True)\n","        #elif NETWORK == \"ResNet\":\n","        #    model = models.make_ResNet(MIN_WIDTH, MIN_HEIGHT, LEARNING_RATE, eye_only = True)\n","        #elif NETWORK == \"VGG\":\n","        #    model = models.make_VGG(MIN_WIDTH, MIN_HEIGHT, LEARNING_RATE, eye_only = True)\n","        #else:\n","        #    sys.exit(\"Incert valid network model. Options: Mobile, ResNet or VGG (case sensitive)\")\n","\n","        # CREATE CALLBACKS\n","        callbacks = [\n","            #EarlyStopping(patience = 10, verbose = 1),\n","            ReduceLROnPlateau(factor = 0.1, patience = 5, min_lr = 0.000001, verbose = 1),\n","            ModelCheckpoint(save_dir + helper.get_model_name(fold_var), verbose = 1, save_best_only = True),\n","            #TensorBoard(log_dir = save_dir + \"logs/\" + datetime.datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"))\n","        ]\n","\n","        # FIT THE MODEL\n","        history = model.fit(\n","            train_norm_dataset,\n","            steps_per_epoch = len(train_norm_dataset),\n","            validation_data = val_norm_dataset,\n","            validation_steps = len(val_norm_dataset), \n","            epochs = EPOCHS,\n","            callbacks = callbacks,\n","            verbose = 1, \n","            class_weight = class_weights,\n","            shuffle = True)\n","        \n","        # PLOT HISTORY\n","        #print(history)\n","\n","\n","        # PLOT TRAIN/VALIDATION LOSSES\n","        fig, (ax1, ax2) = plt.subplots(nrows = 2, ncols = 1, figsize=(10, 10))\n","        ax1.plot(history.history['accuracy'], label='Training Accuracy')\n","        ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","        ax1.grid(True)\n","        ax1.legend(loc='lower right')\n","        ax1.set(ylabel = \"Accuracy\",\n","                title = 'Training and Validation Accuracy')\n","\n","        ax2.plot(history.history[\"loss\"], label='Training Loss')\n","        ax2.plot(history.history[\"val_loss\"], label='Validation Loss')\n","        ax2.plot(np.argmin(history.history[\"val_loss\"]), np.min(history.history[\"val_loss\"]), marker=\"x\", color=\"r\", label = \"Best model\")\n","        ax2.grid(True)\n","        ax2.legend(loc='upper right')\n","        ax2.set(xlabel = 'Epoch', \n","                ylabel = 'Cross Entropy',\n","                title = 'Training and Validation Loss')\n","        plt.show()\n","        fig.savefig(save_dir + \"model_\" + str(fold_var) + \".jpg\", bbox_inches='tight')\n","\n","        \n","        # LOAD BEST MODEL\n","        time.sleep(10) # guarantees enough time for weights to be saved and loaded afterwards, otherwise gives concurrency problems\n","        print(\"Loaded best weights of the training\")\n","        model.load_weights(save_dir + helper.get_model_name(fold_var))\n","\n","        # EVALUATE PERFORMANCE of the model\n","        results = model.evaluate(val_norm_dataset,\n","                                steps = len(val_norm_dataset), \n","                                verbose = 1)\n","\n","        results = dict(zip(model.metrics_names, results))\n","\n","        VALIDATION_ACCURACY.append(results['accuracy'])\n","        VALIDATION_TOP2_ACCURACY.append(results['top2_accuracy'])\n","        VALIDATION_LOSS.append(results['loss'])\n","        VALIDATION_TP.append(results[\"tp\"])\n","        VALIDATION_FP.append(results[\"fp\"])\n","        VALIDATION_TN.append(results[\"tn\"])\n","        VALIDATION_FN.append(results[\"fn\"])\n","        VALIDATION_PRECISION.append(results[\"precision\"])\n","        VALIDATION_RECALL.append(results[\"recall\"])\n","\n","        # MAKE PREDICTIONS\n","        predictions = model.predict(val_norm_dataset)\n","        predictions_non_category = [ np.argmax(t) for t in predictions ]\n","        val_labels_non_category = [ np.argmax(t) for t in val_labels ]\n","\n","        labels = [0,1,2,3,4,5] if not EYE_ONLY else [0,1,2,3,4]\n","        display_labels = labels if not EYE_ONLY else [x+1 for x in labels]\n","        conf_mat = confusion_matrix(val_labels_non_category, predictions_non_category, labels = labels)\n","        disp = ConfusionMatrixDisplay(confusion_matrix = conf_mat, display_labels = display_labels)\n","        conf_mat_display = disp.plot()\n","        plt.savefig(save_dir + \"confusion_matrix_\" + str(fold_var) + \".jpg\", bbox_inches='tight')\n","\n","        # GRAD-CAM ANALYSIS\n","        gradcam_path = save_dir + \"gradcam_heatmaps_{}\".format(fold_var)\n","        os.makedirs(gradcam_path, exist_ok=True)\n","        helper.grad_cam(model, val_dataset, unbatched_val_norm_dataset, predictions, gradcam_path, dropout = DROPOUT)\n","\n","        tf.keras.backend.clear_session()\n","\n","        fold_var += 1\n","\n","        # save the values for each fold\n","        csv_dir = save_dir + 'csv/'\n","        os.makedirs(csv_dir, exist_ok=True)\n","        with open(csv_dir + 'test_accuracy.pkl', 'wb') as handle:\n","            pickle.dump(VALIDATION_ACCURACY, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","        with open(csv_dir + 'test_top2_accuracy.pkl', 'wb') as handle:\n","            pickle.dump(VALIDATION_TOP2_ACCURACY, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","        with open(csv_dir + 'test_loss.pkl', 'wb') as handle:\n","            pickle.dump(VALIDATION_LOSS, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","        with open(csv_dir + 'test_tp.pkl', 'wb') as handle:\n","            pickle.dump(VALIDATION_TP, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","        with open(csv_dir + 'test_fp.pkl', 'wb') as handle:\n","            pickle.dump(VALIDATION_FP, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","        with open(csv_dir + 'test_tn.pkl', 'wb') as handle:\n","            pickle.dump(VALIDATION_TN, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","        with open(csv_dir + 'test_fn.pkl', 'wb') as handle:\n","            pickle.dump(VALIDATION_FN, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","        with open(csv_dir + 'test_precision.pkl', 'wb') as handle:\n","            pickle.dump(VALIDATION_PRECISION, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","        with open(csv_dir + 'test_recall.pkl', 'wb') as handle:\n","            pickle.dump(VALIDATION_RECALL, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","        \n","    return"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ajM8QKP5EIMU"},"source":["###2.1. Run tests"]},{"cell_type":"code","metadata":{"id":"xlawQkP3PfhP"},"source":["##################################\n","###          SETTINGS         ####\n","##################################\n","main_dir = \"SAR_swath_images_VV+VH+WS_dilated\"\n","NETWORK = \"ResNet\"     # options: [\"Mobile\", \"ResNet\", \"VGG\"]\n","#MIN_HEIGHT = 288\n","#MIN_WIDTH = 288\n","EYE_ONLY = True\n","NUM_VARS = False\n","NORMALISE = True\n","#NORM_MODE = \"model\" # options: [\"model\", \"z-norm\", \"simple\", \"none\"]\n","ROTATE = False\n","#N_CROPS = 3\n","CROP_MODE = \"uniform\" # options: [\"uniform\", \"weighted\"]\n","AUGMENT = True\n","BATCH_SIZE = 8\n","BUFFER_SIZE = 100\n","EPOCHS = 30\n","LEARNING_RATE = 0.00001\n","SPLIT_NUMBER = 5\n","DROPOUT = True\n","DROP_RATE = 0.8\n","##################################\n","\n","# Order of inputs:\n","# main_dir, NETWORK, MIN_HEIGHT, MIN_WIDTH, EYE_ONLY, NUM_VARS, NORMALISE, NORM_MODE, ROTATE, N_CROPS, CROP_MODE, AUGMENT, BATCH_SIZE, BUFFER_SIZE, EPOCHS, LEARNING_RATE, SPLIT_NUMBER, DROPOUT, DROP_RATE\n","\n","#fold_cross_validation(main_dir, NETWORK, 288, 288, EYE_ONLY, NUM_VARS, NORMALISE, \"model\", ROTATE, 1, CROP_MODE, AUGMENT, BATCH_SIZE, BUFFER_SIZE, EPOCHS, LEARNING_RATE, SPLIT_NUMBER, DROPOUT, DROP_RATE)\n","#fold_cross_validation(main_dir, NETWORK, 288, 288, EYE_ONLY, NUM_VARS, NORMALISE, \"model\", ROTATE, 3, CROP_MODE, AUGMENT, BATCH_SIZE, BUFFER_SIZE, EPOCHS, LEARNING_RATE, SPLIT_NUMBER, DROPOUT, DROP_RATE)\n","#fold_cross_validation(main_dir, NETWORK, 288, 288, EYE_ONLY, NUM_VARS, NORMALISE, \"model\", ROTATE, 5, CROP_MODE, AUGMENT, 16, BUFFER_SIZE, EPOCHS, LEARNING_RATE, SPLIT_NUMBER, DROPOUT, DROP_RATE)\n","#fold_cross_validation(main_dir, NETWORK, 288, 288, EYE_ONLY, NUM_VARS, NORMALISE, \"z-norm\", ROTATE, 1, CROP_MODE, AUGMENT, BATCH_SIZE, BUFFER_SIZE, EPOCHS, LEARNING_RATE, SPLIT_NUMBER, DROPOUT, DROP_RATE)\n","#fold_cross_validation(main_dir, NETWORK, 288, 288, EYE_ONLY, NUM_VARS, NORMALISE, \"z-norm\", ROTATE, 3, CROP_MODE, AUGMENT, BATCH_SIZE, BUFFER_SIZE, EPOCHS, LEARNING_RATE, SPLIT_NUMBER, DROPOUT, DROP_RATE)\n","#fold_cross_validation(main_dir, NETWORK, 288, 288, EYE_ONLY, NUM_VARS, NORMALISE, \"z-norm\", ROTATE, 5, CROP_MODE, AUGMENT, 16, BUFFER_SIZE, EPOCHS, LEARNING_RATE, SPLIT_NUMBER, DROPOUT, DROP_RATE)\n","#fold_cross_validation(main_dir, NETWORK, 352, 352, EYE_ONLY, NUM_VARS, NORMALISE, \"model\", ROTATE, 1, CROP_MODE, AUGMENT, BATCH_SIZE, BUFFER_SIZE, EPOCHS, LEARNING_RATE, SPLIT_NUMBER, DROPOUT, DROP_RATE)\n","#fold_cross_validation(main_dir, NETWORK, 352, 352, EYE_ONLY, NUM_VARS, NORMALISE, \"model\", ROTATE, 3, CROP_MODE, AUGMENT, BATCH_SIZE, BUFFER_SIZE, EPOCHS, LEARNING_RATE, SPLIT_NUMBER, DROPOUT, DROP_RATE)\n","#fold_cross_validation(main_dir, NETWORK, 352, 352, EYE_ONLY, NUM_VARS, NORMALISE, \"model\", ROTATE, 5, CROP_MODE, AUGMENT, 16, BUFFER_SIZE, EPOCHS, LEARNING_RATE, SPLIT_NUMBER, DROPOUT, DROP_RATE)\n","#fold_cross_validation(main_dir, NETWORK, 352, 352, EYE_ONLY, NUM_VARS, NORMALISE, \"z-norm\", ROTATE, 1, CROP_MODE, AUGMENT, BATCH_SIZE, BUFFER_SIZE, EPOCHS, LEARNING_RATE, SPLIT_NUMBER, DROPOUT, DROP_RATE)\n","#fold_cross_validation(main_dir, NETWORK, 352, 352, EYE_ONLY, NUM_VARS, NORMALISE, \"z-norm\", ROTATE, 3, CROP_MODE, AUGMENT, BATCH_SIZE, BUFFER_SIZE, EPOCHS, LEARNING_RATE, SPLIT_NUMBER, DROPOUT, DROP_RATE)\n","#fold_cross_validation(main_dir, NETWORK, 352, 352, EYE_ONLY, NUM_VARS, NORMALISE, \"z-norm\", ROTATE, 5, CROP_MODE, AUGMENT, 16, BUFFER_SIZE, EPOCHS, LEARNING_RATE, SPLIT_NUMBER, DROPOUT, DROP_RATE)\n","#fold_cross_validation(main_dir, NETWORK, 384, 384, EYE_ONLY, NUM_VARS, NORMALISE, \"model\", ROTATE, 1, CROP_MODE, AUGMENT, BATCH_SIZE, BUFFER_SIZE, EPOCHS, LEARNING_RATE, SPLIT_NUMBER, DROPOUT, DROP_RATE)\n","#fold_cross_validation(main_dir, NETWORK, 416, 416, EYE_ONLY, NUM_VARS, NORMALISE, \"model\", ROTATE, 1, CROP_MODE, AUGMENT, BATCH_SIZE, BUFFER_SIZE, EPOCHS, LEARNING_RATE, SPLIT_NUMBER, DROPOUT, DROP_RATE)\n","fold_cross_validation(main_dir, NETWORK, 384, 384, EYE_ONLY, NUM_VARS, NORMALISE, \"z-norm\", ROTATE, 1, CROP_MODE, AUGMENT, BATCH_SIZE, BUFFER_SIZE, EPOCHS, LEARNING_RATE, SPLIT_NUMBER, DROPOUT, DROP_RATE)\n","#fold_cross_validation(main_dir, NETWORK, 416, 416, EYE_ONLY, NUM_VARS, NORMALISE, \"z-norm\", ROTATE, 1, CROP_MODE, AUGMENT, BATCH_SIZE, BUFFER_SIZE, EPOCHS, LEARNING_RATE, SPLIT_NUMBER, DROPOUT, DROP_RATE)\n","#fold_cross_validation(main_dir, NETWORK, 384, 384, EYE_ONLY, NUM_VARS, False, \"none\", ROTATE, 1, CROP_MODE, AUGMENT, BATCH_SIZE, BUFFER_SIZE, EPOCHS, LEARNING_RATE, SPLIT_NUMBER, DROPOUT, DROP_RATE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yl9NJGQ2jb-g"},"source":["## 3.Perform Fine-Tuning"]},{"cell_type":"markdown","metadata":{"id":"GPP7a2XY3Atf"},"source":["### 3.1. Using the csv split into train, val and test sets."]},{"cell_type":"code","metadata":{"id":"hEykOjodkO-o"},"source":["##################################\n","###          SETTINGS         ####\n","##################################\n","main_dir = \"SAR_swath_images_VV+VH+WS\"\n","NETWORK = \"ResNet\"     # options: [\"Mobile\", \"ResNet\"]\n","MIN_HEIGHT = 700\n","MIN_WIDTH = 400\n","NUM_VARS = False\n","NORMALISE = True\n","ROTATE = False\n","BATCH_SIZE = 8\n","BUFFER_SIZE = 100\n","EPOCHS = 50\n","LEARNING_RATE = 0.0001\n","##################################\n","\n","# load data\n","train_images, train_labels, train_bbox = helper.load_data(\"{}/csv/training.csv\".format(main_dir))\n","val_images, val_labels, val_bbox = helper.load_data(\"{}/csv/val.csv\".format(main_dir))\n","test_images, test_labels, test_bbox = helper.load_data(\"{}/csv/test.csv\".format(main_dir))\n","class_weights = helper.compute_class_weights(\"{}/csv/full_dataset.csv\".format(main_dir))\n","\n","\n","# create an instance of the DataProcessor\n","processor = dp.DataProcessor(model = NETWORK,\n","                             min_height = MIN_HEIGHT,\n","                             min_width = MIN_WIDTH,\n","                             normalise = NORMALISE,           # perform normalisation\n","                             rotate = ROTATE,                 # perform rotation\n","                             plot_light = False,              # plot only select_crop() images\n","                             plot_extensive = False,          # plot extensively all images\n","                             show_prints = False)\n","\n","\n","# generate datasets\n","train_dataset = helper.prepare_dataset(processor, train_images, train_labels, train_bbox)\n","val_dataset = helper.prepare_dataset(processor, val_images, val_labels, val_bbox)\n","test_dataset = helper.prepare_dataset(processor, test_images, test_labels, test_bbox)\n","#for image, label in train_dataset:\n","#  print(\"FINAL - image: {}, max: {}, min: {}, label: {}\".format(image[0].shape, np.max(image[0]), np.min(image[0]),  label))\n","\n","\n","# configure for performance\n","train_dataset = helper.configure_for_performance(train_dataset, BUFFER_SIZE, BATCH_SIZE)\n","val_dataset = helper.configure_for_performance(val_dataset, BUFFER_SIZE, BATCH_SIZE)\n","test_dataset = helper.configure_for_performance(test_dataset, BUFFER_SIZE, BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JPNL_CdNjdpK"},"source":["# create the base pre-trained model\n","if NETWORK == \"ResNet\":\n","    base_model = models.ResNet50(weights='imagenet', include_top=False, input_shape=(MIN_WIDTH, MIN_HEIGHT, 3))\n","elif NETWORK == \"Mobile\":\n","    base_model = models.MobileNetV2(weights='imagenet', include_top=False, input_shape=(MIN_WIDTH, MIN_HEIGHT, 3))\n","else:\n","  sys.exit(\"Incert valid network model. Options: Mobile or ResNet (case sensitive)\")\n","\n","# Freeze the base_model\n","base_model.trainable = False\n","\n","inputs = Input(shape=(MIN_WIDTH, MIN_HEIGHT, 3))\n","#x = data_augmentation(inputs)  # apply random data augmentation\n","x = base_model(inputs, training = False)\n","x = GlobalAveragePooling2D()(x)\n","outputs = Dense(6, activation=\"softmax\")(x)\n","model = Model(inputs, outputs)\n","\n","# compile the model (should be done *after* setting layers to non-trainable)\n","model.compile(optimizer = Adam(learning_rate = LEARNING_RATE), \n","              loss = \"categorical_crossentropy\",\n","              metrics = [CategoricalAccuracy(name=\"accuracy\"), \n","                          Precision(name=\"precision\"), Recall(name=\"recall\"), \n","                          TruePositives(name='tp'), FalsePositives(name='fp'),\n","                          TrueNegatives(name='tn'), FalseNegatives(name='fn')])\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YcGj-neEjnwo"},"source":["# directory to save results\n","dir = '{}_Numeric-{}_BatchSize-{}_lr-{}_Epochs-{}_Norm-{}_FineTune-T'.format(NETWORK, str(NUM_VARS)[0], str(BATCH_SIZE), str(LEARNING_RATE), str(EPOCHS), str(NORMALISE)[0])\n","save_dir = main_dir + '/classification_results/categorization/' + dir + '/'\n","os.makedirs(save_dir, exist_ok=True)\n","\n","# CREATE CALLBACKS\n","callbacks = [\n","    ModelCheckpoint(save_dir + \"best_model_frozen.h5\", verbose = 1, save_best_only = True),\n","    TensorBoard(log_dir = save_dir + \"logs/\" + datetime.datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"))\n","]\n","\n","initial_epochs = 40\n","\n","# train the top layer of the model on the dataset, the weights\n","# of the pre-trained network will not be updated during training\n","history = model.fit(train_dataset,\n","                    steps_per_epoch = len(train_dataset),\n","                    validation_data = val_dataset,\n","                    validation_steps = len(val_dataset),\n","                    epochs = initial_epochs,\n","                    callbacks = callbacks,\n","                    class_weight = class_weights,\n","                    shuffle = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HTQb0boPjqll"},"source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","fig, (ax1, ax2) = plt.subplots(nrows = 2, ncols = 1, figsize=(10, 10))\n","ax1.plot(acc, label='Training Accuracy')\n","ax1.plot(val_acc, label='Validation Accuracy')\n","ax1.legend(loc='lower right')\n","ax1.set(ylabel = \"Accuracy\",\n","        title = 'Training and Validation Accuracy')\n","#plt.ylim([min(plt.ylim()),1])\n","\n","ax2.plot(loss, label='Training Loss')\n","ax2.plot(val_loss, label='Validation Loss')\n","ax2.legend(loc='upper right')\n","ax2.set(xlabel = 'epoch', \n","        ylabel = 'Cross Entropy',\n","        title = 'Training and Validation Loss')\n","#plt.ylim([0,max(plt.ylim())])\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Dn0K4WwjtF6"},"source":["# unfreeze the whole base model\n","base_model.trainable = True\n","\n","# you should try to fine-tune a small number of top layers rather than the whole model\n","# Let's take a look to see how many layers are in the base model\n","print(\"Number of layers in the base model: \", len(base_model.layers))\n","\n","# Fine-tune from this layer onwards\n","fine_tune_at = 100\n","\n","# Freeze all the layers before the `fine_tune_at` layer\n","for layer in base_model.layers[:fine_tune_at]:\n","  layer.trainable =  False\n","\n","# recompile the model for the modifications to take effect, with a low learning rate\n","model.compile(optimizer = Adam(learning_rate = 1e-5),\n","              loss = \"categorical_crossentropy\",\n","              metrics = [CategoricalAccuracy(name=\"accuracy\"), \n","                          Precision(name=\"precision\"), Recall(name=\"recall\"), \n","                          TruePositives(name='tp'), FalsePositives(name='fp'),\n","                          TrueNegatives(name='tn'), FalseNegatives(name='fn')])\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g2vnrD8Rj4p-"},"source":["# adjust callbacks\n","callbacks = [\n","    #EarlyStopping(patience = 5, verbose = 1),\n","    ReduceLROnPlateau(factor = 0.1, patience = 5, min_lr = 0.000001, verbose = 1),\n","    ModelCheckpoint(save_dir + \"best_model_fine_tuned.h5\", verbose = 1, save_best_only = True),\n","    TensorBoard(log_dir = save_dir + \"logs/\" + datetime.datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"))\n","]\n","\n","fine_tune_epochs = 10\n","total_epochs = initial_epochs + fine_tune_epochs\n","\n","# train the entire model end-to-end\n","history_fine = model.fit(train_dataset,\n","                         steps_per_epoch = len(train_dataset),\n","                         validation_data = val_dataset,\n","                         validation_steps = len(val_dataset),\n","                         epochs = total_epochs,\n","                         initial_epoch = history.epoch[-1],\n","                         callbacks = callbacks,\n","                         class_weight = class_weights,\n","                         shuffle = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j0FwWQH1j7aW"},"source":["acc += history_fine.history['accuracy']\n","val_acc += history_fine.history['val_accuracy']\n","\n","loss += history_fine.history['loss']\n","val_loss += history_fine.history['val_loss']\n","\n","\n","fig, (ax1, ax2) = plt.subplots(nrows = 2, ncols = 1, figsize=(10, 10))\n","ax1.plot(acc, label='Training Accuracy')\n","ax1.plot(val_acc, label='Validation Accuracy')\n","#plt.ylim([min(plt.ylim()),1])\n","ax1.plot([initial_epochs-1,initial_epochs-1],\n","          plt.ylim(), label='Start Fine Tuning')\n","ax1.legend(loc='lower right')\n","ax1.set(ylabel = \"Accuracy\",\n","        title = 'Training and Validation Accuracy')\n","\n","ax2.plot(loss, label='Training Loss')\n","ax2.plot(val_loss, label='Validation Loss')\n","#plt.ylim([0, 1.0])\n","ax2.plot([initial_epochs-1,initial_epochs-1],\n","         plt.ylim(), label='Start Fine Tuning')\n","ax2.legend(loc='upper right')\n","ax2.set(xlabel = 'epoch', \n","      ylabel = 'Cross Entropy',\n","      title = 'Training and Validation Loss')\n","plt.show()\n","\n","fig.savefig(save_dir + 'Learning_curves.png', bbox_inches='tight')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gP4_1B_sj_Jy"},"source":["# EVALUATE MODEL\n","print(\"Loaded best weights of the training\")\n","model.load_weights(save_dir + \"best_model_fine_tuned.h5\")\n","\n","results = model.evaluate(test_dataset, \n","                         steps = len(test_dataset), \n","                         verbose = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F3EqPAWN3Kbl"},"source":["### 3.2. Using Stratified-K Fold."]},{"cell_type":"code","metadata":{"id":"0hOERchi3OA3"},"source":["# STRATIFIED CROSS VALIDATION WITH TRANSFER LEARNING AND FINE-TUNING\n","def fold_cross_validation_ft(main_dir, NETWORK, MIN_HEIGHT, MIN_WIDTH, EYE_ONLY, NUM_VARS, NORMALISE, NORM_MODE, ROTATE, N_CROPS, CROP_MODE, AUGMENT, \n","                             BATCH_SIZE, BUFFER_SIZE, INITIAL_EPOCHS, FINE_TUNE_EPOCHS, LEARNING_RATE, SPLIT_NUMBER, FINE_TUNE_AT, DROPOUT, DROP_RATE):\n","\n","    full_dataset_path = \"{}/csv/full_dataset.csv\".format(main_dir)\n","    df = pd.read_csv(full_dataset_path, converters={'bbox_shape': eval}).dropna()\n","    print(\"Dataset dimension: {}\".format(len(df)))\n","\n","    # COMPUTE CLASS WEIGHTS\n","    # if CROP_MODE == weighted (proporional to presence of classes), helper.compute_class_weights()\n","    # will no longer represent the true distribution of classes in the dataset\n","    class_weights = helper.compute_class_weights(full_dataset_path, eye_only = EYE_ONLY) if CROP_MODE != \"weighted\" else None\n","\n","    # extract non-categorical labels for StratifiedKFold\n","    #Y = df[\"label\"]\n","    Y = np.zeros(len(df), dtype=int)\n","    for index, row in df.iterrows():\n","        if df[\"label\"][index] != 0:\n","            cat = df[\"image\"][index].split('/')[1]\n","            Y[index] = int(cat[-1])\n","\n","    if EYE_ONLY:\n","        assert isinstance(Y, np.ndarray)\n","        idx = np.where(Y == 0)[0].tolist()\n","        Y = np.delete(Y, idx)\n","        Y -= 1\n","        df.drop(idx, inplace = True)\n","        df.reset_index(drop=True, inplace=True)\n","        #print(df)\n","        print(\"New dimensions, Y: {} and df: {}\".format(len(Y), len(df)))\n","    \n","    VALIDATION_ACCURACY = []\n","    VALIDATION_TOP2_ACCURACY = []\n","    VALIDATION_LOSS = []\n","    VALIDATION_TP = []\n","    VALIDATION_FP = []\n","    VALIDATION_TN = []\n","    VALIDATION_FN = []\n","    VALIDATION_PRECISION = []\n","    VALIDATION_RECALL = []\n","\n","    # directory to save results\n","    total_epochs = INITIAL_EPOCHS + FINE_TUNE_EPOCHS\n","    dir = '{}_nu-{}_bs-{}_{}x{}_lr-{}_ep-{}_sp-{}_no-{}{}_cr-{}{}_ag-{}_drp-{}{}_ft-{}'.format(\n","        NETWORK, str(NUM_VARS)[0], BATCH_SIZE, MIN_HEIGHT, MIN_WIDTH, LEARNING_RATE, total_epochs, SPLIT_NUMBER, \n","        NORM_MODE[0], str(NORMALISE)[0], CROP_MODE[0], N_CROPS, str(AUGMENT)[0], str(DROPOUT)[0], DROP_RATE, FINE_TUNE_AT)\n","    save_dir = main_dir + '/classification_results/categorization/test_eye_only/' + dir + '/'\n","\n","    \n","    # create an instance of the DataProcessor\n","    processor = dp.DataProcessor(model = NETWORK,\n","                                 min_height = MIN_HEIGHT,\n","                                 min_width = MIN_WIDTH,\n","                                 normalise = NORMALISE,           # perform normalisation\n","                                 rotate = ROTATE,                 # perform rotation\n","                                 plot_light = False,              # plot only select_crop() images\n","                                 plot_extensive = False,          # plot extensively all images\n","                                 show_prints = False)\n","\n","\n","    print(\"Entering in k fold cross validation...\")\n","    stratified_k_fold = StratifiedKFold(n_splits = SPLIT_NUMBER, shuffle = False)    \n","    fold_var = 1\n","\n","    for train_index, val_index in stratified_k_fold.split(np.zeros(len(df)), Y):\n","        training_data = df.iloc[train_index]\n","        validation_data = df.iloc[val_index]\n","\n","        # LOAD DATA\n","        train_images, train_labels, train_bbox = helper.load_data(df = training_data, eye_only = EYE_ONLY)\n","        val_images, val_labels, val_bbox = helper.load_data(df = validation_data, eye_only = EYE_ONLY)\n","        \n","        # GENERATE DATASETS\n","        #train_dataset = helper.prepare_dataset(processor, train_images, train_labels, train_bbox)\n","        #val_dataset = helper.prepare_dataset(processor, val_images, val_labels, val_bbox)\n","        train_dataset = helper.create_dataset(processor, train_images, train_labels, train_bbox, N_CROPS, CROP_MODE, MIN_HEIGHT, MIN_WIDTH)\n","        val_dataset = helper.create_dataset(processor, val_images, val_labels, val_bbox, 1, CROP_MODE, MIN_HEIGHT, MIN_WIDTH)\n","\n","        # PERFORM NORMALISATION\n","        if NORMALISE:\n","            train_norm_dataset, val_norm_dataset = helper.normalisation(train_dataset, val_dataset, mode = NORM_MODE, model = NETWORK)\n","        else:\n","            train_norm_dataset = train_dataset\n","            val_norm_dataset = val_dataset\n","        unbatched_val_norm_dataset = val_norm_dataset\n","\n","        # CONFIGURE FOR PERFORMANCE\n","        SQUARED = True if MIN_HEIGHT == MIN_WIDTH else False\n","        train_norm_dataset = helper.configure_for_performance(train_norm_dataset, BUFFER_SIZE, BATCH_SIZE, shuffle = True, augment = AUGMENT, squared_input = SQUARED)\n","        val_norm_dataset = helper.configure_for_performance(val_norm_dataset, BUFFER_SIZE, BATCH_SIZE)\n","        \n","        # CREATE BASE PRE-TRAINED MODEL\n","        if NETWORK == \"ResNet\":\n","            base_model = models.ResNet50(weights='imagenet', include_top=False, input_shape=(MIN_WIDTH, MIN_HEIGHT, 3))\n","        elif NETWORK == \"Mobile\":\n","            base_model = models.MobileNetV2(weights='imagenet', include_top=False, input_shape=(MIN_WIDTH, MIN_HEIGHT, 3))\n","        else:\n","            sys.exit(\"Incert valid network model. Options: Mobile or ResNet (case sensitive)\")\n","\n","        # FREEZE BASE MODEL\n","        base_model.trainable = False\n","\n","        inputs = Input(shape=(MIN_WIDTH, MIN_HEIGHT, 3))\n","        x = base_model(inputs, training = False)\n","        x = GlobalAveragePooling2D()(x)\n","        if DROPOUT:\n","            x = Dropout(DROP_RATE)(x)\n","        classes = 6 if not EYE_ONLY else 5\n","        outputs = Dense(classes, activation=\"softmax\")(x)\n","        model = Model(inputs, outputs)\n","\n","        # compile the model (should be done *after* setting layers to non-trainable)\n","        model.compile(optimizer = Adam(learning_rate = LEARNING_RATE), \n","                      loss = \"categorical_crossentropy\",\n","                      metrics = [CategoricalAccuracy(name=\"accuracy\"), TopKCategoricalAccuracy(k=2, name=\"top2_accuracy\"),\n","                                 Precision(name=\"precision\"), Recall(name=\"recall\"), \n","                                 TruePositives(name='tp'), FalsePositives(name='fp'),\n","                                 TrueNegatives(name='tn'), FalseNegatives(name='fn')])\n","\n","        # CREATE CALLBACKS\n","        callbacks = [\n","            ModelCheckpoint(save_dir + \"best_model_frozen_{}.h5\".format(str(fold_var)), verbose = 1, save_best_only = True),\n","            #TensorBoard(log_dir = save_dir + \"logs/\" + datetime.datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"))\n","        ]\n","\n","        # FIT THE MODEL\n","        # train the top layer of the model on the dataset, the weights\n","        # of the pre-trained network will not be updated during training\n","        history = model.fit(train_norm_dataset,\n","                            steps_per_epoch = len(train_norm_dataset),\n","                            validation_data = val_norm_dataset,\n","                            validation_steps = len(val_norm_dataset),\n","                            epochs = INITIAL_EPOCHS,\n","                            callbacks = callbacks,\n","                            class_weight = class_weights,\n","                            shuffle = True)\n","\n","        acc = history.history['accuracy']\n","        val_acc = history.history['val_accuracy']\n","\n","        loss = history.history['loss']\n","        val_loss = history.history['val_loss']\n","\n","        # UNFREEZE BASE MODEL\n","        base_model.trainable = True\n","\n","        # Fine-tune from this layer onwards\n","        if isinstance(FINE_TUNE_AT, int):\n","            # Freeze all the layers before the 'FINE_TUNE_AT' layer\n","            for layer in base_model.layers[:FINE_TUNE_AT]:\n","                layer.trainable = False\n","            \n","        else: #string\n","            if \"last\" in FINE_TUNE_AT:\n","                nb_layers_to_fine_tune = int(re.findall(r'\\d+', FINE_TUNE_AT)[0])\n","                print(\"nb_layers_to_fine_tune:\", nb_layers_to_fine_tune)\n","                total_to_freeze = len(base_model.layers) - nb_layers_to_fine_tune\n","                print(\"total_to_freeze:\", total_to_freeze)\n","                for layer in base_model.layers[:total_to_freeze]:\n","                    layer.trainable = False\n","\n","\n","        # recompile the model for the modifications to take effect, with a low learning rate\n","        model.compile(optimizer = Adam(learning_rate = 1e-5),\n","                      loss = \"categorical_crossentropy\",\n","                      metrics = [CategoricalAccuracy(name=\"accuracy\"), TopKCategoricalAccuracy(k=2, name=\"top2_accuracy\"),\n","                                 Precision(name=\"precision\"), Recall(name=\"recall\"), \n","                                 TruePositives(name='tp'), FalsePositives(name='fp'),\n","                                 TrueNegatives(name='tn'), FalseNegatives(name='fn')])\n","        #base_model.summary()\n","        \n","        # adjust callbacks\n","        callbacks = [\n","            ModelCheckpoint(save_dir + \"best_model_fine_tuned_{}.h5\".format(str(fold_var)), verbose = 1, save_best_only = True),\n","            #TensorBoard(log_dir = save_dir + \"logs/\" + datetime.datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"))\n","        ]\n","\n","        # train the entire model end-to-end\n","        history_fine = model.fit(train_norm_dataset,\n","                                 steps_per_epoch = len(train_norm_dataset),\n","                                 validation_data = val_norm_dataset,\n","                                 validation_steps = len(val_norm_dataset),\n","                                 epochs = total_epochs,\n","                                 initial_epoch = history.epoch[-1],\n","                                 callbacks = callbacks,\n","                                 class_weight = class_weights,\n","                                 shuffle = True)\n","\n","        acc += history_fine.history['accuracy']\n","        val_acc += history_fine.history['val_accuracy']\n","\n","        loss += history_fine.history['loss']\n","        val_loss += history_fine.history['val_loss']\n","\n","        # PLOT TRAIN/VALIDATION LOSSES\n","        fig, (ax1, ax2) = plt.subplots(nrows = 2, ncols = 1, figsize=(10, 10))\n","        ax1.plot(acc, label='Training Accuracy')\n","        ax1.plot(val_acc, label='Validation Accuracy')\n","        #plt.ylim([min(plt.ylim()),1])\n","        ax1.plot([INITIAL_EPOCHS-1, INITIAL_EPOCHS-1], plt.ylim(), label='Start Fine Tuning')\n","        ax1.grid(True)\n","        ax1.legend(loc='lower right')\n","        ax1.set(ylabel = \"Accuracy\",\n","                title = 'Training and Validation Accuracy')\n","\n","        ax2.plot(loss, label='Training Loss')\n","        ax2.plot(val_loss, label='Validation Loss')\n","        #plt.ylim([0, 1.0])\n","        ax2.plot([INITIAL_EPOCHS-1, INITIAL_EPOCHS-1], plt.ylim(), label='Start Fine Tuning')\n","        ax2.plot(np.argmin(val_loss), np.min(val_loss), marker=\"x\", color=\"r\", label = \"Best model\")\n","        ax2.grid(True)\n","        ax2.legend(loc='upper right')\n","        ax2.set(xlabel = 'Epoch',\n","                ylabel = 'Cross Entropy',\n","                title = 'Training and Validation Loss')\n","        plt.show()\n","        fig.savefig(save_dir + \"model_\" + str(fold_var) + \".jpg\", bbox_inches='tight')\n","        \n","        ##################################################################################################\n","        \n","\n","        # LOAD BEST MODEL\n","        time.sleep(15) # guarantees enough time for weights to be saved and loaded afterwards, otherwise gives concurrency problems\n","        print(\"Loaded best weights of the training\")\n","        model.load_weights(save_dir + \"best_model_fine_tuned_{}.h5\".format(str(fold_var)))\n","\n","        # EVALUATE PERFORMANCE of the model\n","        results = model.evaluate(val_norm_dataset, \n","                                 steps = len(val_norm_dataset), \n","                                 verbose = 1)\n","\n","        results = dict(zip(model.metrics_names, results))\n","\n","        VALIDATION_ACCURACY.append(results['accuracy'])\n","        VALIDATION_TOP2_ACCURACY.append(results['top2_accuracy'])\n","        VALIDATION_LOSS.append(results['loss'])\n","        VALIDATION_TP.append(results[\"tp\"])\n","        VALIDATION_FP.append(results[\"fp\"])\n","        VALIDATION_TN.append(results[\"tn\"])\n","        VALIDATION_FN.append(results[\"fn\"])\n","        VALIDATION_PRECISION.append(results[\"precision\"])\n","        VALIDATION_RECALL.append(results[\"recall\"])\n","\n","        # MAKE PREDICTIONS\n","        predictions = model.predict(val_norm_dataset)\n","        predictions_non_category = [ np.argmax(t) for t in predictions ]\n","        val_labels_non_category = [ np.argmax(t) for t in val_labels ]\n","\n","        labels = [0,1,2,3,4,5] if not EYE_ONLY else [0,1,2,3,4]\n","        display_labels = labels if not EYE_ONLY else [x+1 for x in labels]\n","        conf_mat = confusion_matrix(val_labels_non_category, predictions_non_category, labels = labels)\n","        disp = ConfusionMatrixDisplay(confusion_matrix = conf_mat, display_labels = display_labels)\n","        conf_mat_display = disp.plot()\n","        plt.savefig(save_dir + \"confusion_matrix_\" + str(fold_var) + \".jpg\", bbox_inches='tight')\n","\n","        # GRAD-CAM ANALYSIS\n","        gradcam_path = save_dir + \"gradcam_heatmaps_{}\".format(fold_var)\n","        os.makedirs(gradcam_path, exist_ok=True)\n","        helper.grad_cam(model, val_dataset, unbatched_val_norm_dataset, predictions, gradcam_path, dropout = DROPOUT, fine_tuning = True, network = NETWORK)\n","\n","        tf.keras.backend.clear_session()\n","\n","        fold_var += 1\n","\n","        # save the values for each fold\n","        csv_dir = save_dir + 'csv/'\n","        os.makedirs(csv_dir, exist_ok=True)\n","        with open(csv_dir + 'test_accuracy.pkl', 'wb') as handle:\n","            pickle.dump(VALIDATION_ACCURACY, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","        with open(csv_dir + 'test_top2_accuracy.pkl', 'wb') as handle:\n","            pickle.dump(VALIDATION_TOP2_ACCURACY, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","        with open(csv_dir + 'test_loss.pkl', 'wb') as handle:\n","            pickle.dump(VALIDATION_LOSS, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","        with open(csv_dir + 'test_tp.pkl', 'wb') as handle:\n","            pickle.dump(VALIDATION_TP, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","        with open(csv_dir + 'test_fp.pkl', 'wb') as handle:\n","            pickle.dump(VALIDATION_FP, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","        with open(csv_dir + 'test_tn.pkl', 'wb') as handle:\n","            pickle.dump(VALIDATION_TN, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","        with open(csv_dir + 'test_fn.pkl', 'wb') as handle:\n","            pickle.dump(VALIDATION_FN, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","        with open(csv_dir + 'test_precision.pkl', 'wb') as handle:\n","            pickle.dump(VALIDATION_PRECISION, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","        with open(csv_dir + 'test_recall.pkl', 'wb') as handle:\n","            pickle.dump(VALIDATION_RECALL, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","    return"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_D-HnbVhEYPS"},"source":["####3.2.1. Run tests"]},{"cell_type":"code","metadata":{"id":"N9_JbXNVsQC6"},"source":["##################################\n","###          SETTINGS         ####\n","##################################\n","main_dir = \"SAR_swath_images_VV+VH+WS_dilated\"\n","NETWORK = \"ResNet\"     # options: [\"Mobile\", \"ResNet\"]\n","#MIN_HEIGHT = 288\n","#MIN_WIDTH = 288\n","EYE_ONLY = True\n","NUM_VARS = False\n","NORMALISE = True\n","#NORM_MODE = \"model\" # options: [\"model\", \"z-norm\", \"simple\"]\n","ROTATE = False\n","#N_CROPS = 3\n","CROP_MODE = \"uniform\" # options: [\"uniform\", \"weighted\"]\n","AUGMENT = True\n","BATCH_SIZE = 8\n","BUFFER_SIZE = 100\n","INITIAL_EPOCHS = 30\n","FINE_TUNE_EPOCHS = 10\n","LEARNING_RATE = 0.0001\n","SPLIT_NUMBER = 5\n","FINE_TUNE_AT = \"last5\"  # options: int() - Layer to start fine-tuning from;\n","                        #          str() - Number of last layers to fine-tune: \"lastX\"\n","DROPOUT = True\n","DROP_RATE = 0.5                     \n","##################################\n","\n","# Order of inputs:\n","#main_dir, NETWORK, MIN_HEIGHT, MIN_WIDTH, EYE_ONLY, NUM_VARS, NORMALISE, NORM_MODE, ROTATE, N_CROPS, CROP_MODE, AUGMENT, BATCH_SIZE, BUFFER_SIZE, INITIAL_EPOCHS, FINE_TUNE_EPOCHS, LEARNING_RATE, SPLIT_NUMBER, FINE_TUNE_AT, DROPOUT, DROP_RATE\n","\n","# Tests:\n","#fold_cross_validation_ft(main_dir, NETWORK, 384, 384, EYE_ONLY, NUM_VARS, NORMALISE, \"model\", ROTATE, 1, CROP_MODE, AUGMENT, BATCH_SIZE, BUFFER_SIZE, INITIAL_EPOCHS, FINE_TUNE_EPOCHS, LEARNING_RATE, SPLIT_NUMBER, FINE_TUNE_AT, DROPOUT, DROP_RATE)\n","#fold_cross_validation_ft(main_dir, NETWORK, 384, 384, EYE_ONLY, NUM_VARS, NORMALISE, \"model\", ROTATE, 3, CROP_MODE, AUGMENT, BATCH_SIZE, BUFFER_SIZE, INITIAL_EPOCHS, FINE_TUNE_EPOCHS, LEARNING_RATE, SPLIT_NUMBER, FINE_TUNE_AT, DROPOUT, DROP_RATE)\n","#fold_cross_validation_ft(main_dir, NETWORK, 384, 384, EYE_ONLY, NUM_VARS, NORMALISE, \"model\", ROTATE, 5, CROP_MODE, AUGMENT, 16, BUFFER_SIZE, INITIAL_EPOCHS, FINE_TUNE_EPOCHS, LEARNING_RATE, SPLIT_NUMBER, FINE_TUNE_AT, DROPOUT, DROP_RATE)\n","fold_cross_validation_ft(main_dir, NETWORK, 384, 384, EYE_ONLY, NUM_VARS, NORMALISE, \"z-norm\", ROTATE, 1, CROP_MODE, AUGMENT, BATCH_SIZE, BUFFER_SIZE, INITIAL_EPOCHS, FINE_TUNE_EPOCHS, LEARNING_RATE, SPLIT_NUMBER, FINE_TUNE_AT, DROPOUT, DROP_RATE)\n","#fold_cross_validation_ft(main_dir, NETWORK, 384, 384, EYE_ONLY, NUM_VARS, NORMALISE, \"z-norm\", ROTATE, 3, CROP_MODE, AUGMENT, BATCH_SIZE, BUFFER_SIZE, INITIAL_EPOCHS, FINE_TUNE_EPOCHS, LEARNING_RATE, SPLIT_NUMBER, FINE_TUNE_AT, DROPOUT, DROP_RATE)\n","#fold_cross_validation_ft(main_dir, NETWORK, 384, 384, EYE_ONLY, NUM_VARS, NORMALISE, \"z-norm\", ROTATE, 5, CROP_MODE, AUGMENT, 16, BUFFER_SIZE, INITIAL_EPOCHS, FINE_TUNE_EPOCHS, LEARNING_RATE, SPLIT_NUMBER, FINE_TUNE_AT, DROPOUT, DROP_RATE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9ht1Z-KOohzD"},"source":["## 4.Test random things"]},{"cell_type":"code","metadata":{"id":"CVGvPi-nodHv"},"source":["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","val_labels1 = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4,5,5]\n","predictions1 =[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,2,2,3,1,1,1,3,4,0,3,3,3,4,3,3,4,4,4,4,5]\n","\n","val_labels2 = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5]\n","predictions2 =[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,3,1,2,2,2,3,1,1,2,3,3,3,1,1,2,2,3,2,4]\n","\n","val_labels3 = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4,4,5]\n","predictions3 =[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,0,1,1,1,1,2,2,2,3,4,4,0,0,0,1,2,1,2,2,3,4,1,4,4,4,5,5,4]\n","\n","val_labels4 = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4,5,5]\n","predictions4 =[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,2,1,1,1,1,1,1,1,2,3,3,1,1,1,3,3,2,2,3,4,4,1,1,4,4,4,0,3]\n","\n","val_labels5 = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4,5,5]\n","predictions5 =[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,3,4,0,1,1,1,3,1,1,3,3,3,1,3,4,4,4,0,3]\n","\n","\n","val_labels = val_labels1 + val_labels2 + val_labels3 + val_labels4 + val_labels5\n","predictions = predictions1 + predictions2 + predictions3 + predictions4 + predictions5\n","\n","conf_mat = confusion_matrix(val_labels5, predictions5, labels = [0,1,2,3,4,5])\n","disp = ConfusionMatrixDisplay(confusion_matrix = conf_mat, display_labels = [0,1,2,3,4,5])\n","conf_mat_display = disp.plot()\n","\n","import matplotlib.pyplot as plt\n","#plt.savefig(\"SAR_swath_images_VV+VH+WS/classification_results/categorization/ResNet_Numeric-F_BatchSize-8_lr-0.0001_Epochs-20_Folds-5_Norm-T_Aug-T/global_confusion_matrix.jpg\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yw_oMLfDi1km"},"source":["##################################\n","###          SETTINGS         ####\n","##################################\n","main_dir = \"SAR_swath_images_VV+VH+WS_dilated\"\n","NETWORK = \"Mobile\"     # options: [\"Mobile\", \"ResNet\"]\n","#MIN_HEIGHT = 288\n","#MIN_WIDTH = 288\n","#NORM_MODE = \"model\" # options: [\"model\", \"z-norm\", \"simple\"]\n","#N_CROPS = 3\n","\n","CROP_MODE = \"uniform\" # options: [\"uniform\", \"weighted\"]\n","NUM_VARS = False\n","NORMALISE = True\n","ROTATE = False\n","AUGMENT = True\n","SPLIT_NUMBER = 5\n","##################################\n","\n","\n","def test_normalisation_modes(main_dir, NETWORK, MIN_HEIGHT, MIN_WIDTH, NUM_VARS, NORMALISE, NORM_MODE, ROTATE, N_CROPS, \n","                          CROP_MODE, AUGMENT, SPLIT_NUMBER):\n","    \n","    full_dataset_path = \"{}/csv/full_dataset.csv\".format(main_dir)\n","    df = pd.read_csv(full_dataset_path, converters={'bbox_shape': eval}).dropna()\n","    print(\"Dataset dimension: {}\".format(len(df)))\n","\n","    # COMPUTE CLASS WEIGHTS\n","    # if CROP_MODE == weighted (proporional to presence of classes), helper.compute_class_weights()\n","    # will no longer represent the true distribution of classes in the dataset\n","    class_weights = helper.compute_class_weights(full_dataset_path) if CROP_MODE != \"weighted\" else None\n","\n","    # extract non-categorical labels for StratifiedKFold\n","    #Y = df[\"label\"]\n","    Y = np.zeros(len(df), dtype=int)\n","    for index, row in df.iterrows():\n","        if df[\"label\"][index] != 0:\n","            cat = df[\"image\"][index].split('/')[1]\n","            Y[index] = int(cat[-1])\n","\n","\n","    # create an instance of the DataProcessor\n","    processor = dp.DataProcessor(model = NETWORK,\n","                                min_height = MIN_HEIGHT,\n","                                min_width = MIN_WIDTH,\n","                                normalise = NORMALISE,           # perform normalisation [DEPRECATED]\n","                                rotate = ROTATE,                 # perform rotation\n","                                plot_light = False,              # plot only select_crop() images\n","                                plot_extensive = False,          # plot extensively all images\n","                                show_prints = False)\n","\n","\n","    print(\"Entering in k fold cross validation...\")\n","    stratified_k_fold = StratifiedKFold(n_splits = SPLIT_NUMBER, shuffle = False)\n","    fold_var = 1\n","\n","    for train_index, val_index in stratified_k_fold.split(np.zeros(len(df)), Y):\n","        training_data = df.iloc[train_index]\n","        validation_data = df.iloc[val_index]\n","\n","        # LOAD DATA\n","        train_images, train_labels, train_bbox = helper.load_data(df = training_data)\n","        val_images, val_labels, val_bbox = helper.load_data(df = validation_data)\n","        \n","        # GENERATE DATASETS\n","        #train_dataset = helper.prepare_dataset(processor, train_images, train_labels, train_bbox)\n","        #val_dataset = helper.prepare_dataset(processor, val_images, val_labels, val_bbox)\n","        train_dataset = helper.create_dataset(processor, train_images, train_labels, train_bbox, N_CROPS, CROP_MODE, MIN_HEIGHT, MIN_WIDTH)\n","        val_dataset = helper.create_dataset(processor, val_images, val_labels, val_bbox, 1, CROP_MODE, MIN_HEIGHT, MIN_WIDTH)\n","        # since we increased dataset size with more crops, val_labels also increased\n","        #val_labels = val_dataset.map(lambda x, y: y)\n","\n","        # PERFORM NORMALISATION\n","        if NORMALISE:\n","            train_norm_dataset, val_norm_dataset = helper.normalisation(train_dataset, val_dataset, mode = NORM_MODE, model = NETWORK)\n","\n","        train_norm_images = train_norm_dataset.map(lambda x, y: x)\n","        train_images = train_dataset.map(lambda x, y: x)\n","\n","        cnt = 0\n","        dir = \"{}/model_norm_2/{}\".format(main_dir, fold_var)\n","        os.makedirs(dir, exist_ok=True)\n","        min = np.inf\n","        max = 0\n","        for norm_img, img in zip(train_norm_images, train_images):\n","            \n","            arr = norm_img.numpy()\n","            print(\"Norm image range:\", np.max(arr), np.min(arr))\n","            #if np.max(arr) > max:\n","            #    max = np.max(arr)\n","            #if np.min(arr) < min:\n","            #    min = np.min(arr)\n","            #new_arr = ((arr - arr.min()) * (1/(arr.max() - arr.min()) * 255)).astype('uint8')\n","            #print(np.max(new_arr), np.min(new_arr))\n","            #imstack = np.hstack((img.numpy(), new_arr))\n","            #cv2_imshow(imstack)\n","\n","            #print(\"------------------------------------------------------\")\n","            #cnt += 1\n","            #cv2.imwrite(\"{}/image_{}.png\".format(dir, cnt), imstack)\n","            #im = Image.fromarray(imstack, 'RGB')\n","            #im.save(\"{}/image_{}.png\".format(dir, cnt))\n","            #plt.imshow(imstack)\n","            #plt.savefig(\"{}/image_{}.png\".format(dir, cnt))\n","            #matplotlib.image.imsave(\"{}/image_{}.png\".format(dir, cnt), imstack)\n","\n","        fold_var += 1\n","\n","        if fold_var > 1:\n","            print(min, max)\n","            import sys\n","            sys.exit()\n","\n","\n","test_normalisation_modes(main_dir, NETWORK, 288, 288, NUM_VARS, NORMALISE, \"z-norm\", ROTATE, 1, CROP_MODE, AUGMENT, SPLIT_NUMBER)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"757T18SjhQHW"},"source":["zero = tf.constant(0, dtype = tf.float32)\n","print(\"zero:\", zero)\n","nan = tf.constant(np.nan, dtype = tf.float32)\n","print(\"nan:\", nan)\n","A = tf.convert_to_tensor(np.eye(6), dtype = tf.float32)\n","print(\"A:\", A)\n","img = tf.where(tf.equal(A, zero), nan, A)\n","print(\"img:\", img)\n","\n","mean = 4\n","std = 3\n","aux = (img - mean)/std\n","print(\"aux:\", aux)\n","new = tf.where(tf.math.is_nan(aux), tf.zeros_like(aux), aux)\n","print(\"new:\", new)"],"execution_count":null,"outputs":[]}]}