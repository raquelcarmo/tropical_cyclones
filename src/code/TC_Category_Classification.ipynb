{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TC_Category_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sUEMhiDjZ9bi",
        "GPP7a2XY3Atf",
        "F3EqPAWN3Kbl",
        "9ht1Z-KOohzD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raquelcarmo/tropical_cyclones/blob/import-py-files/src/code/TC_Category_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jnxs1L1PWjeL",
        "outputId": "e39aa819-c697-4ac8-ffeb-f445b34f9d88"
      },
      "source": [
        "# Load the Drive helper and mount\r\n",
        "from google.colab import drive\r\n",
        "\r\n",
        "# This will prompt for authorization.\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNUcV31BIHhF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0a15176-bf59-4e7f-9728-0230099c0c56"
      },
      "source": [
        "%cd /content/drive/My Drive/ESRIN_PhiLab/Tropical_Cyclones\r\n",
        "%ls\r\n",
        "\r\n",
        "import imp \r\n",
        "# import helper.py\r\n",
        "helper = imp.new_module('helper_functions')\r\n",
        "exec(open(\"./helper_functions.py\").read(), helper.__dict__)\r\n",
        "# import models.py\r\n",
        "models = imp.new_module('models')\r\n",
        "exec(open(\"./models.py\").read(), models.__dict__)\r\n",
        "# import data_processor.py\r\n",
        "dp = imp.new_module('data_processor')\r\n",
        "exec(open(\"./data_processor.py\").read(), dp.__dict__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/ESRIN_PhiLab/Tropical_Cyclones\n",
            " \u001b[0m\u001b[01;34mdata\u001b[0m/                             'TC_downloader_&_converter.ipynb'\n",
            " data_processor.py                  TC_Eye_Detection.ipynb\n",
            " helper_functions.py                TC_GradCAM.ipynb\n",
            " \u001b[01;34mMeetings\u001b[0m/                          TC_model_comparison.ipynb\n",
            " models.py                          TC_parametric_model.ipynb\n",
            " TC_best_tracks.ipynb               TC_Vmax_Regression.ipynb\n",
            " TC_Category_Classification.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4p-mSrzKWqUb",
        "outputId": "55640955-3219-4517-d7c1-edc3ddbf9fa2"
      },
      "source": [
        "# insert your desired path to work on\r\n",
        "%cd /content/drive/My Drive/ESRIN_PhiLab/Tropical_Cyclones/data\r\n",
        "%ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/ESRIN_PhiLab/Tropical_Cyclones/data\n",
            "\u001b[0m\u001b[01;34mbest_track\u001b[0m/                            \u001b[01;36mSAR_swath_images\u001b[0m@\n",
            "categorisation.png                     \u001b[01;34mSAR_swath_images_VV+VH+VH\u001b[0m/\n",
            "\u001b[01;34mclassification_results\u001b[0m/                \u001b[01;34mSAR_swath_images_VV+VH+WS\u001b[0m/\n",
            "identification.png                     \u001b[01;34mSAR_swath_images_WS+sWSO+cWSO\u001b[0m/\n",
            "\u001b[01;34mmodel_comparisons_between_datasets\u001b[0m/    \u001b[01;34mSAR_swath_images_WS+WS+WS\u001b[0m/\n",
            "model.png                              \u001b[01;34mSAR_swath_masks\u001b[0m/\n",
            "\u001b[01;34mparametric_model\u001b[0m/                      \u001b[01;34mSAR_swath_nc\u001b[0m/\n",
            "\u001b[01;36mSAR_IMAGE_DATASET_EYE_CLASSIFICATION\u001b[0m@  \u001b[01;34mSAR_swath_Vmax\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZu5fkuXWxdC"
      },
      "source": [
        "# general imports\r\n",
        "import random\r\n",
        "import glob\r\n",
        "import os\r\n",
        "import sys\r\n",
        "sys.stdout.flush()\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import cv2\r\n",
        "import matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.cm as cm\r\n",
        "import math\r\n",
        "import imageio\r\n",
        "import os.path\r\n",
        "import time\r\n",
        "from PIL import Image\r\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\r\n",
        "from scipy import ndimage\r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "import random\r\n",
        "from shapely.geometry import Point\r\n",
        "import re\r\n",
        "import pickle\r\n",
        "import scipy\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\r\n",
        "import datetime\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "from tensorflow.data import Dataset\r\n",
        "from tensorflow.keras import Input\r\n",
        "from tensorflow.keras.applications import resnet50, mobilenet_v2\r\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\r\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras.layers import concatenate, Dense, GlobalAveragePooling2D\r\n",
        "from tensorflow.keras.optimizers import SGD, Adam\r\n",
        "from tensorflow.keras.metrics import CategoricalAccuracy, Precision, Recall, TruePositives, FalsePositives, TrueNegatives, FalseNegatives\r\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\r\n",
        "\r\n",
        "np.set_printoptions(precision=4)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUEMhiDjZ9bi"
      },
      "source": [
        "## Train on data according to csv split into train, val and test sets.\r\n",
        "\r\n",
        "Prepare the tf.data.Dataset instances to be fed to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySpk4sOVemVz"
      },
      "source": [
        "##################################\r\n",
        "###          SETTINGS         ####\r\n",
        "##################################\r\n",
        "main_dir = \"SAR_swath_images_VV+VH+WS\"\r\n",
        "NETWORK = \"ResNet\"     # options: [\"Mobile\", \"ResNet\"]\r\n",
        "MIN_HEIGHT = 288\r\n",
        "MIN_WIDTH = 288\r\n",
        "NUM_VARS = False\r\n",
        "NORMALISE = True\r\n",
        "ROTATE = False\r\n",
        "BATCH_SIZE = 8\r\n",
        "BUFFER_SIZE = 100\r\n",
        "EPOCHS = 20\r\n",
        "LEARNING_RATE = 0.0001\r\n",
        "##################################\r\n",
        "\r\n",
        "# load data\r\n",
        "train_images, train_labels, train_bbox = helper.load_data_with_cat(\"{}/csv/training.csv\".format(main_dir))\r\n",
        "val_images, val_labels, val_bbox = helper.load_data_with_cat(\"{}/csv/val.csv\".format(main_dir))\r\n",
        "test_images, test_labels, test_bbox = helper.load_data_with_cat(\"{}/csv/test.csv\".format(main_dir))\r\n",
        "class_weights = helper.compute_class_weights(\"{}/csv/full_dataset.csv\".format(main_dir))\r\n",
        "\r\n",
        "\r\n",
        "# create an instance of the DataProcessor\r\n",
        "processor = dp.DataProcessor(model = NETWORK,\r\n",
        "                             min_height = MIN_HEIGHT,\r\n",
        "                             min_width = MIN_WIDTH,\r\n",
        "                             normalise = False,           # perform normalisation\r\n",
        "                             rotate = ROTATE,                 # perform rotation\r\n",
        "                             plot_light = False,              # plot only select_crop() images\r\n",
        "                             plot_extensive = False,          # plot extensively all images\r\n",
        "                             show_prints = False)\r\n",
        "\r\n",
        "\r\n",
        "# generate datasets\r\n",
        "train_dataset = helper.prepare_dataset(processor, train_images, train_labels, train_bbox)\r\n",
        "val_dataset = helper.prepare_dataset(processor, val_images, val_labels, val_bbox)\r\n",
        "test_dataset = helper.prepare_dataset(processor, test_images, test_labels, test_bbox)\r\n",
        "#for image, label in train_dataset:\r\n",
        "#  plt.imshow(image)\r\n",
        "#  plt.show()\r\n",
        "#  print(\"FINAL - image: {}, label: {}\".format(image.shape, label))\r\n",
        "#  print(\"FINAL - image: {}, max: {}, min: {}, label: {}\".format(image.shape, np.max(image), np.min(image),  label))\r\n",
        "\r\n",
        "train_dataset, val_dataset = helper.z_norm(train_dataset, val_dataset)\r\n",
        "#cnt = 0\r\n",
        "#for image, label in train_dataset:\r\n",
        "#  cnt +=1\r\n",
        "#  plt.imshow(image)\r\n",
        "#  plt.show()\r\n",
        "\r\n",
        "# configure for performance\r\n",
        "train_dataset = helper.configure_for_performance(train_dataset, BUFFER_SIZE, BATCH_SIZE, shuffle = True, augment = True)\r\n",
        "val_dataset = helper.configure_for_performance(val_dataset, BUFFER_SIZE, BATCH_SIZE)\r\n",
        "test_dataset = helper.configure_for_performance(test_dataset, BUFFER_SIZE, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9-tkgGrid0-"
      },
      "source": [
        "Script to perform end-to-end training in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0eTiiKghBZg"
      },
      "source": [
        "# directory to save results\r\n",
        "dir = '{}_Numeric-{}_BatchSize-{}_Epochs-{}_Norm-{}'.format(NETWORK, str(NUM_VARS)[0], str(BATCH_SIZE), str(EPOCHS), str(NORMALISE)[0])\r\n",
        "save_dir = 'classification_results/categorization/' + dir + '/'\r\n",
        "os.makedirs(save_dir, exist_ok=True)\r\n",
        "\r\n",
        "# MAKE MODEL\r\n",
        "if NETWORK == \"Mobile\":\r\n",
        "  model = models.make_MobileNet(MIN_WIDTH, MIN_HEIGHT, LEARNING_RATE)\r\n",
        "elif NETWORK == \"ResNet\":\r\n",
        "  model = models.make_ResNet(MIN_WIDTH, MIN_HEIGHT, LEARNING_RATE)\r\n",
        "else:\r\n",
        "  sys.exit(\"Incert valid network model. Options: Mobile or ResNet (case sensitive)\")\r\n",
        "\r\n",
        "# CREATE CALLBACKS\r\n",
        "callbacks = [\r\n",
        "    EarlyStopping(patience = 7, verbose = 1),\r\n",
        "    ReduceLROnPlateau(factor = 0.1, patience = 5, min_lr = 0.00001, verbose = 1),\r\n",
        "    ModelCheckpoint(save_dir + \"best_model.h5\", verbose = 1, save_best_only = True),\r\n",
        "    TensorBoard(log_dir = save_dir + \"logs/\" + datetime.datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"))\r\n",
        "]\r\n",
        "\r\n",
        "# TRAIN THE NETWORK\r\n",
        "H = model.fit(\r\n",
        "    train_dataset,\r\n",
        "    steps_per_epoch = len(train_dataset),\r\n",
        "    validation_data = val_dataset,\r\n",
        "    validation_steps = len(val_dataset),\r\n",
        "    epochs = EPOCHS,\r\n",
        "    callbacks = callbacks,\r\n",
        "    verbose = 1,\r\n",
        "    class_weight = class_weights,\r\n",
        "    shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0wRibZ1YUh-"
      },
      "source": [
        "# Load the TensorBoard notebook extension\r\n",
        "#%load_ext tensorboard\r\n",
        "%reload_ext tensorboard\r\n",
        "%tensorboard --logdir classification_results/categorization/ResNet_Numeric-F_BatchSize-8_Epochs-20_Norm-T/logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBRY3vfnhF-D"
      },
      "source": [
        "# EVALUATE MODEL\r\n",
        "print(\"Loaded best weights of the training\")\r\n",
        "model.load_weights(save_dir + \"best_model.h5\")\r\n",
        "\r\n",
        "results = model.evaluate(test_dataset, \r\n",
        "                         steps = len(test_dataset), \r\n",
        "                         verbose = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGg9tMhEhKut"
      },
      "source": [
        "# MAKE PREDICTIONS\r\n",
        "# Retrieve a batch of images from the test set\r\n",
        "predictions = model.predict(test_dataset.)\r\n",
        "\r\n",
        "predictions = tf.where(predictions < 0.5, 0, 1)\r\n",
        "\r\n",
        "print('Predictions:\\n', predictions.numpy())\r\n",
        "print('Labels:\\n')\r\n",
        "for label in test_labels_dataset:\r\n",
        "  print(label)\r\n",
        "#class_names = {0: \"No eye\", 1: \"Eye\"}\r\n",
        "#plt.figure(figsize=(10, 10))\r\n",
        "#for i in range(9):\r\n",
        "#  ax = plt.subplot(3, 3, i + 1)\r\n",
        "#  plt.imshow(image_batch[i].astype(\"uint8\"))\r\n",
        "#  plt.title(class_names[predictions[i]])\r\n",
        "#  plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V1lTSzbDUyS"
      },
      "source": [
        "## Train on data using the Stratified K Fold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxvI67fsr2zD"
      },
      "source": [
        "##################################\r\n",
        "###          SETTINGS         ####\r\n",
        "##################################\r\n",
        "main_dir = \"SAR_swath_images_VV+VH+WS\"\r\n",
        "NETWORK = \"ResNet\"     # options: [\"Mobile\", \"ResNet\"]\r\n",
        "MIN_HEIGHT = 288\r\n",
        "MIN_WIDTH = 288\r\n",
        "NUM_VARS = False\r\n",
        "NORMALISE = True\r\n",
        "ROTATE = False\r\n",
        "AUGMENT = True\r\n",
        "BATCH_SIZE = 16\r\n",
        "BUFFER_SIZE = 100\r\n",
        "EPOCHS = 3\r\n",
        "FOLDS = 5\r\n",
        "LEARNING_RATE = 0.0001\r\n",
        "##################################\r\n",
        "\r\n",
        "# STRATIFIED CROSS VALIDATION\r\n",
        "def fold_cross_validation(main_dir, NETWORK, MIN_HEIGHT, MIN_WIDTH, NUM_VARS, NORMALISE, ROTATE, AUGMENT, LEARNING_RATE, EPOCHS, BUFFER_SIZE, BATCH_SIZE, SPLIT_NUMBER):\r\n",
        "\r\n",
        "  df = pd.read_csv(main_dir + \"/csv/full_dataset.csv\", converters={'bbox_shape': eval}).dropna()\r\n",
        "  class_weights = helper.compute_class_weights(\"{}/csv/full_dataset.csv\".format(main_dir))\r\n",
        "\r\n",
        "  #Y = df[\"label\"]\r\n",
        "  Y = np.zeros(len(df), dtype=int)\r\n",
        "  for index, row in df.iterrows():\r\n",
        "    if df[\"label\"][index] != 0:\r\n",
        "      cat = df[\"image\"][index].split('/')[1]\r\n",
        "      Y[index] = int(cat[-1])\r\n",
        "\r\n",
        "  stratified_k_fold = StratifiedKFold(n_splits = SPLIT_NUMBER, random_state = 42, shuffle = False)\r\n",
        "\r\n",
        "  VALIDATION_ACCURACY = []\r\n",
        "  VALIDATION_LOSS = []\r\n",
        "  VALIDATION_TP = []\r\n",
        "  VALIDATION_FP = []\r\n",
        "  VALIDATION_TN = []\r\n",
        "  VALIDATION_FN = []\r\n",
        "  VALIDATION_PRECISION = []\r\n",
        "  VALIDATION_RECALL = []\r\n",
        "\r\n",
        "  print(\"Entering in k fold cross validation\")\r\n",
        "  print(\"Dataset dim: {}\".format(len(df)))\r\n",
        "\r\n",
        "  # directory to save results\r\n",
        "  dir = '{}_Numeric-{}_BatchSize-{}_{}x{}_lr-{}_Epochs-{}_Folds-{}_Norm-z{}_Aug-4{}_randomCrops_withoutclassweights'.format(NETWORK, str(NUM_VARS)[0], BATCH_SIZE, MIN_HEIGHT, MIN_WIDTH, LEARNING_RATE, EPOCHS, SPLIT_NUMBER, str(NORMALISE)[0], str(AUGMENT)[0])\r\n",
        "  save_dir = main_dir + '/classification_results/categorization/test677/' + dir + '/'\r\n",
        "  fold_var = 1\r\n",
        "  \r\n",
        "  # create an instance of the DataProcessor\r\n",
        "  processor = dp.DataProcessor(model = NETWORK,\r\n",
        "                               min_height = MIN_HEIGHT,\r\n",
        "                               min_width = MIN_WIDTH,\r\n",
        "                               normalise = False,           # perform normalisation\r\n",
        "                               rotate = ROTATE,                 # perform rotation\r\n",
        "                               plot_light = False,              # plot only select_crop() images\r\n",
        "                               plot_extensive = False,          # plot extensively all images\r\n",
        "                               show_prints = False)\r\n",
        "\r\n",
        "\r\n",
        "  for train_index, val_index in stratified_k_fold.split(np.zeros(len(df)), Y):\r\n",
        "    training_data = df.iloc[train_index]\r\n",
        "    validation_data = df.iloc[val_index]\r\n",
        "\r\n",
        "    train_images, train_labels, train_bbox = helper.load_from_df_with_cat(training_data)\r\n",
        "    val_images, val_labels, val_bbox = helper.load_from_df_with_cat(validation_data)\r\n",
        "    \r\n",
        "    # generate datasets\r\n",
        "    #train_dataset = helper.prepare_dataset(processor, train_images, train_labels, train_bbox)\r\n",
        "    #val_dataset = helper.prepare_dataset(processor, val_images, val_labels, val_bbox)\r\n",
        "    train_dataset = helper.create_dataset(processor, 5, train_images, train_labels, train_bbox)\r\n",
        "    val_dataset = helper.create_dataset(processor, 5, val_images, val_labels, val_bbox)\r\n",
        "\r\n",
        "    val_labels = val_dataset.map(lambda x, y: y)\r\n",
        "\r\n",
        "    # perform z-normalisation to the images\r\n",
        "    train_dataset, val_dataset = helper.z_norm(train_dataset, val_dataset)\r\n",
        "\r\n",
        "    # configure for performance\r\n",
        "    train_dataset = helper.configure_for_performance(train_dataset, BUFFER_SIZE, BATCH_SIZE, shuffle = True, augment = AUGMENT)\r\n",
        "    val_dataset = helper.configure_for_performance(val_dataset, BUFFER_SIZE, BATCH_SIZE)\r\n",
        "\r\n",
        "    # CREATE NEW MODEL\r\n",
        "    \"\"\"\r\n",
        "    # multi GPU strategy\r\n",
        "    strategy = tf.distribute.MirroredStrategy()\r\n",
        "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\r\n",
        "    with strategy.scope():\r\n",
        "        model = make_resNet(IMG_SIZE, start_from_scratch=True)\r\n",
        "    \"\"\"\r\n",
        "    if NETWORK == \"Mobile\":\r\n",
        "      model = models.make_MobileNet(MIN_WIDTH, MIN_HEIGHT, LEARNING_RATE)\r\n",
        "    elif NETWORK == \"ResNet\":\r\n",
        "      model = models.make_ResNet(MIN_WIDTH, MIN_HEIGHT, LEARNING_RATE)\r\n",
        "    else:\r\n",
        "      sys.exit(\"Incert valid network model. Options: Mobile or ResNet (case sensitive)\")\r\n",
        "\r\n",
        "    # CREATE CALLBACKS\r\n",
        "    callbacks = [\r\n",
        "        #EarlyStopping(patience = 10, verbose = 1),\r\n",
        "        ReduceLROnPlateau(factor = 0.1, patience = 5, min_lr = 0.00001, verbose = 1),\r\n",
        "        ModelCheckpoint(save_dir + helper.get_model_name(fold_var), verbose = 1, save_best_only = True),\r\n",
        "        TensorBoard(log_dir = save_dir + \"logs/\" + datetime.datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"))\r\n",
        "    ]\r\n",
        "\r\n",
        "    # FIT THE MODEL\r\n",
        "    history = model.fit(\r\n",
        "        train_dataset,\r\n",
        "        steps_per_epoch = len(train_dataset),\r\n",
        "        validation_data = val_dataset,\r\n",
        "        validation_steps = len(val_dataset), \r\n",
        "        epochs = EPOCHS,\r\n",
        "        callbacks = callbacks,\r\n",
        "        verbose = 1, \r\n",
        "        #class_weight = class_weights,\r\n",
        "        shuffle = True)\r\n",
        "    \r\n",
        "    # PLOT HISTORY\r\n",
        "    #print(history)\r\n",
        "\r\n",
        "\r\n",
        "    # PLOT TRAIN/VALIDATION LOSSES\r\n",
        "    fig, (ax1, ax2) = plt.subplots(nrows = 2, ncols = 1, figsize=(10, 10))\r\n",
        "    ax1.plot(history.history['accuracy'], label='Training Accuracy')\r\n",
        "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\r\n",
        "    ax1.grid(True)\r\n",
        "    ax1.legend(loc='lower right')\r\n",
        "    ax1.set(ylabel = \"Accuracy\",\r\n",
        "            title = 'Training and Validation Accuracy')\r\n",
        "\r\n",
        "    ax2.plot(history.history[\"loss\"], label='Training Loss')\r\n",
        "    ax2.plot(history.history[\"val_loss\"], label='Validation Loss')\r\n",
        "    ax2.plot(np.argmin(history.history[\"val_loss\"]), np.min(history.history[\"val_loss\"]), marker=\"x\", color=\"r\", label = \"Best model\")\r\n",
        "    ax2.grid(True)\r\n",
        "    ax2.legend(loc='upper right')\r\n",
        "    ax2.set(xlabel = 'Epoch', \r\n",
        "          ylabel = 'Cross Entropy',\r\n",
        "          title = 'Training and Validation Loss')\r\n",
        "    plt.show()\r\n",
        "    fig.savefig(save_dir + \"model_\" + str(fold_var) + \".jpg\")\r\n",
        "\r\n",
        "    \r\n",
        "    time.sleep(10) # guarantees enough time so that weights are saved and can be loaded after\r\n",
        "\r\n",
        "    # LOAD BEST MODEL\r\n",
        "    print(\"Loaded best weights of the training\")\r\n",
        "    model.load_weights(save_dir + helper.get_model_name(fold_var))     # Maybe a problem with colab, weights saved and loaded too fast giving concurrency problems\r\n",
        "\r\n",
        "    # EVALUATE PERFORMANCE of the model\r\n",
        "    results = model.evaluate(val_dataset, \r\n",
        "                             steps = len(val_dataset), \r\n",
        "                             verbose = 1)\r\n",
        "\r\n",
        "    results = dict(zip(model.metrics_names, results))\r\n",
        "\r\n",
        "    VALIDATION_ACCURACY.append(results['accuracy'])\r\n",
        "    VALIDATION_LOSS.append(results['loss'])\r\n",
        "    VALIDATION_TP.append(results[\"tp\"])\r\n",
        "    VALIDATION_FP.append(results[\"fp\"])\r\n",
        "    VALIDATION_TN.append(results[\"tn\"])\r\n",
        "    VALIDATION_FN.append(results[\"fn\"])\r\n",
        "    VALIDATION_PRECISION.append(results[\"precision\"])\r\n",
        "    VALIDATION_RECALL.append(results[\"recall\"])\r\n",
        "\r\n",
        "    # MAKE PREDICTIONS\r\n",
        "    predictions = model.predict(val_dataset)\r\n",
        "    predictions_non_category = [ np.argmax(t) for t in predictions ]\r\n",
        "    val_labels_non_category = [ np.argmax(t) for t in val_labels ]\r\n",
        "\r\n",
        "    conf_mat = confusion_matrix(val_labels_non_category, predictions_non_category, labels = [0,1,2,3,4,5])\r\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix = conf_mat, display_labels = [0,1,2,3,4,5])\r\n",
        "    conf_mat_display = disp.plot()\r\n",
        "    plt.savefig(save_dir + \"confusion_matrix_\" + str(fold_var) + \".jpg\")\r\n",
        "\r\n",
        "    tf.keras.backend.clear_session()\r\n",
        "\r\n",
        "    fold_var += 1\r\n",
        "\r\n",
        "    # save the values for each fold\r\n",
        "    csv_dir = save_dir + 'csv/'\r\n",
        "    os.makedirs(csv_dir, exist_ok=True)\r\n",
        "    with open(csv_dir + 'test_accuracy.pkl', 'wb') as handle:\r\n",
        "        pickle.dump(VALIDATION_ACCURACY, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "    with open(csv_dir + 'test_loss.pkl', 'wb') as handle:\r\n",
        "        pickle.dump(VALIDATION_LOSS, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "    with open(csv_dir + 'test_tp.pkl', 'wb') as handle:\r\n",
        "        pickle.dump(VALIDATION_TP, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "    with open(csv_dir + 'test_fp.pkl', 'wb') as handle:\r\n",
        "        pickle.dump(VALIDATION_FP, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "    with open(csv_dir + 'test_tn.pkl', 'wb') as handle:\r\n",
        "        pickle.dump(VALIDATION_TN, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "    with open(csv_dir + 'test_fn.pkl', 'wb') as handle:\r\n",
        "        pickle.dump(VALIDATION_FN, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "    with open(csv_dir + 'test_precision.pkl', 'wb') as handle:\r\n",
        "        pickle.dump(VALIDATION_PRECISION, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "    with open(csv_dir + 'test_recall.pkl', 'wb') as handle:\r\n",
        "        pickle.dump(VALIDATION_RECALL, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "  return\r\n",
        "\r\n",
        "#######################################\r\n",
        "# test\r\n",
        "fold_cross_validation(main_dir, NETWORK, MIN_HEIGHT, MIN_WIDTH, NUM_VARS, NORMALISE, ROTATE, AUGMENT, LEARNING_RATE, EPOCHS, BUFFER_SIZE, BATCH_SIZE, FOLDS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl9NJGQ2jb-g"
      },
      "source": [
        "## Perform Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPP7a2XY3Atf"
      },
      "source": [
        "### Using the csv split into train, val and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEykOjodkO-o"
      },
      "source": [
        "##################################\r\n",
        "###          SETTINGS         ####\r\n",
        "##################################\r\n",
        "main_dir = \"SAR_swath_images_VV+VH+WS\"\r\n",
        "NETWORK = \"ResNet\"     # options: [\"Mobile\", \"ResNet\"]\r\n",
        "MIN_HEIGHT = 700\r\n",
        "MIN_WIDTH = 400\r\n",
        "NUM_VARS = False\r\n",
        "NORMALISE = True\r\n",
        "ROTATE = False\r\n",
        "BATCH_SIZE = 8\r\n",
        "BUFFER_SIZE = 100\r\n",
        "EPOCHS = 50\r\n",
        "LEARNING_RATE = 0.0001\r\n",
        "##################################\r\n",
        "\r\n",
        "# load data\r\n",
        "train_images, train_labels, train_bbox = helper.load_data_with_cat(\"{}/csv/training.csv\".format(main_dir))\r\n",
        "val_images, val_labels, val_bbox = helper.load_data_with_cat(\"{}/csv/val.csv\".format(main_dir))\r\n",
        "test_images, test_labels, test_bbox = helper.load_data_with_cat(\"{}/csv/test.csv\".format(main_dir))\r\n",
        "class_weights = helper.compute_class_weights(\"{}/csv/full_dataset.csv\".format(main_dir))\r\n",
        "\r\n",
        "\r\n",
        "# create an instance of the DataProcessor\r\n",
        "processor = dp.DataProcessor(model = NETWORK,\r\n",
        "                             min_height = MIN_HEIGHT,\r\n",
        "                             min_width = MIN_WIDTH,\r\n",
        "                             normalise = NORMALISE,           # perform normalisation\r\n",
        "                             rotate = ROTATE,                 # perform rotation\r\n",
        "                             plot_light = False,              # plot only select_crop() images\r\n",
        "                             plot_extensive = False,          # plot extensively all images\r\n",
        "                             show_prints = False)\r\n",
        "\r\n",
        "\r\n",
        "# generate datasets\r\n",
        "train_dataset = helper.prepare_dataset(processor, train_images, train_labels, train_bbox)\r\n",
        "val_dataset = helper.prepare_dataset(processor, val_images, val_labels, val_bbox)\r\n",
        "test_dataset = helper.prepare_dataset(processor, test_images, test_labels, test_bbox)\r\n",
        "#for image, label in train_dataset:\r\n",
        "#  print(\"FINAL - image: {}, max: {}, min: {}, label: {}\".format(image[0].shape, np.max(image[0]), np.min(image[0]),  label))\r\n",
        "\r\n",
        "\r\n",
        "# configure for performance\r\n",
        "train_dataset = helper.configure_for_performance(train_dataset, BUFFER_SIZE, BATCH_SIZE)\r\n",
        "val_dataset = helper.configure_for_performance(val_dataset, BUFFER_SIZE, BATCH_SIZE)\r\n",
        "test_dataset = helper.configure_for_performance(test_dataset, BUFFER_SIZE, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPNL_CdNjdpK"
      },
      "source": [
        "# create the base pre-trained model\r\n",
        "if NETWORK == \"ResNet\":\r\n",
        "  base_model = models.ResNet50(weights='imagenet', include_top=False, input_shape=(MIN_WIDTH, MIN_HEIGHT, 3))\r\n",
        "elif NETWORK == \"Mobile\":\r\n",
        "  base_model = models.MobileNetV2(weights='imagenet', include_top=False, input_shape=(MIN_WIDTH, MIN_HEIGHT, 3))\r\n",
        "else:\r\n",
        "  sys.exit(\"Incert valid network model. Options: Mobile or ResNet (case sensitive)\")\r\n",
        "\r\n",
        "# Freeze the base_model\r\n",
        "base_model.trainable = False\r\n",
        "\r\n",
        "inputs = Input(shape=(MIN_WIDTH, MIN_HEIGHT, 3))\r\n",
        "#x = data_augmentation(inputs)  # apply random data augmentation\r\n",
        "x = base_model(inputs, training = False)\r\n",
        "x = GlobalAveragePooling2D()(x)\r\n",
        "outputs = Dense(6, activation=\"softmax\")(x)\r\n",
        "model = Model(inputs, outputs)\r\n",
        "\r\n",
        "# compile the model (should be done *after* setting layers to non-trainable)\r\n",
        "model.compile(optimizer = Adam(learning_rate = LEARNING_RATE), \r\n",
        "              loss = \"categorical_crossentropy\",\r\n",
        "              metrics = [CategoricalAccuracy(name=\"accuracy\"), \r\n",
        "                          Precision(name=\"precision\"), Recall(name=\"recall\"), \r\n",
        "                          TruePositives(name='tp'), FalsePositives(name='fp'),\r\n",
        "                          TrueNegatives(name='tn'), FalseNegatives(name='fn')])\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcGj-neEjnwo"
      },
      "source": [
        "# directory to save results\r\n",
        "dir = '{}_Numeric-{}_BatchSize-{}_lr-{}_Epochs-{}_Norm-{}_FineTune-T'.format(NETWORK, str(NUM_VARS)[0], str(BATCH_SIZE), str(LEARNING_RATE), str(EPOCHS), str(NORMALISE)[0])\r\n",
        "save_dir = main_dir + '/classification_results/categorization/' + dir + '/'\r\n",
        "os.makedirs(save_dir, exist_ok=True)\r\n",
        "\r\n",
        "# CREATE CALLBACKS\r\n",
        "callbacks = [\r\n",
        "    ModelCheckpoint(save_dir + \"best_model_frozen.h5\", verbose = 1, save_best_only = True),\r\n",
        "    TensorBoard(log_dir = save_dir + \"logs/\" + datetime.datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"))\r\n",
        "]\r\n",
        "\r\n",
        "initial_epochs = 40\r\n",
        "\r\n",
        "# train the top layer of the model on the dataset, the weights\r\n",
        "# of the pre-trained network will not be updated during training\r\n",
        "history = model.fit(train_dataset,\r\n",
        "                    steps_per_epoch = len(train_dataset),\r\n",
        "                    validation_data = val_dataset,\r\n",
        "                    validation_steps = len(val_dataset),\r\n",
        "                    epochs = initial_epochs,\r\n",
        "                    callbacks = callbacks,\r\n",
        "                    class_weight = class_weights,\r\n",
        "                    shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTQb0boPjqll"
      },
      "source": [
        "acc = history.history['accuracy']\r\n",
        "val_acc = history.history['val_accuracy']\r\n",
        "\r\n",
        "loss = history.history['loss']\r\n",
        "val_loss = history.history['val_loss']\r\n",
        "\r\n",
        "fig, (ax1, ax2) = plt.subplots(nrows = 2, ncols = 1, figsize=(10, 10))\r\n",
        "ax1.plot(acc, label='Training Accuracy')\r\n",
        "ax1.plot(val_acc, label='Validation Accuracy')\r\n",
        "ax1.legend(loc='lower right')\r\n",
        "ax1.set(ylabel = \"Accuracy\",\r\n",
        "        title = 'Training and Validation Accuracy')\r\n",
        "#plt.ylim([min(plt.ylim()),1])\r\n",
        "\r\n",
        "ax2.plot(loss, label='Training Loss')\r\n",
        "ax2.plot(val_loss, label='Validation Loss')\r\n",
        "ax2.legend(loc='upper right')\r\n",
        "ax2.set(xlabel = 'epoch', \r\n",
        "        ylabel = 'Cross Entropy',\r\n",
        "        title = 'Training and Validation Loss')\r\n",
        "#plt.ylim([0,max(plt.ylim())])\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Dn0K4WwjtF6"
      },
      "source": [
        "# unfreeze the whole base model\r\n",
        "base_model.trainable = True\r\n",
        "\r\n",
        "# you should try to fine-tune a small number of top layers rather than the whole model\r\n",
        "# Let's take a look to see how many layers are in the base model\r\n",
        "print(\"Number of layers in the base model: \", len(base_model.layers))\r\n",
        "\r\n",
        "# Fine-tune from this layer onwards\r\n",
        "fine_tune_at = 100\r\n",
        "\r\n",
        "# Freeze all the layers before the `fine_tune_at` layer\r\n",
        "for layer in base_model.layers[:fine_tune_at]:\r\n",
        "  layer.trainable =  False\r\n",
        "\r\n",
        "# recompile the model for the modifications to take effect, with a low learning rate\r\n",
        "model.compile(optimizer = Adam(learning_rate = 1e-5),\r\n",
        "              loss = \"categorical_crossentropy\",\r\n",
        "              metrics = [CategoricalAccuracy(name=\"accuracy\"), \r\n",
        "                          Precision(name=\"precision\"), Recall(name=\"recall\"), \r\n",
        "                          TruePositives(name='tp'), FalsePositives(name='fp'),\r\n",
        "                          TrueNegatives(name='tn'), FalseNegatives(name='fn')])\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2vnrD8Rj4p-"
      },
      "source": [
        "# adjust callbacks\r\n",
        "callbacks = [\r\n",
        "    #EarlyStopping(patience = 5, verbose = 1),\r\n",
        "    ReduceLROnPlateau(factor = 0.1, patience = 5, min_lr = 0.000001, verbose = 1),\r\n",
        "    ModelCheckpoint(save_dir + \"best_model_fine_tuned.h5\", verbose = 1, save_best_only = True),\r\n",
        "    TensorBoard(log_dir = save_dir + \"logs/\" + datetime.datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"))\r\n",
        "]\r\n",
        "\r\n",
        "fine_tune_epochs = 10\r\n",
        "total_epochs = initial_epochs + fine_tune_epochs\r\n",
        "\r\n",
        "# train the entire model end-to-end\r\n",
        "history_fine = model.fit(train_dataset,\r\n",
        "                         steps_per_epoch = len(train_dataset),\r\n",
        "                         validation_data = val_dataset,\r\n",
        "                         validation_steps = len(val_dataset),\r\n",
        "                         epochs = total_epochs,\r\n",
        "                         initial_epoch = history.epoch[-1],\r\n",
        "                         callbacks = callbacks,\r\n",
        "                         class_weight = class_weights,\r\n",
        "                         shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0FwWQH1j7aW"
      },
      "source": [
        "acc += history_fine.history['accuracy']\r\n",
        "val_acc += history_fine.history['val_accuracy']\r\n",
        "\r\n",
        "loss += history_fine.history['loss']\r\n",
        "val_loss += history_fine.history['val_loss']\r\n",
        "\r\n",
        "\r\n",
        "fig, (ax1, ax2) = plt.subplots(nrows = 2, ncols = 1, figsize=(10, 10))\r\n",
        "ax1.plot(acc, label='Training Accuracy')\r\n",
        "ax1.plot(val_acc, label='Validation Accuracy')\r\n",
        "#plt.ylim([min(plt.ylim()),1])\r\n",
        "ax1.plot([initial_epochs-1,initial_epochs-1],\r\n",
        "          plt.ylim(), label='Start Fine Tuning')\r\n",
        "ax1.legend(loc='lower right')\r\n",
        "ax1.set(ylabel = \"Accuracy\",\r\n",
        "        title = 'Training and Validation Accuracy')\r\n",
        "\r\n",
        "ax2.plot(loss, label='Training Loss')\r\n",
        "ax2.plot(val_loss, label='Validation Loss')\r\n",
        "#plt.ylim([0, 1.0])\r\n",
        "ax2.plot([initial_epochs-1,initial_epochs-1],\r\n",
        "         plt.ylim(), label='Start Fine Tuning')\r\n",
        "ax2.legend(loc='upper right')\r\n",
        "ax2.set(xlabel = 'epoch', \r\n",
        "      ylabel = 'Cross Entropy',\r\n",
        "      title = 'Training and Validation Loss')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "fig.savefig(save_dir + 'Learning_curves.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP4_1B_sj_Jy"
      },
      "source": [
        "# EVALUATE MODEL\r\n",
        "print(\"Loaded best weights of the training\")\r\n",
        "model.load_weights(save_dir + \"best_model_fine_tuned.h5\")\r\n",
        "\r\n",
        "results = model.evaluate(test_dataset, \r\n",
        "                         steps = len(test_dataset), \r\n",
        "                         verbose = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3EqPAWN3Kbl"
      },
      "source": [
        "### Using Stratified-K Fold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0hOERchi3OA3",
        "outputId": "809d9b27-a833-4a76-b76b-826567295084"
      },
      "source": [
        "##################################\r\n",
        "###          SETTINGS         ####\r\n",
        "##################################\r\n",
        "main_dir = \"SAR_swath_images_VV+VH+VH\"\r\n",
        "NETWORK = \"ResNet\"     # options: [\"Mobile\", \"ResNet\"]\r\n",
        "MIN_HEIGHT = 700\r\n",
        "MIN_WIDTH = 400\r\n",
        "NUM_VARS = False\r\n",
        "NORMALISE = True\r\n",
        "ROTATE = False\r\n",
        "AUGMENT = True\r\n",
        "BATCH_SIZE = 8\r\n",
        "BUFFER_SIZE = 100\r\n",
        "EPOCHS = 44\r\n",
        "FOLDS = 5\r\n",
        "LEARNING_RATE = 0.0001\r\n",
        "##################################\r\n",
        "\r\n",
        "# STRATIFIED CROSS VALIDATION\r\n",
        "def fold_cross_validation(main_dir, NETWORK, MIN_HEIGHT, MIN_WIDTH, NUM_VARS, NORMALISE, ROTATE, AUGMENT, LEARNING_RATE, EPOCHS, BUFFER_SIZE, BATCH_SIZE, SPLIT_NUMBER):\r\n",
        "\r\n",
        "  df = pd.read_csv(main_dir + \"/csv/full_dataset.csv\", converters={'bbox_shape': eval}).dropna()\r\n",
        "  class_weights = helper.compute_class_weights(\"{}/csv/full_dataset.csv\".format(main_dir))\r\n",
        "\r\n",
        "  #Y = df[\"label\"]\r\n",
        "  Y = np.zeros(len(df), dtype=int)\r\n",
        "  for index, row in df.iterrows():\r\n",
        "    if df[\"label\"][index] != 0:\r\n",
        "      cat = df[\"image\"][index].split('/')[1]\r\n",
        "      Y[index] = int(cat[-1])\r\n",
        "\r\n",
        "  stratified_k_fold = StratifiedKFold(n_splits = SPLIT_NUMBER, random_state = 42, shuffle = False)\r\n",
        "\r\n",
        "  VALIDATION_ACCURACY = []\r\n",
        "  VALIDATION_LOSS = []\r\n",
        "  VALIDATION_TP = []\r\n",
        "  VALIDATION_FP = []\r\n",
        "  VALIDATION_TN = []\r\n",
        "  VALIDATION_FN = []\r\n",
        "  VALIDATION_PRECISION = []\r\n",
        "  VALIDATION_RECALL = []\r\n",
        "\r\n",
        "  print(\"Entering in k fold cross validation\")\r\n",
        "  print(\"Dataset dim: {}\".format(len(df)))\r\n",
        "\r\n",
        "  # directory to save results\r\n",
        "  dir = '{}_Numeric-{}_BatchSize-{}_lr-{}_Epochs-{}_Folds-{}_Norm-{}_Aug-{}_FineTune-T_from165'.format(NETWORK, str(NUM_VARS)[0], str(BATCH_SIZE), str(LEARNING_RATE), str(EPOCHS), str(SPLIT_NUMBER), str(NORMALISE)[0], str(AUGMENT)[0])\r\n",
        "  save_dir = main_dir + '/classification_results/categorization/' + dir + '/'\r\n",
        "  fold_var = 1\r\n",
        "  \r\n",
        "  # create an instance of the DataProcessor\r\n",
        "  processor = dp.DataProcessor(model = NETWORK,\r\n",
        "                               min_height = MIN_HEIGHT,\r\n",
        "                               min_width = MIN_WIDTH,\r\n",
        "                               normalise = NORMALISE,           # perform normalisation\r\n",
        "                               rotate = ROTATE,                 # perform rotation\r\n",
        "                               plot_light = False,              # plot only select_crop() images\r\n",
        "                               plot_extensive = False,          # plot extensively all images\r\n",
        "                               show_prints = False)\r\n",
        "\r\n",
        "\r\n",
        "  for train_index, val_index in stratified_k_fold.split(np.zeros(len(df)), Y):\r\n",
        "    training_data = df.iloc[train_index]\r\n",
        "    validation_data = df.iloc[val_index]\r\n",
        "\r\n",
        "    train_images, train_labels, train_bbox = helper.load_from_df_with_cat(training_data)\r\n",
        "    val_images, val_labels, val_bbox = helper.load_from_df_with_cat(validation_data)\r\n",
        "    \r\n",
        "    # generate datasets\r\n",
        "    train_dataset = helper.prepare_dataset(processor, train_images, train_labels, train_bbox)\r\n",
        "    val_dataset = helper.prepare_dataset(processor, val_images, val_labels, val_bbox)\r\n",
        "\r\n",
        "    # configure for performance\r\n",
        "    train_dataset = helper.configure_for_performance(train_dataset, BUFFER_SIZE, BATCH_SIZE, shuffle = True, augment = AUGMENT)\r\n",
        "    val_dataset = helper.configure_for_performance(val_dataset, BUFFER_SIZE, BATCH_SIZE)\r\n",
        "\r\n",
        "    # create the base pre-trained model\r\n",
        "    if NETWORK == \"ResNet\":\r\n",
        "      base_model = models.ResNet50(weights='imagenet', include_top=False, input_shape=(MIN_WIDTH, MIN_HEIGHT, 3))\r\n",
        "    elif NETWORK == \"Mobile\":\r\n",
        "      base_model = models.MobileNetV2(weights='imagenet', include_top=False, input_shape=(MIN_WIDTH, MIN_HEIGHT, 3))\r\n",
        "    else:\r\n",
        "      sys.exit(\"Incert valid network model. Options: Mobile or ResNet (case sensitive)\")\r\n",
        "\r\n",
        "    # Freeze the base_model\r\n",
        "    base_model.trainable = False\r\n",
        "\r\n",
        "    inputs = Input(shape=(MIN_WIDTH, MIN_HEIGHT, 3))\r\n",
        "    x = base_model(inputs, training = False)\r\n",
        "    x = GlobalAveragePooling2D()(x)\r\n",
        "    outputs = Dense(6, activation=\"softmax\")(x)\r\n",
        "    model = Model(inputs, outputs)\r\n",
        "\r\n",
        "    # compile the model (should be done *after* setting layers to non-trainable)\r\n",
        "    model.compile(optimizer = Adam(learning_rate = LEARNING_RATE), \r\n",
        "                  loss = \"categorical_crossentropy\",\r\n",
        "                  metrics = [CategoricalAccuracy(name=\"accuracy\"), \r\n",
        "                              Precision(name=\"precision\"), Recall(name=\"recall\"), \r\n",
        "                              TruePositives(name='tp'), FalsePositives(name='fp'),\r\n",
        "                              TrueNegatives(name='tn'), FalseNegatives(name='fn')])\r\n",
        "\r\n",
        "    # CREATE CALLBACKS\r\n",
        "    callbacks = [\r\n",
        "        ModelCheckpoint(save_dir + \"best_model_frozen_{}.h5\".format(str(fold_var)), verbose = 1, save_best_only = True),\r\n",
        "        TensorBoard(log_dir = save_dir + \"logs/\" + datetime.datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"))\r\n",
        "    ]\r\n",
        "\r\n",
        "    initial_epochs = 2\r\n",
        "\r\n",
        "    # train the top layer of the model on the dataset, the weights\r\n",
        "    # of the pre-trained network will not be updated during training\r\n",
        "    history = model.fit(train_dataset,\r\n",
        "                        steps_per_epoch = len(train_dataset),\r\n",
        "                        validation_data = val_dataset,\r\n",
        "                        validation_steps = len(val_dataset),\r\n",
        "                        epochs = initial_epochs,\r\n",
        "                        callbacks = callbacks,\r\n",
        "                        class_weight = class_weights,\r\n",
        "                        shuffle = True)\r\n",
        "\r\n",
        "    acc = history.history['accuracy']\r\n",
        "    val_acc = history.history['val_accuracy']\r\n",
        "\r\n",
        "    loss = history.history['loss']\r\n",
        "    val_loss = history.history['val_loss']\r\n",
        "\r\n",
        "    # unfreeze the whole base model\r\n",
        "    base_model.trainable = True\r\n",
        "\r\n",
        "    # Fine-tune from this layer onwards\r\n",
        "    fine_tune_at = 165\r\n",
        "\r\n",
        "    # Freeze all the layers before the `fine_tune_at` layer\r\n",
        "    for layer in base_model.layers[:fine_tune_at]:\r\n",
        "      layer.trainable = False\r\n",
        "\r\n",
        "    # recompile the model for the modifications to take effect, with a low learning rate\r\n",
        "    model.compile(optimizer = Adam(learning_rate = 1e-5),\r\n",
        "                  loss = \"categorical_crossentropy\",\r\n",
        "                  metrics = [CategoricalAccuracy(name=\"accuracy\"), \r\n",
        "                              Precision(name=\"precision\"), Recall(name=\"recall\"), \r\n",
        "                              TruePositives(name='tp'), FalsePositives(name='fp'),\r\n",
        "                              TrueNegatives(name='tn'), FalseNegatives(name='fn')])\r\n",
        "    base_model.summary()\r\n",
        "    \r\n",
        "    # adjust callbacks\r\n",
        "    callbacks = [\r\n",
        "        ModelCheckpoint(save_dir + \"best_model_fine_tuned_{}.h5\".format(str(fold_var)), verbose = 1, save_best_only = True),\r\n",
        "        TensorBoard(log_dir = save_dir + \"logs/\" + datetime.datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"))\r\n",
        "    ]\r\n",
        "\r\n",
        "    fine_tune_epochs = 10\r\n",
        "    total_epochs = initial_epochs + fine_tune_epochs\r\n",
        "\r\n",
        "    # train the entire model end-to-end\r\n",
        "    history_fine = model.fit(train_dataset,\r\n",
        "                            steps_per_epoch = len(train_dataset),\r\n",
        "                            validation_data = val_dataset,\r\n",
        "                            validation_steps = len(val_dataset),\r\n",
        "                            epochs = total_epochs,\r\n",
        "                            initial_epoch = history.epoch[-1],\r\n",
        "                            callbacks = callbacks,\r\n",
        "                            class_weight = class_weights,\r\n",
        "                            shuffle = True)\r\n",
        "\r\n",
        "    acc += history_fine.history['accuracy']\r\n",
        "    val_acc += history_fine.history['val_accuracy']\r\n",
        "\r\n",
        "    loss += history_fine.history['loss']\r\n",
        "    val_loss += history_fine.history['val_loss']\r\n",
        "\r\n",
        "    # PLOT TRAIN/VALIDATION LOSSES\r\n",
        "    fig, (ax1, ax2) = plt.subplots(nrows = 2, ncols = 1, figsize=(10, 10))\r\n",
        "    ax1.plot(acc, label='Training Accuracy')\r\n",
        "    ax1.plot(val_acc, label='Validation Accuracy')\r\n",
        "    #plt.ylim([min(plt.ylim()),1])\r\n",
        "    ax1.plot([initial_epochs-1, initial_epochs-1], plt.ylim(), label='Start Fine Tuning')\r\n",
        "    ax1.grid(True)\r\n",
        "    ax1.legend(loc='lower right')\r\n",
        "    ax1.set(ylabel = \"Accuracy\",\r\n",
        "            title = 'Training and Validation Accuracy')\r\n",
        "\r\n",
        "    ax2.plot(loss, label='Training Loss')\r\n",
        "    ax2.plot(val_loss, label='Validation Loss')\r\n",
        "    #plt.ylim([0, 1.0])\r\n",
        "    ax2.plot([initial_epochs-1, initial_epochs-1], plt.ylim(), label='Start Fine Tuning')\r\n",
        "    ax2.plot(np.argmin(val_loss), np.min(val_loss), marker=\"x\", color=\"r\", label = \"Best model\")\r\n",
        "    ax2.grid(True)\r\n",
        "    ax2.legend(loc='upper right')\r\n",
        "    ax2.set(xlabel = 'Epoch', \r\n",
        "          ylabel = 'Cross Entropy',\r\n",
        "          title = 'Training and Validation Loss')\r\n",
        "    plt.show()\r\n",
        "    fig.savefig(save_dir + \"model_\" + str(fold_var) + \".jpg\")\r\n",
        "    \r\n",
        "\r\n",
        "    ##################################################################################################\r\n",
        "    time.sleep(15) # guarantees enough time so that weights are saved and can be loaded after\r\n",
        "\r\n",
        "    # LOAD BEST MODEL\r\n",
        "    print(\"Loaded best weights of the training\")\r\n",
        "    model.load_weights(save_dir + \"best_model_fine_tuned_{}.h5\".format(str(fold_var)))     # Maybe a problem with colab, weights saved and loaded too fast giving concurrency problems\r\n",
        "\r\n",
        "    # EVALUATE PERFORMANCE of the model\r\n",
        "    results = model.evaluate(val_dataset, \r\n",
        "                             steps = len(val_dataset), \r\n",
        "                             verbose = 1)\r\n",
        "\r\n",
        "    results = dict(zip(model.metrics_names, results))\r\n",
        "\r\n",
        "    VALIDATION_ACCURACY.append(results['accuracy'])\r\n",
        "    VALIDATION_LOSS.append(results['loss'])\r\n",
        "    VALIDATION_TP.append(results[\"tp\"])\r\n",
        "    VALIDATION_FP.append(results[\"fp\"])\r\n",
        "    VALIDATION_TN.append(results[\"tn\"])\r\n",
        "    VALIDATION_FN.append(results[\"fn\"])\r\n",
        "    VALIDATION_PRECISION.append(results[\"precision\"])\r\n",
        "    VALIDATION_RECALL.append(results[\"recall\"])\r\n",
        "\r\n",
        "    # MAKE PREDICTIONS\r\n",
        "    predictions = model.predict(val_dataset)\r\n",
        "    predictions_non_category = [ np.argmax(t) for t in predictions ]\r\n",
        "    val_labels_non_category = [ np.argmax(t) for t in val_labels ]\r\n",
        "\r\n",
        "    conf_mat = confusion_matrix(val_labels_non_category, predictions_non_category, labels = [0,1,2,3,4,5])\r\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix = conf_mat, display_labels = [0,1,2,3,4,5])\r\n",
        "    conf_mat_display = disp.plot()\r\n",
        "    plt.savefig(save_dir + \"confusion_matrix_\" + str(fold_var) + \".jpg\")\r\n",
        "\r\n",
        "    tf.keras.backend.clear_session()\r\n",
        "\r\n",
        "    fold_var += 1\r\n",
        "\r\n",
        "    # save the values for each fold\r\n",
        "    csv_dir = save_dir + 'csv/'\r\n",
        "    os.makedirs(csv_dir, exist_ok=True)\r\n",
        "    with open(csv_dir + 'test_accuracy.pkl', 'wb') as handle:\r\n",
        "        pickle.dump(VALIDATION_ACCURACY, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "    with open(csv_dir + 'test_loss.pkl', 'wb') as handle:\r\n",
        "        pickle.dump(VALIDATION_LOSS, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "    with open(csv_dir + 'test_tp.pkl', 'wb') as handle:\r\n",
        "        pickle.dump(VALIDATION_TP, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "    with open(csv_dir + 'test_fp.pkl', 'wb') as handle:\r\n",
        "        pickle.dump(VALIDATION_FP, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "    with open(csv_dir + 'test_tn.pkl', 'wb') as handle:\r\n",
        "        pickle.dump(VALIDATION_TN, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "    with open(csv_dir + 'test_fn.pkl', 'wb') as handle:\r\n",
        "        pickle.dump(VALIDATION_FN, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "    with open(csv_dir + 'test_precision.pkl', 'wb') as handle:\r\n",
        "        pickle.dump(VALIDATION_PRECISION, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "    with open(csv_dir + 'test_recall.pkl', 'wb') as handle:\r\n",
        "        pickle.dump(VALIDATION_RECALL, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "  return\r\n",
        "\r\n",
        "##################################################################################################\r\n",
        "# test\r\n",
        "fold_cross_validation(main_dir, NETWORK, MIN_HEIGHT, MIN_WIDTH, NUM_VARS, NORMALISE, ROTATE, AUGMENT, LEARNING_RATE, EPOCHS, BUFFER_SIZE, BATCH_SIZE, FOLDS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEICAYAAABGaK+TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAZmUlEQVR4nO3df5zcVX3v8debBASyQPhlGpJAvIIg5TdbwAvihh8WgRYKFFCIiRdu6lUrVryK1EfFW1vhcatAoV6agiVIICCoWKggBRYeKD9MEIgQUMRAkgIBIYFFlKKf+8c5C99MZnZnZ3d2crLv5+Mxj3x/n3NmvvOe8z3znY0iAjMzK88Gna6AmZm1xgFuZlYoB7iZWaEc4GZmhXKAm5kVygFuZlYoB3gHSZouKSSNH+J+Z0u6tF31srFHyb9KeknS/U1sf7mkL+fpHknLWyjzzWNYa9bZAJd0kKQfSVot6UVJP5T0R5X1XZL6JH2/zr5LJb2W1z+bT5Su0W3B2nK9DhvucSLi7yPi9JGoU6dIOjk/H6pZPl7SSkmzJL0h6Z119v2OpH8YQlmPSfofdZafIWlhnu6VdHrN+iEHk6RLJF1RZ/mekn4raStJ50i6ss42IWnHmmW3SHr/UOowxPr2H/8g4HBgakTs167yWlXv9Sm5nJGyTga4pM2BG4GLgK2AKcCXgN9WNjs+zx8u6Q/qHOZPIqIL2AvYG/h8WyttQ/VdYCLwvprlRwABzAduA2ZWV0raCjgSmFd7wPxBPbtOWfOAD9dZPrPecZohabakyxuUdZykCXXKujEiXhxCGROAbuDOVuo4xOPvACyNiFfbUZa1xzoZ4MC7ACLi6oj4XUS8FhE/iIiHK9vMAi4BHgZObXSgiHgWuIUU5AOSdKSkRyW9ImmFpM/k5XdKOj5PH5h7Skfl+UMlPZin3ynpdkm/kvSCpPmSJuZ13wS2B/4tXxl8tlL0KZKezvv8dRP1fLMHVxmG+YikZfkS+KOS/kjSw5JWSbq4sm/DOub1+0j6SX4OviXpmuplrqSjJT2Yj/sjSXtU1n0uP2+vSHpc0qGN2hARvwGuZe1g/TBwVUS8QQrDmTXrTwYejYjFgz1PFd8EDpK0Q6WuuwJ7AFcP4TiDioh7gBWkDkZ/WeOADwFr9cwHcSjww4j4raRxSkNnv8jP7yJJ05Scn69aXpa0WNJukvZXuvocV6nHn0l6uPb4pPfPpcB78rn5pfwBdXe1MvWuEIZC0t6SHsj1vwbYuLJuS0k3Sno+n8M3Spqa1/0d8F7g4ly/i/PyC/M5/3J+Pt5bOd5+khbmdc9J+lpl3QH53F0l6SFJPQOVU6cdx+T3wMv59TgiL58t6cncvl9KOkXS23I5u1X231ZphODtrT6Xb4qIde4BbA78ivQG/gCwZc36HYDfA7sCZwIP16xfChyWp6cCi4ELmyj3GeC9eXpLYJ88/X+Ai/L02cAvgPMq6y7M0zuSLkPfBmwL3AVcUK9eeX46qbf5L8AmwJ6kq4p3D1LPc4Ara45xCekN8X7gN6Qe7ttJVy8rgfcNVkdgI+Ap4AxgQ+A44HXgy3n93vlY+wPjSB+iS/OxdgaWAdtV6vXOQdpxIPAysEme3wJ4Ddgrz28CrAYOquxzD/CpBse7HJjdYN2twBcq818BvluZ7wVOr9mnB1je4HizgcsbrPtr4D8q838MPA9sWPv61ewXwI6V+UuAv8jT/5t0Hu8MKJ8rW+djLyJdzQh4NzA57/ML4PDK8b4FnNXg+LOBu2vad3ej+uXnuv+8aPg8VfbtP7f+Kp9bJwD/VTnG1qQPvU2BzXJdB3t9Ts37jSflwLPAxpXzZGae7gIOyNNTSNlyJKkDe3ie37ZROTVl7kc6Jw/P+08BdgEmkM7lnfN2k4E/zNPfAP6ucoyPAzcPlkfNPDoe1gM8Ue/OJ8ly4A3ge8CkvO4LwIOVF+R3wN6VfZcCfcAr+aS7DZjYRJlPA38BbF6z/FDyhwRwM3A6cG+evxM4rsHxjgV+UlOvegE+tbLsfuDkQep5DmsH+JTK+l8BJ1Xmr6dx6L1ZR+BgUu9RlfV389ab7P8Bf1uz/+OkYZAdSeF+GDmomnydfw58KE//T+ChmvWXAnPz9E6kD5S3NzjW5TQO8FOBx/P0Bvm1/rPK+l7g18CqyqOP1gJ8e1I4Tc3z86l0IPLr93pNWatYO8CfBqZVnudj6pR1CPAz4ABgg5p1Xwa+kac3A14Fdmhw/Nm0N8APBv6z5tz6Uf8x6my/F/BSzevTMFjzNi8Be+bpu0jDrtvUbPM54Js1y24BZjVTDvDPwPl1lk/Ir+Hx5A5JZd1hwC8q8z8EPtzse2Sgx7o6hEJELImI2RExFdgN2A64IK/+MOlNQUSsIIXorJpDHBsRm5FOrl2AbZoo9njSJ/NTSsMm78nL7wHeJWkS6cS6ApgmaRvSJ/JdAJImSVqQhxFeBq5sstxnK9O/JvUYhuq5yvRrdea7mqjjdsCKyGdZtqwyvQNwZr4kXCVpFTCN1Ot+AvgUKZxW5jK2a6LeV/DWMMpM1h5mmAf8uaSN8/pbImJl/8rKMNEq0jDF1yv1+3rlON8GJks6gHRObArcVFPWJyNiYv8DOLq6UtLXK2V9HfhQpaw3hyYi4mnSOXGq0pfnx9Zp17XVsnJ51bJ2B1ZHRP/zP43Uo15DRNwOXAz8E+l5n6v0HRLAVaTx+LeRrqYeiIinGhy/3eqdW0/1T0jaVNI/S3oqn5d3AROrQ0C1JH1G0hKlGx1Wka7g+s/l00hDsY9J+rGk/tdyB9L5VD2HDyL1mJvR6HV4FTgJ+CjwjKSbJO2SV98BbJqHtaaTMuQ7TZY3oHU2wKsi4jHSJ/5ukv47qSf2+TzG9yzpkv5DqnM7XkTcmfcd9K6FiPhxRBxDGnr4LmmMloj4Neky9QzgpxHxOqn38GnSJ+sL+RB/T+ql7B4Rm5N6fdW7LKonb6cMVMdngCnSGneGTKtMLyNdClaDZ9OIuBogIq6KiINIb5IAzmuiPt8EDs0flgeQP5gr7gZeBI7JdV3jS8eI2KMSgFcBH6vU7WOV7X4NXEf6sJgJLMivY9Mi4mOVsj5GGqvvL2uPms37x++PB34ZEYuGUhapI/HvlfllwFp35OR6/WNE7EsaUnwXabiFiHiUFJIfIH24XTXA8Wu9SvqQA0D1bxQYinrn1vaV6TNJw0P75/Py4P6i879rvHfyePdngRNJQ6wTSUMbAoiIn0fEB0nv5fOA65S+tF1G6oFXz+EJEXFuvXLqGOh1uCUiDid9GDxGGholIn5HypIP5seNEfHKIOU0ZZ0McEm7SDqz8iXGNFLD7yX1tG8lnax75cdupPHSDzQ45AWku1X2HKDMjfKXDltExH+RxrN+X9nkTuATvHVHQG/NPKTL1D5gtaQp5DdSxXPAfxug6aNhoDreQxqO+oTS7XzHkK4w+v0L8NHck5CkCZKOkrSZpJ0lHZJ7e78h9fqrz19dEbGUFNJXA7dG+tK5uj5IvdfzSOO8/9Zas4EUqieRQrWlu0+G4HpSQH2pxbKOZM0rhEuBv5W0U37u95C0tdKX1ftL2pAUur9hzef9KlLH42DSuHKj49d6CPhDSXvlq59zWmhD1T2kodBPStpQ0nGseW5tRjpnVindafTFmv1r3zub5eM9D4yX9Dek784AkHSqpG0j4vekoQ1Iz8uVwJ9I+mOlL4Y3VrpddGqDcmpdBnxE6eaFDSRNyXk1SenLzQmk77H6WPt1OAk4hTU/SIdlnQxw0tj1/sB9kl4lBfdPSZ/SJ5K+UHy28vglqSdXO4wCQEQ8TwqBvxmk3JnA0nwJ91HSk93vTtJJc1eDeUhv1n1IPYGbSJftVV8BvpAv3T4zSF3apWEdc4/0ONLl5ypSj/dG8u2bEbGQNE59MWm88QnSWCmkLzLPBV4gDQm9neZv3ZxH6rU3ukvjClIYXhMRv22wTTPuIrV7eUT8eBjHGVS+pL6e9CV67VXFgJTuCtqVdJXX72ukXtwPSJ2Ly0idls1JH6wvkXrbvwL+b2W/q0nfUdzef6XY4Pi19f8Z6Qv6/yB9T3F3o22bUTm3ZpOuqE5izffHBbk9L5De7zfXHOJC4ASlO1T+kTRufTNp/P8p0gdXdTjoCOARSX1535Mj3c22jHQ1dzYp/JeROjEbNCgHSY9IOiW3437gI8D5pHOp/xbMDUhX5P+Z2/c+4H9V2n8f6QN2O2Ct3660SmsOSZmtSdJ9wCUR8a+drstYIelE4ISIOLHE49voWVd74NYhkt4n6Q/yEMos0r3Stb0ha69VpB5eqce3UTLmAjxfDvXVeZwy+N6jR9L3G9Tz7DYXvTNp/HMVacjqhIh4ptWDdbAdxYr0o7V7Sju+pO0bvNZ9krYf/Ag2VB5CMTMr1JjrgZuZrS+G9GdMh2ubbbaJ6dOnt7Tvq6++yoQJtX8faP3mNo8NbvP6b7jtXbRo0QsRsW3t8lEN8OnTp7Nw4cKW9u3t7aWnp2dkK7SOc5vHBrd5/Tfc9kp6qt5yD6GYmRXKAW5mVigHuJlZoRzgZmaFcoCbmRXKAW5mVigHuJlZoRzgZmaFcoCbmRVqVH+JORyLV6xm9lkD/Qci7bH03KNGvUwzs2a4B25mVigHuJlZoRzgZmaFcoCbmRXKAW5mVigHuJlZoRzgZmaFcoCbmRXKAW5mVigHuJlZoRzgZmaFcoCbmRXKAW5mVigHuJlZoRzgZmaFcoCbmRWqqQCX9FeSHpH0U0lXS9pY0jsk3SfpCUnXSNqo3ZU1M7O3DBrgkqYAnwS6I2I3YBxwMnAecH5E7Ai8BJzWzoqamdmamh1CGQ9sImk8sCnwDHAIcF1ePw84duSrZ2ZmjQwa4BGxAvgH4GlScK8GFgGrIuKNvNlyYEq7KmlmZmtTRAy8gbQlcD1wErAK+Bap531OHj5B0jTg+3mIpXb/OcAcgEmTJu27YMGCliq68sXVPPdaS7sOy+5Tthj9QrO+vj66uro6Vn4nuM1jw1hr83DbO2PGjEUR0V27vJn/lf4w4JcR8TyApG8DBwITJY3PvfCpwIp6O0fEXGAuQHd3d/T09LTUgIvm38BXFzdT3ZG19JSeUS+zX29vL60+X6Vym8eGsdbmdrW3mTHwp4EDJG0qScChwKPAHcAJeZtZwA0jXjszM2uomTHw+0hDJg8Ai/M+c4HPAZ+W9ASwNXBZG+tpZmY1mhqTiIgvAl+sWfwksN+I18jMzJriX2KamRXKAW5mVigHuJlZoRzgZmaFcoCbmRXKAW5mVigHuJlZoRzgZmaFcoCbmRXKAW5mVigHuJlZoRzgZmaFcoCbmRXKAW5mVigHuJlZoRzgZmaFcoCbmRXKAW5mVigHuJlZoRzgZmaFcoCbmRXKAW5mVigHuJlZoRzgZmaFcoCbmRXKAW5mVigHuJlZoRzgZmaFcoCbmRXKAW5mVigHuJlZoRzgZmaFcoCbmRXKAW5mVigHuJlZoRzgZmaFcoCbmRXKAW5mVqimAlzSREnXSXpM0hJJ75G0laRbJf08/7tluytrZmZvabYHfiFwc0TsAuwJLAHOAm6LiJ2A2/K8mZmNkkEDXNIWwMHAZQAR8XpErAKOAeblzeYBx7arkmZmtjZFxMAbSHsBc4FHSb3vRcAZwIqImJi3EfBS/3zN/nOAOQCTJk3ad8GCBS1VdOWLq3nutZZ2HZbdp2wx+oVmfX19dHV1daz8TnCbx4ax1ubhtnfGjBmLIqK7dnkzAd4N3AscGBH3SboQeBn4y2pgS3opIgYcB+/u7o6FCxe21ICL5t/AVxePb2nf4Vh67lGjXma/3t5eenp6OlZ+J7jNY8NYa/Nw2yupboA3Mwa+HFgeEffl+euAfYDnJE3OB58MrGy5dmZmNmSDBnhEPAssk7RzXnQoaTjle8CsvGwWcENbamhmZnU1Oybxl8B8SRsBTwIfIYX/tZJOA54CTmxPFc3MrJ6mAjwiHgTWGn8h9cbNzKwD/EtMM7NCOcDNzArlADczK5QD3MysUA5wM7NCOcDNzArlADczK5QD3MysUA5wM7NCOcDNzAo1+n+f1Zq2eMVqZp9106iX28k/oWtmzXMP3MysUA5wM7NCOcDNzArlADczK5QD3MysUA5wM7NCOcDNzArlADczK5QD3MysUA5wM7NCOcDNzArlADczK5QD3MysUA5wM7NCOcDNzArlADczK5QD3MysUA5wM7NCOcDNzArlADczK5QD3MysUA5wM7NCOcDNzArlADczK5QD3MysUA5wM7NCOcDNzArVdIBLGifpJ5JuzPPvkHSfpCckXSNpo/ZV08zMag2lB34GsKQyfx5wfkTsCLwEnDaSFTMzs4E1FeCSpgJHAZfmeQGHANflTeYBx7ajgmZmVp8iYvCNpOuArwCbAZ8BZgP35t43kqYB34+I3ersOweYAzBp0qR9FyxY0FJFV764mudea2nXYdl9yhajX2g2Ftvc19dHV1dXx8rvBLd5/Tfc9s6YMWNRRHTXLh8/2I6SjgZWRsQiST1DLTgi5gJzAbq7u6OnZ8iHAOCi+Tfw1cWDVnfELT2lZ9TL7DcW29zb20ur50ip3Ob1X7va20w6HAj8qaQjgY2BzYELgYmSxkfEG8BUYMWI187MzBoadAw8Ij4fEVMjYjpwMnB7RJwC3AGckDebBdzQtlqamdlahnMf+OeAT0t6AtgauGxkqmRmZs0Y0gBrRPQCvXn6SWC/ka+SmZk1w7/ENDMrlAPczKxQDnAzs0I5wM3MCuUANzMrlAPczKxQDnAzs0I5wM3MCuUANzMrlAPczKxQDnAzs0I5wM3MCuUANzMrlAPczKxQDnAzs0I5wM3MCuUANzMrlAPczKxQDnAzs0I5wM3MCuUANzMrlAPczKxQDnAzs0I5wM3MCuUANzMrlAPczKxQ4ztdATMbexavWM3ss24a9XKXnnvUqJfZTu6Bm5kVygFuZlYoD6HYOmUsXlqPxTbbyHAP3MysUA5wM7NCOcDNzArlADczK5QD3MysUA5wM7NCOcDNzAo1aIBLmibpDkmPSnpE0hl5+VaSbpX08/zvlu2vrpmZ9WumB/4GcGZE7AocAHxc0q7AWcBtEbETcFueNzOzUTJogEfEMxHxQJ5+BVgCTAGOAeblzeYBx7arkmZmtjZFRPMbS9OBu4DdgKcjYmJeLuCl/vmafeYAcwAmTZq074IFC1qq6MoXV/Pcay3tOiy7T9li9AvN3ObR4zaPrrHW5r6+Prq6ulref8aMGYsiort2edN/C0VSF3A98KmIeDlldhIRIanuJ0FEzAXmAnR3d0dPT88Qq55cNP8Gvrp49P90y9JTeka9zH5u8+hxm0fXWGtzb28vrWbfQJq6C0XShqTwnh8R386Ln5M0Oa+fDKwc8dqZmVlDzdyFIuAyYElEfK2y6nvArDw9C7hh5KtnZmaNNHMNcyAwE1gs6cG87GzgXOBaSacBTwEntqeKZmZWz6ABHhF3A2qw+tCRrY6ZmTXLv8Q0MyuUA9zMrFAOcDOzQjnAzcwK5QA3MyuUA9zMrFAOcDOzQjnAzcwK5QA3MyuUA9zMrFAOcDOzQjnAzcwK5QA3MyuUA9zMrFAOcDOzQjnAzcwK5QA3MyuUA9zMrFAOcDOzQjnAzcwK1cz/Sm9mtl6YftZNHSn38iMmtOW47oGbmRXKAW5mVigHuJlZoRzgZmaFcoCbmRXKAW5mVigHuJlZoRzgZmaFcoCbmRXKAW5mVigHuJlZoRzgZmaFcoCbmRXKAW5mVigHuJlZoRzgZmaFcoCbmRVqWAEu6QhJj0t6QtJZI1UpMzMbXMsBLmkc8E/AB4BdgQ9K2nWkKmZmZgMbTg98P+CJiHgyIl4HFgDHjEy1zMxsMIqI1naUTgCOiIjT8/xMYP+I+ETNdnOAOXl2Z+DxFuu6DfBCi/uWym0eG9zm9d9w27tDRGxbu7Dt/yt9RMwF5g73OJIWRkT3CFSpGG7z2OA2r//a1d7hDKGsAKZV5qfmZWZmNgqGE+A/BnaS9A5JGwEnA98bmWqZmdlgWh5CiYg3JH0CuAUYB3wjIh4ZsZqtbdjDMAVym8cGt3n915b2tvwlppmZdZZ/iWlmVigHuJlZoYoI8LH2k31J35C0UtJPO12X0SBpmqQ7JD0q6RFJZ3S6Tu0maWNJ90t6KLf5S52u02iRNE7STyTd2Om6jAZJSyUtlvSgpIUjeux1fQw8/2T/Z8DhwHLS3S8fjIhHO1qxNpJ0MNAHXBERu3W6Pu0maTIwOSIekLQZsAg4dj1/jQVMiIg+SRsCdwNnRMS9Ha5a20n6NNANbB4RR3e6Pu0maSnQHREj/sOlEnrgY+4n+xFxF/Bip+sxWiLimYh4IE+/AiwBpnS2Vu0VSV+e3TA/1u3e1AiQNBU4Cri003VZH5QQ4FOAZZX55aznb+6xTNJ0YG/gvs7WpP3yUMKDwErg1ohY79sMXAB8Fvh9pysyigL4gaRF+U+LjJgSAtzGCEldwPXApyLi5U7Xp90i4ncRsRfpV8z7SVqvh8skHQ2sjIhFna7LKDsoIvYh/eXWj+ch0hFRQoD7J/tjQB4Hvh6YHxHf7nR9RlNErALuAI7odF3a7EDgT/OY8ALgEElXdrZK7RcRK/K/K4HvkIaFR0QJAe6f7K/n8hd6lwFLIuJrna7PaJC0raSJeXoT0pf0j3W2Vu0VEZ+PiKkRMZ30Pr49Ik7tcLXaStKE/MU8kiYA7wdG7O6ydT7AI+INoP8n+0uAa9v8k/2Ok3Q1cA+ws6Tlkk7rdJ3a7EBgJqlH9mB+HNnpSrXZZOAOSQ+TOim3RsSYuK1ujJkE3C3pIeB+4KaIuHmkDr7O30ZoZmb1rfM9cDMzq88BbmZWKAe4mVmhHOBmZoVygJuZFcoBbmZWKAe4mVmh/j+80FELFUEK/gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Weights for class 0: 0.43, class 1: 0.71, class 2: 1.51, class 3: 1.51, class 4: 1.46, class 5: 4.20\n",
            "Entering in k fold cross validation\n",
            "Dataset dim: 227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "46/46 [==============================] - 14s 168ms/step - loss: 1.9568 - accuracy: 0.2704 - precision: 0.5493 - recall: 0.0475 - tp: 7.9149 - fp: 6.7447 - tn: 951.1277 - fn: 183.6596 - val_loss: 1.8930 - val_accuracy: 0.2174 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_tp: 0.0000e+00 - val_fp: 1.0000 - val_tn: 229.0000 - val_fn: 46.0000\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.89299, saving model to SAR_swath_images_VV+VH+VH/classification_results/categorization/ResNet_Numeric-F_BatchSize-8_lr-0.0001_Epochs-44_Folds-5_Norm-T_Aug-T_FineTune-T_from165/best_model_frozen_1.h5\n",
            "Epoch 2/2\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 1.7406 - accuracy: 0.1816 - precision: 0.4238 - recall: 0.0059 - tp: 1.5957 - fp: 0.7872 - tn: 957.0851 - fn: 189.9787 - val_loss: 1.8214 - val_accuracy: 0.2609 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_tp: 0.0000e+00 - val_fp: 2.0000 - val_tn: 228.0000 - val_fn: 46.0000\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.89299 to 1.82141, saving model to SAR_swath_images_VV+VH+VH/classification_results/categorization/ResNet_Numeric-F_BatchSize-8_lr-0.0001_Epochs-44_Folds-5_Norm-T_Aug-T_FineTune-T_from165/best_model_frozen_1.h5\n",
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 400, 700, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 406, 706, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 200, 350, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 200, 350, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 200, 350, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 202, 352, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 100, 175, 64) 0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 100, 175, 64) 4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 100, 175, 64) 256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 100, 175, 64) 0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 100, 175, 64) 36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 100, 175, 64) 256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 100, 175, 64) 0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 100, 175, 256 16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 100, 175, 256 16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 100, 175, 256 1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 100, 175, 256 1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 100, 175, 256 0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 100, 175, 256 0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 100, 175, 64) 16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 100, 175, 64) 256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 100, 175, 64) 0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 100, 175, 64) 36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 100, 175, 64) 256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 100, 175, 64) 0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 100, 175, 256 16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 100, 175, 256 1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 100, 175, 256 0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 100, 175, 256 0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 100, 175, 64) 16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 100, 175, 64) 256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 100, 175, 64) 0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 100, 175, 64) 36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 100, 175, 64) 256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 100, 175, 64) 0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 100, 175, 256 16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 100, 175, 256 1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 100, 175, 256 0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 100, 175, 256 0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 50, 88, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 50, 88, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 50, 88, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 50, 88, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 50, 88, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 50, 88, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 50, 88, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 50, 88, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 50, 88, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 50, 88, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 50, 88, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 50, 88, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 50, 88, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 50, 88, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 50, 88, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 50, 88, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 50, 88, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 50, 88, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 50, 88, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 50, 88, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 50, 88, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 50, 88, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 50, 88, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 50, 88, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 50, 88, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 50, 88, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 50, 88, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 50, 88, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 50, 88, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 50, 88, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 50, 88, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 50, 88, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 50, 88, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 50, 88, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 50, 88, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 50, 88, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 50, 88, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 50, 88, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 50, 88, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 50, 88, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 50, 88, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 50, 88, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 25, 44, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 25, 44, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 25, 44, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 25, 44, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 25, 44, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 25, 44, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 25, 44, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 25, 44, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 25, 44, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 25, 44, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 25, 44, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 25, 44, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 25, 44, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 25, 44, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 25, 44, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 25, 44, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 25, 44, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 25, 44, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 25, 44, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 25, 44, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 25, 44, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 25, 44, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 25, 44, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 25, 44, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 25, 44, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 25, 44, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 25, 44, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 25, 44, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 25, 44, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 25, 44, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 25, 44, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 25, 44, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 25, 44, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 25, 44, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 25, 44, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 25, 44, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 25, 44, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 25, 44, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 25, 44, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 25, 44, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 25, 44, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 25, 44, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 25, 44, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 25, 44, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 25, 44, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 25, 44, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 25, 44, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 25, 44, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 25, 44, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 25, 44, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 25, 44, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 25, 44, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 25, 44, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 25, 44, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 25, 44, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 25, 44, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 25, 44, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 25, 44, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 25, 44, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 25, 44, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 25, 44, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 25, 44, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 13, 22, 512)  524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 13, 22, 512)  2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 13, 22, 512)  0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 13, 22, 512)  2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 13, 22, 512)  2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 13, 22, 512)  0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 13, 22, 2048) 2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 13, 22, 2048) 1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 13, 22, 2048) 8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 13, 22, 2048) 8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 13, 22, 2048) 0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 13, 22, 2048) 0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 13, 22, 512)  1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 13, 22, 512)  2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 13, 22, 512)  0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 13, 22, 512)  2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 13, 22, 512)  2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 13, 22, 512)  0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 13, 22, 2048) 1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 13, 22, 2048) 8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 13, 22, 2048) 0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 13, 22, 2048) 0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 13, 22, 512)  1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 13, 22, 512)  2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 13, 22, 512)  0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 13, 22, 512)  2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 13, 22, 512)  2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 13, 22, 512)  0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 13, 22, 2048) 1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 13, 22, 2048) 8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 13, 22, 2048) 0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 13, 22, 2048) 0           conv5_block3_add[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 23,587,712\n",
            "Trainable params: 4,465,664\n",
            "Non-trainable params: 19,122,048\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 2/12\n",
            "39/46 [========================>.....] - ETA: 0s - loss: 1.6996 - accuracy: 0.2571 - precision: 0.8718 - recall: 0.0091 - tp: 1.4615 - fp: 0.0000e+00 - tn: 800.0000 - fn: 158.5385"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4b9af316a3a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;31m##################################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;31m# test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m \u001b[0mfold_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNETWORK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMIN_HEIGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMIN_WIDTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_VARS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNORMALISE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mROTATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAUGMENT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBUFFER_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFOLDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-4b9af316a3a5>\u001b[0m in \u001b[0;36mfold_cross_validation\u001b[0;34m(main_dir, NETWORK, MIN_HEIGHT, MIN_WIDTH, NUM_VARS, NORMALISE, ROTATE, AUGMENT, LEARNING_RATE, EPOCHS, BUFFER_SIZE, BATCH_SIZE, SPLIT_NUMBER)\u001b[0m\n\u001b[1;32m    160\u001b[0m                             \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                             \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                             shuffle = True)\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mhistory_fine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ht1Z-KOohzD"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "CVGvPi-nodHv",
        "outputId": "5e4e2836-80fe-4aa3-b1d2-5141a254fcb9"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\r\n",
        "\r\n",
        "val_labels1 = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4,5,5]\r\n",
        "predictions1 =[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,2,2,3,1,1,1,3,4,0,3,3,3,4,3,3,4,4,4,4,5]\r\n",
        "\r\n",
        "val_labels2 = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5]\r\n",
        "predictions2 =[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,3,1,2,2,2,3,1,1,2,3,3,3,1,1,2,2,3,2,4]\r\n",
        "\r\n",
        "val_labels3 = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4,4,5]\r\n",
        "predictions3 =[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,0,1,1,1,1,2,2,2,3,4,4,0,0,0,1,2,1,2,2,3,4,1,4,4,4,5,5,4]\r\n",
        "\r\n",
        "val_labels4 = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4,5,5]\r\n",
        "predictions4 =[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,2,1,1,1,1,1,1,1,2,3,3,1,1,1,3,3,2,2,3,4,4,1,1,4,4,4,0,3]\r\n",
        "\r\n",
        "val_labels5 = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4,5,5]\r\n",
        "predictions5 =[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,3,4,0,1,1,1,3,1,1,3,3,3,1,3,4,4,4,0,3]\r\n",
        "\r\n",
        "\r\n",
        "val_labels = val_labels1 + val_labels2 + val_labels3 + val_labels4 + val_labels5\r\n",
        "predictions = predictions1 + predictions2 + predictions3 + predictions4 + predictions5\r\n",
        "\r\n",
        "conf_mat = confusion_matrix(val_labels5, predictions5, labels = [0,1,2,3,4,5])\r\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix = conf_mat, display_labels = [0,1,2,3,4,5])\r\n",
        "conf_mat_display = disp.plot()\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "#plt.savefig(\"SAR_swath_images_VV+VH+WS/classification_results/categorization/ResNet_Numeric-F_BatchSize-8_lr-0.0001_Epochs-20_Folds-5_Norm-T_Aug-T/global_confusion_matrix.jpg\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATIAAAEGCAYAAADmLRl+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZhU1Z3/8fenuqvZZGubXQyQII7jIPjrqMSMgzqjxviTzCSZ7M9klhhRk5gY+emYjNE8Ms5kMslMVAyjTmLcQqIMJnFpJRLRERQIKgiIISyyRJpFVLburu/vj7qNDULVvdVVfU+139fz3Ieu6lvnfLmFX8899ywyM5xzrppl0g7AOec6yxOZc67qeSJzzlU9T2TOuarnicw5V/Vq0w6go4b6Ghs1Mpt2GAesfqlf2iG8g7W2ph2Cq2J7eYv9tk+dKePcM/vYtu1tsc5d/MK+R83svM7UF0dQiWzUyCzPPjoy7TAOOP+kv0g7hHdo27o17RBcFVtocztdxrbtbTz76LGxzq0Ztrqh0xXGEFQic86Fz4AcubTDOIgnMudcIobRYvFuLbuKJzLnXGLeInPOVTXDaAtsaqMnMudcYjk8kTnnqpgBbZ7InHPVzltkzrmqZkBLYH1kPkXJOZeIYbTFPIqRdIek1yQtO+T9L0laKWm5pH8tVo63yJxzyRi0la9B9iPgJuDO9jcknQlMAU4ys32SBhcrxBOZcy6R/Mj+MpVl9qSkUYe8PRW40cz2Ree8Vqwcv7V0ziUk2mIeQIOkRR2Oi2JUcBzwp5IWSvqNpPcX+0BVt8i++9WRLHy8HwMaWpn5xCoAbvjie3j1dz0BeGtXDX36tTHj8VWpxHf5dcs55Yxmdm6v45KPTkolhkM1Tt7Fxd/eRE3GePjeembdNMTj8XgSyXf2x15Ao9nMGhNWUQvUA6cB7wdmSRpjBTYYqWiLTNJ5klZJekXSVeUu/5xPbOeGu9cc9N41P1zHjMdXMePxVZz+4Z2cfv7Oclcb2+NzhvPNqRNTq/9QmYxx6fSNfOMzo/nC5HGcOWUnx47d6/F4PInkx5HFbpGV4lXgAct7lvydbMFVNCqWyCTVADcDHwJOAD4l6YRy1vEnp71F34GHn7xqBk8+OIAzP7KjnFUmsmzJQN7YFc76auMm7mbT2jq2rO9Ba0uGeXMGMOnc1z0ejyexnCnWUaL/Ac4EkHQcUAc0F/pAJVtkpwCvmNkaM9sP3Ef+SUSXWLawDwMHtTJizP6uqjJ4Rw9tYeumugOvmzdnaRjW4vF4PImUs0Um6V7gGWCcpFcl/T1wBzAmGpJxH/A3hW4robJ9ZCOADR1evwqceuhJUeffRQDHjihfOE/8z0Amp9gac667MkRbmdpAZvapI/zqs0nKSf2ppZnNNLNGM2scdHRNWcpsa4WnH+rPn12YXv9YiLZtyTJo+Nst1IZhLTRvTu/W1+Oprng6qvCtZWKVTGQbgY7rVh8TvVdxS+b3ZeT79jFoePrN8JCsWtqbEaP3M2TkPmqzOSZP2cmCpv4ej8eTiCH2W02so6tU8tbyOWCspNHkE9gngU+Xs4J/nvoeXnjmKF7fXstn/s8JfO6KLZz36e38Zk4Yt5XTbnyR8Y076DeghTub5nPXjDE0zR6RWjy5NnHzNSOYfs8aMjXQdF89617u6fF4PInkB8SmfjN3EBXpQ+tc4dL5wPeBGuAOM7uh0PmNJ/U033ykMN98xHXGQpvLLtveqXu+ceN72owH3xPr3LNHv7y4hHFkiVV0QKyZPQQ8VMk6nHNdy0y0WVgtsqoe2e+cS0eu9MGuFeGJzDmXSL6zP6zUEVY0zrnghdjZ74nMOZdYWxeOEYvDE5lzLpFyjuwvF09kzrnEcv7U0jlXzfKTxj2ROeeqmCFaunD6URyeyJxziZjhA2Kdc9VOPiDWOVfdDG+ROee6Ae/sL+ClzYOYeMMlaYdxwLCGAFeaCGz1i5pBg9IOIXjdbcUSo3yLJkq6A7gAeM3MTjzkd1cA/wYMMrPU1ux3znVD+e3gamMdMfwIOO/QNyWNBM4B1scpxBOZcy6hRBv0FmRmTwLbD/Or7wHTyOfNooK6tXTOhc9INLK/QdKiDq9nmtnMQh+QNAXYaGbPS/FuYT2ROecSS7D5bqKdxiX1Bv6R/G1lbJ7InHOJmKmScy3fC4wG2ltjxwBLJJ1iZluO9CFPZM65RPKd/ZWZomRmLwKD219LWgs0+lNL51yZ5dfsj3MULenwO40n5i0y51wi+c7+8owjK7DTePvvR8UpxxOZcy4xH9nvnKtq5RzZXy6eyJxzifnmI865qmYGLTlPZM65Kpa/tfRE5pyrcglG9neJbpXIjuqxj2s/PI/3DtqOAdf98kxe2Dg0lVgaBu3miqueY+DAvZiJR341mjkPjE0llo4aJ+/i4m9voiZjPHxvPbNuGpJaLJdft5xTzmhm5/Y6LvnopNTi6Ci0mEL6vtqVc/hFuVQskRVaZ6hSpp3zFP+7ZiRXPnAutZk2emZbu6Law2prE7fdOp7frR5Ir14t/Oetc1myeAgb1vVLLaZMxrh0+kau/uQYmjdn+cFDq1nwaH/Wr+6ZSjyPzxnOL+4dyRU3LE+l/sMJKabQvq+3hXdrWclofsRh1hmqlKN67OPkYzcze+kfAdCaq+HNfT26qvp32LG9F79bPRCAPXuyrF/Xl4aGPanFAzBu4m42ra1jy/oetLZkmDdnAJPOfT21eJYtGcgbu7Kp1X84IcUU2vfVUS5at7/Y0VUq1iIzsycljapU+YcaPuANduzuxXUXPMFxQ7axYksD/9r0Qfa2pP+PcvCQt3jv+3ayckV9qnEcPbSFrZvqDrxu3pzl+JN3pxiRKyTU7yv/1DKs7eBSbx9KukjSIkmLWve8VXI5tZkcxw/dys+W/DGfuv3j7Nmf5e8+8NsyRlqanj1bueZbzzDzlgns2Z1+UnWus9oHxMY5ukrqiczMZppZo5k11vbqU3I5f9h1FK/tOoplm/KdoY+vHMPxQ9NdK72mJsc133qGeXOP5X+fGpFqLADbtmQZNHz/gdcNw1po3uzJNVQhf1+h3VqmnsjKZdtbvdmyqw/vqd8BwCmjNrJm68AUIzIu//oiNqzvy+yfH5diHG9btbQ3I0bvZ8jIfdRmc0yespMFTf3TDssdQajfV/tTy5BaZN1q+MW/NP0p0z8yl9pMGxt39uPaX56VWiwnnLiNs89Zz+/X9OcHP3wMgB/ffiKLnh2WWky5NnHzNSOYfs8aMjXQdF89615O7wnYtBtfZHzjDvoNaOHOpvncNWMMTbPTbbmGFFNo31dHoT21lFmstf2TF5xfZ2gy0AD8AbjWzG4v9Jneg0fa2E98rSLxlGLYr8Pbxqttxeq0QziIbwdXXEjbwS20ueyy7Z1qKg08frCddcfHYp37wOkzFidZ6rpUlXxqWXCdIedc9QptQGxY7UPnXPDK2Ucm6Q5Jr0la1uG970haKekFSbMlDShWjicy51xiZezs/xHvHDj/GHCimY0HXgauLlaIJzLnXCLlHEd2uA16zazJzNrnFy4gv5NSQd3qqaVzrmskGCOWeIPeQ/wd8NNiJ3kic84lYgat8RdWTLRBb0eSrgFagbuLneuJzDmXWKWfWkr6PPnVc862GGPEPJE55xKp9OYjks4DpgF/ZmaxZsl7Z79zLjEzxTqKOcIGvTcBfYHHJC2VdGuxcrxF5pxLrFwTwo8wcL7gDKDD8UTmnEvELLyR/Z7InHMJiTbfDs45V+3i9H91paASWXbHPobNCmd1h9bj0l8M8VBh/fMJT0grTXRX76pdlJxz3ZTl+8lC4onMOZdYVy5jHYcnMudcIuad/c657sBvLZ1zVc+fWjrnqpqZJzLnXDfgwy+cc1XP+8icc1XNEDl/aumcq3aBNcg8kTnnEvLOfudctxBYk8wTmXMusappkUn6AQXyrpl9uSIRdcLl1y3nlDOa2bm9jks+OintcMhm2/ju9Q+Trc1RU5Nj/oJR/GTWhFRjapy8i4u/vYmajPHwvfXMumlIarGE9n1BWNcnxHggWv0iV55EJukO8puMvGZmJ0bv1ZPfAm4UsBb4azPbUaicQo8eFgGLCxzFAhwp6QlJL0laLukrxT7TWY/PGc43p06sdDWxtbRkmHbduUy98kKmXnkh75+wkePHprfMTCZjXDp9I9/4zGi+MHkcZ07ZybFj96YWT2jfV2jXJ7R4DjDAFO8o7ke8c6fxq4C5ZjYWmBu9LuiILTIz+3HH15J6x93RJNIKXGFmSyT1BRZLeszMXkpQRiLLlgxk8PA9lSq+BGLv3iwAtTX5VlmafQvjJu5m09o6tqzvAcC8OQOYdO7rrF/dM5V4Qvu+Qrs+ocXTUbnGkZnZk5JGHfL2FGBy9POPgXnA/ytUTtHBIJImSXoJWBm9PknSLTEC3GxmS6Kf3wBWAOGtVFhhmUyOGd95kFm3/5QlLwxn5SuDUovl6KEtbN1Ud+B18+YsDcNaUosnNKFdn9DiOYjFPKKdxjscF8UofYiZbY5+3gIUvZ+O09n/feBc4EEAM3te0hkxPndAlHEnAgsP87uLgIsAemaOSlJsVcjlMky98kL69N7PtVc+waiRO1i7YWDaYTnXCfG2eouUvNM4gJmZpKLtv1jDc81swyFvtcUNRNJRwP3A5Wa26zBlzzSzRjNrrMv0ilts1Xlrdx3PLx9K44SNqcWwbUuWQcP3H3jdMKyF5s3Z1OIJTWjXJ7R4DhK/RVaKP0gaBhD9+VqxD8RJZBskfQAwSVlJXyd/m1iUpCz5JHa3mT0Q5zPdSf9+e+nTO/8Psa6ulZPHb2LDxv6pxbNqaW9GjN7PkJH7qM3mmDxlJwua0osnNKFdn9DiOcDAcop1lOhB4G+in/8GmFPsA3FuLS8G/oN8/9Ym4FHg0mIfkiTyG22uMLN/j1FPp0278UXGN+6g34AW7myaz10zxtA0O71uufoBu7nysqfJZIyMjN88M4qFS0amFk+uTdx8zQim37OGTA003VfPupfT6zgO7fsK7fqEFs/Byjb84l7yHfsNkl4FrgVuBGZFu46vA/66aDlWoWnskj4IzAdeBHLR2/9oZg8d6TP9s4NtUv3HKhJPKYLcRenppWmHcJCaQek9vDgc30WpsIU2l122vVNZqMfoY2zYt74U69x1n79qcWf6yOIq2iKTNIZ8i+w08ne9zwBfNbM1hT5nZk/hu5c51z0FNkUpTh/ZPcAsYBgwHPgZcG8lg3LOBay8A2LLIk4i621mPzGz1ui4CwjlRt05lwKzeEdXKTTXsj768WFJVwH3kc/FnwCO2M/lnHsXKNNcy3Ip1Ee2mHziao/4ix1+Z8DVlQrKORe24kNUu1ahuZajuzIQ51yV6Nxg14qItR6ZpBOBE+jQN2Zmd1YqKOdcyLq2Iz+OOMMvriU/YO0E8n1jHwKeAjyROfduFViLLM5Ty48BZwNbzOxvgZOAAOZJOOdSk4t5dJE4t5Z7zCwnqVVSP/ITONObZ+OcS1f7OLKAxElkiyQNAP6L/JPMN8mP7nfOvUtVzVPLdmZ2SfTjrZIeAfqZ2QuVDcs5F7RqSWSSTi70u/bVX51zLm2FWmTfLfA7A84qcyxYa2tQqxfUNgxIO4R3iL2iZRcJbYUQBfTvpzurmltLMzuzKwNxzlUJo6qmKDnn3OEF1iKLtWa/c851JIt3FC1H+mq07+0ySfdKKmllHU9kzrnkyrD5iKQRwJeBxmiX8Rrgk6WEE2dfS0n6rKR/il4fK+mUUipzznUT5dtFqRboJakW6E1+X5DE4rTIbgEmAZ+KXr8B3FxKZc656hf3tlJFNug1s43AvwHrgc3A62bWVEpMcTr7TzWzkyX9Nqp8h6S6Yh9yznVj8Z9aHnGDXkkDgSnAaGAn8DNJn41WoU4kTousRVINUUNR0iC6dDqocy40Zers/3Pg92a21cxagAeAD5QST5xE9p/AbGCwpBvIL+EzvZTKnHPdRHn6yNYDp0nqHe2DezYxN/8+VJy5lndLWhxVIuAjZlZSZc65biDm0IqixZgtlPRzYAnQCvwWmFlKWXEWVjwW2A38ouN7Zra+lAqdc91AmQbEmtm15HcX75Q4nf2/4u1NSHqS75hbBfxxZyt3zlUnBdZLHufW8k86vo5WxbjkCKc751yXSzzX0syWSDq1EsF0VuPkXVz87U3UZIyH761n1k1DUoulYdBurrjqOQYO3IuZeORXo5nzwNjU4mkX0jXKZtv47vUPk63NUVOTY/6CUfxk1oTU4oGwrk+I8RwQ2FzLOH1kX+vwMgOcTIzRt9GcqSeBHlE9P4/uhysikzEunb6Rqz85hubNWX7w0GoWPNqf9avT2RS9rU3cdut4frd6IL16tfCft85lyeIhbFjXL5V4ILxr1NKSYdp157J3b5aamhzf+/bDPPfbEaxcPSiVeEK7PqHFc0CZOvvLKc7wi74djh7k+8ymxPjcPuAsMzsJmACcJ+m0UgMtZtzE3WxaW8eW9T1obckwb84AJp37eqWqK2rH9l78bvVAAPbsybJ+XV8aGvakFg+Ed41A7N2bBaC2Jt8qS/P/9KFdn9DiOUj5piiVRcEWWTQQtq+ZfT1pwWZm5Nf3B8hGR8X+akcPbWHrprcnHDRvznL8ybsrVV0ig4e8xXvft5OVK+pTjSPEa5TJ5Lj5X37J8KFv8OAjx7PylXRaYxDe9QktnoNUS4tMUq2ZtQGnl1q4pBpJS8nvvPSYmS08zDkXtc/DamFfqVUFq2fPVq751jPMvGUCe3Zn0w4nOLlchqlXXsinv/hxxr2vmVEjd6QdkitC5J9axjm6SqFby2ejP5dKelDS5yT9VfsRp3AzazOzCcAxwCnRjuWHnjPTzBrNrDFLj+R/g8i2LVkGDd9/4HXDsBaaN6ebOGpqclzzrWeYN/dY/vep9JeEDvEatXtrdx3PLx9K44SNqcUQ2vUJLZ4Dkk0a7xJx+sh6AtvIr9F/AfB/oz9jM7OdwBPAeUkDjGvV0t6MGL2fISP3UZvNMXnKThY0pbmPsHH51xexYX1fZv/8uBTjeFto16h/v7306Z3/D7WurpWTx29iw8b04gnt+oQWz0GqqI9scPTEchlvD4htVzTEaHJ5i5ntlNQL+AvgXzoTbCG5NnHzNSOYfs8aMjXQdF89615O7+nOCSdu4+xz1vP7Nf35wQ8fA+DHt5/IomeHpRZTaNeofsBurrzsaTIZIyPjN8+MYuGS9PZ+Du36hBbPQQLrI1O+T/4wv5A2AzM4OIG1MzO7vmDB0njgx+RXfcwAs4p9pp/q7VSdHSfuLlHzR+mP+zpU24rVaYdwEDs93XFfh9LTS9MOIWgLbS67bHundg7pNWykjfn814qfCLx049cWH2kZn3Iq1CLbXCzxFBJt4jux1M875wIWWIusUCILa78n51wYrLrmWoZzj+ecC0u1tMjMbHtXBuKcqx6hTVHyDXqdc8l5InPOVbUuHiMWh2/Q65xLRJR1p/EBkn4uaaWkFZImlRKTt8icc4mVsY/sP4BHzOxj0TaTvUspxBOZcy65MiQySf2BM4DPA5jZfmB/oc8cid9aOueSiz/X8og7jZPf/2Mr8N+SfivpNkl9SgnHE5lzLplkq180t69uEx0dt3urJb/i9Awzmwi8BVxVSkieyJxzyZVn9YtXgVc7rFP4c/KJLTFPZM65xMqxsKKZbQE2SBoXvXU28FIp8XhnfyHNO9OOIHi1L6e3EOLhtAa2Ggd0zxU5yvjU8kvA3dETyzXA35ZSiCcy51wyZRwQa2ZLgU4v8+OJzDmXXGAj+z2ROecSaR/ZHxJPZM65xJQLK5N5InPOJRPgpHFPZM65xPzW0jlX/TyROeeqnbfInHPVzxOZc66qVdkuSs459w4+jsw51z1YWJnME5lzLjFvkVVQ4+RdXPztTdRkjIfvrWfWTUNSjefy65ZzyhnN7NxexyUfLWlPhbIL6RqFdn2y2Ta+e/3DZGtz1NTkmL9gFD+Zle5qGiF9XwcEOCC24uuRSaqJlrH9ZSXryWSMS6dv5BufGc0XJo/jzCk7OXbs3kpWWdTjc4bzzakTU42ho9CuUWjXp6Ulw7TrzmXqlRcy9coLef+EjRw/dmtq8YT2fXVUjvXIyqkrFlb8CrCi0pWMm7ibTWvr2LK+B60tGebNGcCkc1+vdLUFLVsykDd2ZVONoaPQrlFo1wfE3r35eGpr8q2yNFseoX1fHb2rEpmkY4APA7dVsh6Ao4e2sHVT3YHXzZuzNAxrqXS1VcWvUXGZTI4Z33mQWbf/lCUvDGflK4NSiyXY78vId/bHObpIpVtk3wemAUfMzZIuat9hpYV9FQ7HucJyuQxTr7yQT3/x44x7XzOjRu5IO6QglWuDXihP91PFEpmkC4DXzGxxofPMbGb7DitZepRc37YtWQYNf3tLvIZhLTRvDum2JX1+jeJ7a3cdzy8fSuOE9JbyDvr7Ks/mI+063f1UyRbZ6cCFktYC9wFnSbqrUpWtWtqbEaP3M2TkPmqzOSZP2cmCpv6Vqq4q+TUqrH+/vfTpnU8cdXWtnDx+Exs2pnd9Qv2+2gfElqNFVq7up4oNvzCzq4GrASRNBr5uZp+tVH25NnHzNSOYfs8aMjXQdF89617uWanqYpl244uMb9xBvwEt3Nk0n7tmjKFp9ojU4gntGoV2feoH7ObKy54mkzEyMn7zzCgWLhmZWjyhfV8HmJVzYcX27qe+nSlE1gUdch0S2QWFzuunejtVZ1c8nrhqBqXX0XskbVvTGw5wOKFdo9bj0kuERxLSLkoLbS67bLs6U0bfAcfYxDO+Euvc+b+Ytg5o7vDWzPZNeqPup/PN7JK4OeJIumRArJnNA+Z1RV3OucpLMLK/2cyOtEtSe/fT+UBPoJ+ku0q5c/MNep1zyRiQs3hHoWLMrjazY8xsFPBJ4Neldj91qylKzrkuEtgUJU9kzrnEyj1pvLPdT57InHOJ+XZwzrnqFuDqF57InHOJ5AfEhpXJPJE555LzNfudc9XOW2TOuermfWTOuepX1rmWZeGJzDmXnN9aOueqmm/Q65zrFrxFdmSqraWmPpxlYUJbMscVF9KSOd1aWHksrETmnKsOyoV1b+mJzDmXjOEDYp1z1U2YD4h1znUDnsicc1XPE5lzrqoF2Efma/Y75xJTLhfrKFiGNFLSE5JekrRcUrytmQ7DW2TOuYSsXLeWrcAVZrZEUl9gsaTHzOylpAV5InPOJWOUJZGZ2WZgc/TzG5JWACMAT2TOuS4Qv4+sQdKiDq8PbNDbkaRRwERgYSnheCJzziWWYBxZoQ1682VJRwH3A5eb2a5S4vFE5pxLrkzDLyRlySexu83sgVLL8UTmnEvGDNo6P/5CkoDbgRVm9u+dKatbDb+4/Lrl3PPEb7jl/mfSDgWAxsm7uG3+Sv776RX89WV/SDscIKyYQvu+IKzrE2I8B5jFOwo7HfgccJakpdFxfinhVDSRSVor6cUowEXFP9E5j88ZzjenTqx0NbFkMsal0zfyjc+M5guTx3HmlJ0cO3avx9RBSN8XhHd9QovnIGVIZGb2lJnJzMab2YToeKiUcLqiRXZmFGDBDr9yWLZkIG/syla6mljGTdzNprV1bFnfg9aWDPPmDGDSua97TB2E9H1BeNcntHgOMCBn8Y4u0q1uLUNy9NAWtm6qO/C6eXOWhmEtKUYUZkwhCe36hBbP2wwsF+/oIpVOZAY0SVos6aLDnSDpIkmLJC3an9tT4XCcc51m5Dv74xxdpNJPLT9oZhslDQYek7TSzJ7seEI0OG4mQP/s4LCm1HfCti1ZBg3ff+B1w7AWmjenexsVYkwhCe36hBbPQQJb/aKiLTIz2xj9+RowGzilkvWFZNXS3owYvZ8hI/dRm80xecpOFjT195gCFtr1CS2eg5TnqWXZVKxFJqkPkInmUPUBzgGur1R9ANNufJHxjTvoN6CFO5vmc9eMMTTNHlHJKo8o1yZuvmYE0+9ZQ6YGmu6rZ93LPVOJJdSYQvq+ILzrE1o8b+vaJBWHrEIBSRpDvhUG+YR5j5ndUOgz/bODbVL9xyoSTyl8F6XiagaFs+sV+HdWzEKbyy7brs6U0T872D7Q8PFY5z6y5ZbFXTFioWItMjNbA5xUqfKdcykKrEXmU5SccwmVZ4pSOXkic84lY2BdOEYsDk9kzrnkunDUfhyeyJxzyXkfmXOuqplBkY1FuponMudcct4ic85VN8Pa2tIO4iCeyJxzybQv4xMQT2TOueQCG37h65E55xIxwHIW6yhG0nmSVkl6RdJVpcbkicw5l4yVZ2FFSTXAzcCHgBOAT0k6oZSQ/NbSOZdYmTr7TwFeieZlI+k+YAol7DResdUvSiFpK7CuDEU1AM1lKKdcPJ7CQosHwoupXPG8x8w6tWSJpEeieOLoCXTcMeXATuOSPgacZ2b/EL3+HHCqmV2WNKagWmSdvcDtJC3qiqVD4vJ4CgstHggvppDiMbPz0o7hUN5H5pxLy0ZgZIfXx0TvJeaJzDmXlueAsZJGS6oDPgk8WEpBQd1altHMtAM4hMdTWGjxQHgxhRZPp5lZq6TLgEeBGuAOM1teSllBdfY751wp/NbSOVf1PJE556pet0pk5ZruUMZ47pD0mqRlaccCIGmkpCckvSRpuaSvpBxPT0nPSno+iue6NONpJ6lG0m8l/TLtWAAkrZX0oqSlkhalHU+Iuk0fWTTd4WXgL4BXyT8R+ZSZJR4lXMaYzgDeBO40sxPTiqNDPMOAYWa2RFJfYDHwkbSukSQBfczsTUlZ4CngK2a2II14OsT1NaAR6GdmF6QZSxTPWqDRzEIaoBuU7tQiOzDdwcz2A+3THVJjZk8C29OMoSMz22xmS6Kf3wBWAKntiGt5b0Yvs9GR6v9ZJR0DfBi4Lc04XDLdKZGNADZ0eP0qKf5HGjpJo4CJwMKU46iRtBR4DXjMzFKNB/g+MA0IaZ0aA5okLZZ0UdrBhKg7JTIXk6SjgPuBy81sV5qxmFmbmU0gP6r7FEmp3YJLugB4zcwWpxXDEXzQzE4mv0rEpVGXheugOyWysk136M6ivqj7gbvN7IG042lnZjuBJ4A05wLKFEkAAAPoSURBVPGdDlwY9UndB5wl6a4U4wHAzDZGf74GzCbfjeI66E6JrGzTHbqrqHP9dmCFmf17APEMkjQg+rkX+Qc1K9OKx8yuNrNjzGwU+X8/vzazz6YVD4CkPtGDGST1Ac4BgngKHpJuk8jMrBVon+6wAphV6nSHcpF0L/AMME7Sq5L+Ps14yLc4Pke+pbE0Os5PMZ5hwBOSXiD/P6LHzCyIIQ8BGQI8Jel54FngV2b2SMoxBafbDL9wzr17dZsWmXPu3csTmXOu6nkic85VPU9kzrmq54nMOVf1PJFVEUlt0ZCJZZJ+Jql3J8r6UbSLDZJuK7SfoKTJkj5QQh1rJb1jt50jvX/IOW8W+v1hzv+WpK8njdF1D57IqsseM5sQraSxH7i44y8llbR0uZn9Q5EVMCYDiROZc13FE1n1mg+8L2otzZf0IPBSNAn7O5Kek/SCpC9CflS/pJui9doeBwa3FyRpnqTG6OfzJC2J1gibG00uvxj4atQa/NNoRP79UR3PSTo9+uzRkpqitcVuA1TsLyHpf6LJ0MsPnRAt6XvR+3MlDYree6+kR6LPzJd0fDkupqtu3XXzkW4tanl9CGgf4X0ycKKZ/T5KBq+b2fsl9QCeltREfqWLceS3ph9CfjfnOw4pdxDwX8AZUVn1ZrZd0q3Am2b2b9F59wDfM7OnJB1LfjbFHwHXAk+Z2fWSPgzEmcnwd1EdvYDnJN1vZtuAPsAiM/uqpH+Kyr6M/CYcF5vZakmnArcAZ5VwGV034omsuvSKlryBfIvsdvK3fM+a2e+j988Bxrf3fwH9gbHAGcC9ZtYGbJL068OUfxrwZHtZZnaktdT+HDghP3UTgH7RihpnAH8VffZXknbE+Dt9WdJfRj+PjGLdRn4ZnZ9G798FPBDV8QHgZx3q7hGjDtfNeSKrLnuiJW8OiP6DfqvjW8CXzOzRQ84r55zKDHCame09TCyxSZpMPilOMrPdkuYBPY9wukX17jz0GjjnfWTdz6PA1Gi5HiQdF62a8CTwiagPbRhw5mE+uwA4Q9Lo6LP10ftvAH07nNcEfKn9haT2xPIk8OnovQ8BA4vE2h/YESWx48m3CNtlgPZW5afJ37LuAn4v6eNRHZJ0UpE63LuAJ7Lu5zby/V9LlN/05IfkW96zgdXR7+4kvyrHQcxsK3AR+du453n71u4XwF+2d/YDXwYao4cJL/H209PryCfC5eRvMdcXifURoFbSCuBG8om03VvkF1pcRr4P7Pro/c8Afx/Ft5yUlzN3YfDVL5xzVc9bZM65queJzDlX9TyROeeqnicy51zV80TmnKt6nsicc1XPE5lzrur9fz250DO5I43IAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}