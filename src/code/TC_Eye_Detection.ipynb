{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"TC_Eye_Detection.ipynb","provenance":[],"collapsed_sections":["sUEMhiDjZ9bi"]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"Jnxs1L1PWjeL"},"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YCk96rRrM1T-"},"source":["%cd /content/drive/My Drive/ESRIN_PhiLab/Tropical_Cyclones/tropical_cyclones/src/code\n","%ls\n","\n","import imp \n","# import helper.py\n","helper = imp.new_module('helper_functions')\n","exec(open(\"./helper_functions.py\").read(), helper.__dict__)\n","# import models.py\n","models = imp.new_module('models')\n","exec(open(\"./models.py\").read(), models.__dict__)\n","# import data_processor.py\n","dp = imp.new_module('data_processor')\n","exec(open(\"./data_processor.py\").read(), dp.__dict__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4p-mSrzKWqUb"},"source":["# insert your desired path to work on\n","%cd /content/drive/My Drive/ESRIN_PhiLab/Tropical_Cyclones/data\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PZu5fkuXWxdC"},"source":["# general imports\n","import random\n","import glob\n","import os\n","import sys\n","sys.stdout.flush()\n","import pandas as pd\n","import numpy as np\n","import cv2\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","import math\n","import imageio\n","import os.path\n","import time\n","from PIL import Image\n","from mpl_toolkits.axes_grid1 import make_axes_locatable\n","from scipy import ndimage\n","from google.colab.patches import cv2_imshow\n","import random\n","from shapely.geometry import Point\n","import re\n","import pickle\n","import scipy\n","from sklearn.model_selection import StratifiedKFold\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n","import datetime\n","\n","import tensorflow as tf\n","from tensorflow.data import Dataset\n","from tensorflow.keras import Input\n","from tensorflow.keras.applications import resnet50, mobilenet_v2\n","from tensorflow.keras.applications.resnet50 import ResNet50\n","from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import concatenate, Dense, GlobalAveragePooling2D\n","from tensorflow.keras.optimizers import SGD, Adam\n","from tensorflow.keras.metrics import BinaryAccuracy, Precision, Recall, TruePositives, FalsePositives, TrueNegatives, FalseNegatives\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","np.set_printoptions(precision=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sUEMhiDjZ9bi"},"source":["## Train on data according to csv split into train, val and test sets.\n","\n","Prepare the tf.data.Dataset instances to be fed to the model."]},{"cell_type":"code","metadata":{"id":"ySpk4sOVemVz"},"source":["##################################\n","###          SETTINGS         ####\n","##################################\n","main_dir = \"SAR_swath_images_VV+VH+WS\"\n","NETWORK = \"ResNet\"     # options: [\"Mobile\", \"ResNet\"]\n","MIN_HEIGHT = 700\n","MIN_WIDTH = 400\n","NUM_VARS = False\n","NORMALISE = True\n","ROTATE = False\n","BATCH_SIZE = 16  # creates batches of 16 elements\n","BUFFER_SIZE = 100\n","EPOCHS = 10\n","LEARNING_RATE = 0.0001\n","##################################\n","\n","# load data\n","train_images, train_labels, train_bbox = helper.load_data(\"{}/csv/training.csv\".format(main_dir), with_cat = False)\n","val_images, val_labels, val_bbox = helper.load_data(\"{}/csv/val.csv\".format(main_dir), with_cat = False)\n","test_images, test_labels, test_bbox = helper.load_data(\"{}/csv/test.csv\".format(main_dir), with_cat = False)\n","\n","\n","# create an instance of the DataProcessor\n","processor = dp.DataProcessor(model = NETWORK,\n","                             min_height = MIN_HEIGHT,\n","                             min_width = MIN_WIDTH,\n","                             normalise = NORMALISE,           # perform normalisation\n","                             rotate = ROTATE,                 # perform rotation\n","                             plot_light = False,              # plot only select_crop() images\n","                             plot_extensive = False,          # plot extensively all images\n","                             show_prints = False)\n","\n","\n","# generate datasets\n","train_dataset = helper.prepare_dataset(processor, train_images, train_labels, train_bbox)\n","val_dataset = helper.prepare_dataset(processor, val_images, val_labels, val_bbox)\n","test_dataset = helper.prepare_dataset(processor, test_images, test_labels, test_bbox)\n","#for image, label in train_dataset:\n","#  plt.imshow(image)\n","#  plt.show()\n","#  print(\"FINAL - image: {}, label: {}\".format(image.shape, label))\n","#  print(\"FINAL - image: {}, max: {}, min: {}, label: {}\".format(image.shape, np.max(image), np.min(image),  label))\n","\n","\n","# configure for performance\n","train_dataset = helper.configure_for_performance(train_dataset, BUFFER_SIZE, BATCH_SIZE, shuffle = True)\n","val_dataset = helper.configure_for_performance(val_dataset, BUFFER_SIZE, BATCH_SIZE)\n","test_dataset = helper.configure_for_performance(test_dataset, BUFFER_SIZE, BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H9-tkgGrid0-"},"source":["Script to perform end-to-end training in the model."]},{"cell_type":"code","metadata":{"id":"w0eTiiKghBZg"},"source":["# directory to save results\n","dir = '{}_Numeric-{}_BatchSize-{}_Epochs-{}_Norm-{}'.format(NETWORK, str(NUM_VARS)[0], str(BATCH_SIZE), str(EPOCHS), str(NORMALISE)[0])\n","save_dir = 'classification_results/identification/' + dir + '/'\n","os.makedirs(save_dir, exist_ok=True)\n","\n","# MAKE MODEL\n","if NETWORK == \"Mobile\":\n","    model = models.make_MobileNet(MIN_WIDTH, MIN_HEIGHT, LEARNING_RATE, detection = True)\n","elif NETWORK == \"ResNet\":\n","    model = models.make_ResNet(MIN_WIDTH, MIN_HEIGHT, LEARNING_RATE, detection = True)\n","else:\n","  sys.exit(\"Incert valid network model. Options: Mobile or ResNet (case sensitive)\")\n","\n","# CREATE CALLBACKS\n","callbacks = [\n","    EarlyStopping(patience = 5, verbose = 1),\n","    ReduceLROnPlateau(factor = 0.1, patience = 5, min_lr = 0.00001, verbose = 1),\n","    ModelCheckpoint(save_dir + \"best_model.h5\", verbose = 1, save_best_only = True),\n","    TensorBoard(log_dir = save_dir + \"logs/\" + datetime.datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"))\n","]\n","\n","# TRAIN THE NETWORK\n","H = model.fit(\n","    train_dataset,\n","    steps_per_epoch = len(train_dataset),\n","    validation_data = val_dataset,\n","    validation_steps = len(val_dataset),\n","    epochs = EPOCHS,\n","    callbacks = callbacks,\n","    verbose = 1,\n","    class_weight = {0:1.6, 1:1},\n","    shuffle = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z0wRibZ1YUh-"},"source":["# Load the TensorBoard notebook extension\n","#%load_ext tensorboard\n","%reload_ext tensorboard\n","%tensorboard --logdir test/classification_results/ResNet_Numeric-F_BatchSize-16_Epochs-10_Norm-T/logs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oBRY3vfnhF-D"},"source":["# EVALUATE MODEL\n","print(\"Loaded best weights of the training\")\n","model.load_weights(save_dir + \"best_model.h5\")\n","\n","results = model.evaluate(test_dataset, \n","                         steps = len(test_dataset), \n","                         verbose = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kGg9tMhEhKut"},"source":["# MAKE PREDICTIONS\n","# Retrieve a batch of images from the test set\n","predictions = model.predict(test_dataset.)\n","\n","predictions = tf.where(predictions < 0.5, 0, 1)\n","\n","print('Predictions:\\n', predictions.numpy())\n","print('Labels:\\n')\n","for label in test_labels_dataset:\n","  print(label)\n","#class_names = {0: \"No eye\", 1: \"Eye\"}\n","#plt.figure(figsize=(10, 10))\n","#for i in range(9):\n","#  ax = plt.subplot(3, 3, i + 1)\n","#  plt.imshow(image_batch[i].astype(\"uint8\"))\n","#  plt.title(class_names[predictions[i]])\n","#  plt.axis(\"off\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-V1lTSzbDUyS"},"source":["## Train on data using the Stratified K Fold.\n","\n"]},{"cell_type":"code","metadata":{"id":"NxvI67fsr2zD"},"source":["##################################\n","###          SETTINGS         ####\n","##################################\n","main_dir = \"SAR_swath_images_WS+sWSO+cWSO\"\n","NETWORK = \"ResNet\"     # options: [\"Mobile\", \"ResNet\"]\n","MIN_HEIGHT = 700\n","MIN_WIDTH = 400\n","NUM_VARS = False\n","NORMALISE = True\n","ROTATE = False\n","BATCH_SIZE = 8\n","BUFFER_SIZE = 100\n","EPOCHS = 20\n","FOLDS = 5\n","LEARNING_RATE = 0.0001\n","##################################\n","\n","# STRATIFIED CROSS VALIDATION\n","def fold_cross_validation(main_dir, NETWORK, MIN_HEIGHT, MIN_WIDTH, NUM_VARS, NORMALISE, ROTATE, LEARNING_RATE, EPOCHS, BUFFER_SIZE, BATCH_SIZE, SPLIT_NUMBER):\n","\n","  df = pd.read_csv(main_dir + \"/csv/full_dataset.csv\", converters={'bbox_shape': eval}).dropna()\n","  Y = df[\"label\"]\n","\n","  stratified_k_fold = StratifiedKFold(n_splits = SPLIT_NUMBER, random_state = 42, shuffle = False)\n","\n","  VALIDATION_ACCURACY = []\n","  VALIDATION_LOSS = []\n","  VALIDATION_TP = []\n","  VALIDATION_FP = []\n","  VALIDATION_TN = []\n","  VALIDATION_FN = []\n","  VALIDATION_PRECISION = []\n","  VALIDATION_RECALL = []\n","\n","  print(\"Entering in k fold cross validation\")\n","  print(\"Dataset dim: {}\".format(len(df)))\n","\n","  # directory to save results\n","  dir = '{}_Numeric-{}_BatchSize-{}_{}x{}_lr-{}_Epochs-{}_Folds-{}_Norm-{}'.format(\n","      NETWORK, str(NUM_VARS)[0], BATCH_SIZE, MIN_HEIGHT, MIN_WIDTH, LEARNING_RATE, EPOCHS, SPLIT_NUMBER, str(NORMALISE)[0])\n","  save_dir = main_dir + '/classification_results/identification/' + dir + '/'\n","  fold_var = 1\n","  \n","  # create an instance of the DataProcessor\n","  processor = dp.DataProcessor(model = NETWORK,\n","                               min_height = MIN_HEIGHT,\n","                               min_width = MIN_WIDTH,\n","                               normalise = NORMALISE,           # perform normalisation\n","                               rotate = ROTATE,                 # perform rotation\n","                               plot_light = False,              # plot only select_crop() images\n","                               plot_extensive = False,          # plot extensively all images\n","                               show_prints = False)\n","\n","\n","  for train_index, val_index in stratified_k_fold.split(np.zeros(len(df)), Y):\n","    training_data = df.iloc[train_index]\n","    validation_data = df.iloc[val_index]\n","\n","    train_images, train_labels, train_bbox = helper.load_data(df = training_data, with_cat = False)\n","    val_images, val_labels, val_bbox = helper.load_data(df = validation_data, with_cat = False)\n","    \n","    # generate datasets\n","    train_dataset = helper.prepare_dataset(processor, train_images, train_labels, train_bbox)\n","    val_dataset = helper.prepare_dataset(processor, val_images, val_labels, val_bbox)\n","\n","    # configure for performance\n","    train_dataset = helper.configure_for_performance(train_dataset, BUFFER_SIZE, BATCH_SIZE, shuffle = True)\n","    val_dataset = helper.configure_for_performance(val_dataset, BUFFER_SIZE, BATCH_SIZE)\n","\n","    # CREATE NEW MODEL\n","    \"\"\"\n","    # multi GPU strategy\n","    strategy = tf.distribute.MirroredStrategy()\n","    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n","    with strategy.scope():\n","        model = make_resNet(IMG_SIZE, start_from_scratch=True)\n","    \"\"\"\n","    if NETWORK == \"Mobile\":\n","        model = models.make_MobileNet(MIN_WIDTH, MIN_HEIGHT, LEARNING_RATE, detection = True)\n","    elif NETWORK == \"ResNet\":\n","        model = models.make_ResNet(MIN_WIDTH, MIN_HEIGHT, LEARNING_RATE, detection = True)\n","    else:\n","        sys.exit(\"Incert valid network model. Options: Mobile or ResNet (case sensitive)\")\n","\n","    # CREATE CALLBACKS\n","    callbacks = [\n","        EarlyStopping(patience = 10, verbose = 1),\n","        ReduceLROnPlateau(factor = 0.1, patience = 5, min_lr = 0.00001, verbose = 1),\n","        ModelCheckpoint(save_dir + helper.get_model_name(fold_var), verbose = 1, save_best_only = True),\n","        TensorBoard(log_dir = save_dir + \"logs/\" + datetime.datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"))\n","    ]\n","\n","    # FIT THE MODEL\n","    history = model.fit(\n","        train_dataset,\n","        steps_per_epoch = len(train_dataset),\n","        validation_data = val_dataset,\n","        validation_steps = len(val_dataset), \n","        epochs = EPOCHS,\n","        callbacks = callbacks,\n","        verbose = 1, \n","        class_weight = {0:1.6, 1:1},\n","        shuffle = True)\n","    \n","    # PLOT HISTORY\n","    #print(history)\n","\n","\n","    # PLOT TRAIN/VALIDATION LOSSES\n","    fig, (ax1, ax2) = plt.subplots(nrows = 2, ncols = 1, figsize=(10, 10))\n","    ax1.plot(history.history['accuracy'], label='Training Accuracy')\n","    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","    ax1.grid(True)\n","    ax1.legend(loc='lower right')\n","    ax1.set(ylabel = \"Accuracy\",\n","            title = 'Training and Validation Accuracy')\n","\n","    ax2.plot(history.history[\"loss\"], label='Training Loss')\n","    ax2.plot(history.history[\"val_loss\"], label='Validation Loss')\n","    ax2.plot(np.argmin(history.history[\"val_loss\"]), np.min(history.history[\"val_loss\"]), marker=\"x\", color=\"r\", label = \"Best model\")\n","    ax2.grid(True)\n","    ax2.legend(loc='upper right')\n","    ax2.set(xlabel = 'Epoch', \n","          ylabel = 'Cross Entropy',\n","          title = 'Training and Validation Loss')\n","    plt.show()\n","    fig.savefig(save_dir + \"model_\" + str(fold_var) + \".jpg\")\n","    \n","\n","    ##################################################################################################\n","    time.sleep(12) # guarantees enough time so that weights are saved and can be loaded after\n","\n","    # LOAD BEST MODEL\n","    print(\"Loaded best weights of the training\")\n","    model.load_weights(save_dir + helper.get_model_name(fold_var))     # Maybe a problem with colab, weights saved and loaded too fast giving concurrency problems\n","\n","    # EVALUATE PERFORMANCE of the model\n","    results = model.evaluate(val_dataset, \n","                             steps = len(val_dataset), \n","                             verbose = 1)\n","\n","    results = dict(zip(model.metrics_names, results))\n","\n","    VALIDATION_ACCURACY.append(results['accuracy'])\n","    VALIDATION_LOSS.append(results['loss'])\n","    VALIDATION_TP.append(results[\"tp\"])\n","    VALIDATION_FP.append(results[\"fp\"])\n","    VALIDATION_TN.append(results[\"tn\"])\n","    VALIDATION_FN.append(results[\"fn\"])\n","    VALIDATION_PRECISION.append(results[\"precision\"])\n","    VALIDATION_RECALL.append(results[\"recall\"])\n","\n","    # MAKE PREDICTIONS\n","    predictions = model.predict(val_dataset)\n","    predictions = tf.where(predictions < 0.5, 0, 1)\n","\n","    conf_mat = confusion_matrix(val_labels, predictions, labels = [0,1])\n","    disp = ConfusionMatrixDisplay(confusion_matrix = conf_mat, display_labels = [0,1])\n","    conf_mat_display = disp.plot()\n","    plt.savefig(save_dir + \"confusion_matrix_\" + str(fold_var) + \".jpg\")\n","\n","    tf.keras.backend.clear_session()\n","\n","    fold_var += 1\n","\n","    # save the values for each fold\n","    csv_dir = save_dir + 'csv/'\n","    os.makedirs(csv_dir, exist_ok=True)\n","    with open(csv_dir + 'test_accuracy.pkl', 'wb') as handle:\n","        pickle.dump(VALIDATION_ACCURACY, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","    with open(csv_dir + 'test_loss.pkl', 'wb') as handle:\n","        pickle.dump(VALIDATION_LOSS, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","    with open(csv_dir + 'test_tp.pkl', 'wb') as handle:\n","        pickle.dump(VALIDATION_TP, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","    with open(csv_dir + 'test_fp.pkl', 'wb') as handle:\n","        pickle.dump(VALIDATION_FP, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","    with open(csv_dir + 'test_tn.pkl', 'wb') as handle:\n","        pickle.dump(VALIDATION_TN, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","    with open(csv_dir + 'test_fn.pkl', 'wb') as handle:\n","        pickle.dump(VALIDATION_FN, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","    with open(csv_dir + 'test_precision.pkl', 'wb') as handle:\n","        pickle.dump(VALIDATION_PRECISION, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","    with open(csv_dir + 'test_recall.pkl', 'wb') as handle:\n","        pickle.dump(VALIDATION_RECALL, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","  return\n","\n","##################################################################################################\n","# test\n","fold_cross_validation(main_dir, NETWORK, MIN_HEIGHT, MIN_WIDTH, NUM_VARS, NORMALISE, ROTATE, LEARNING_RATE, EPOCHS, BUFFER_SIZE, BATCH_SIZE, FOLDS)"],"execution_count":null,"outputs":[]}]}