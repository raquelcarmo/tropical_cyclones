{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TC_Vmax_Regression.ipynb","provenance":[],"collapsed_sections":["MhLZr2Y-vwkk","8X91rnaZE2f0"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/raquelcarmo/tropical_cyclones/blob/main/src/code/TC_Vmax_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","metadata":{"id":"Jnxs1L1PWjeL"},"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4p-mSrzKWqUb"},"source":["# insert your desired path to work on\n","%cd /content/drive/My Drive/ESRIN_PhiLab/Tropical_Cyclones/data\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZHrKRH_b4KhY"},"source":["!pip install netcdf4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PZu5fkuXWxdC"},"source":["# general imports\n","import netCDF4\n","import random\n","import glob\n","import os\n","import sys\n","sys.stdout.flush()\n","import pandas as pd\n","import numpy as np\n","import cv2\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","import math\n","import imageio\n","import os.path\n","import time\n","from PIL import Image\n","from mpl_toolkits.axes_grid1 import make_axes_locatable\n","from scipy import ndimage\n","from scipy.interpolate import interp1d\n","from google.colab.patches import cv2_imshow\n","import random\n","from shapely.geometry import Point\n","import re\n","import pickle\n","import scipy\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n","import datetime\n","\n","import tensorflow as tf\n","from tensorflow.data import Dataset\n","from tensorflow.keras import Input\n","from keras.utils import to_categorical \n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.applications import resnet50, mobilenet_v2\n","from tensorflow.keras.applications.resnet50 import ResNet50\n","from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import concatenate, Dense, GlobalAveragePooling2D, Activation\n","from tensorflow.keras.optimizers import SGD, Adam\n","from tensorflow.keras.metrics import BinaryAccuracy, Precision, Recall, TruePositives, FalsePositives, TrueNegatives, FalseNegatives\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","np.set_printoptions(precision=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kADmGNs14dNr"},"source":["def knots_to_m_sec(kts):\n","    \"\"\" Converts knots (kt) to meters/second (m/s) \"\"\"\n","    if np.isnan(kts):\n","        return kts\n","    else:\n","        return kts * 0.514444"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MhLZr2Y-vwkk"},"source":["##1. Test dilation of land mask on category 1 data"]},{"cell_type":"code","metadata":{"id":"8KT8eOkQxPFd"},"source":["file_list_cat4 = glob.glob('SAR_swath_nc/category4/*.nc')\n","file_list_cat5 = [os.path.basename(x) for x in glob.glob('SAR_swath_nc/category5/*.nc')]\n","list_cat4 = [file for file in file_list_cat4 if os.path.basename(file) not in file_list_cat5]\n","\n","#file_list_cat1 = glob.glob('SAR_swath_nc/category1/*.nc')\n","#file_list_cat2 = [os.path.basename(x) for x in glob.glob('SAR_swath_nc/category2/*.nc')]\n","#list_cat1 = [file for file in file_list_cat1 if os.path.basename(file) not in file_list_cat2]\n","\n","print(len(list_cat4))\n","values = []\n","\n","for single_file in list_cat4:\n","    full_info_image = netCDF4.Dataset(single_file, mode='r') \n","\n","    feature_wind = full_info_image.variables[\"wind_speed\"][:]\n","    feature_wind = feature_wind[0]\n","\n","    plt.imshow(feature_wind)\n","    plt.title(\"Original wind feature\")\n","    plt.colorbar()\n","    plt.show()\n","    print(\"First Vmax value:\", np.max(feature_wind))\n","    ind = np.unravel_index(np.argmax(feature_wind, axis=None), feature_wind.shape)\n","    print(\"Pixel for the Vmax found:\", ind)\n","\n","    mask = full_info_image.variables[\"mask_flag\"][:]\n","    mask = mask[0]\n","    mask[mask != 0] = 1\n","    #np.int8(mask != 0)\n","\n","    # dilate mask\n","    # source: https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.html\n","    kernel = np.ones((11, 11), np.int8)\n","    dilation = cv2.dilate(mask,kernel,iterations = 1)\n","    feature_wind[dilation != 0] = 0\n","\n","    #plt.imshow(dilation)\n","    #plt.colorbar()\n","    #plt.show()\n","\n","    plt.imshow(feature_wind)\n","    plt.title(\"Wind feature with land mask\")\n","    plt.colorbar()\n","    plt.show()\n","    print(\"Intermediate Vmax value:\", np.max(feature_wind))\n","\n","    heterogeneity_mask = full_info_image.variables[\"heterogeneity_mask\"][:]\n","    heterogeneity_mask = heterogeneity_mask[0]\n","\n","    plt.imshow(heterogeneity_mask)\n","    plt.title(\"heterogeneity mask\")\n","    plt.colorbar()\n","    plt.show()\n","    print(\"heterogeneity mask unique values:\", np.unique(heterogeneity_mask))\n","\n","    feature_wind[heterogeneity_mask == 3] = 0\n","    feature_wind[heterogeneity_mask == 2] = 0\n","    plt.imshow(feature_wind)\n","    plt.title(\"Wind feature with both masks\")\n","    plt.colorbar()\n","    plt.show()\n","\n","    ind = np.unravel_index(np.argmax(feature_wind, axis=None), feature_wind.shape)\n","    ws_Vmax = np.max(feature_wind)\n","    print(os.path.basename(single_file))\n","    print(\"Final Vmax value:\", ws_Vmax)\n","    print(\"Pixel for the Vmax found:\", ind)\n","\n","    values.append(ws_Vmax)\n","    #plt.imshow(feature_wind)\n","    #plt.title(\"Final image\")\n","    #plt.colorbar()\n","    #plt.show()\n","    print(\"---------------------------------------\")\n","print(\"Values:\", values)\n","plt.hist(values)\n","plt.title(\"Category 4 Vmax distribution\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gk0SR8lr1yZs"},"source":["plt.hist(values)\n","plt.title(\"Category 4 Vmax distribution\")\n","plt.savefig(\"SAR_swath_Vmax/Category 4 Vmax distribution.png\", bbox_inches='tight')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lQMbpy9_v2dB"},"source":["##2. Create dataframe with all Vmax values for all categories"]},{"cell_type":"code","metadata":{"id":"BZSD06Ni1ggp"},"source":["# dir to save results\n","save_dir = \"SAR_swath_Vmax/with feature_wind[heterogeneity_mask == 2,3] = 0 and feature_wind[dilated_mask != 0] = 0\"\n","os.makedirs(save_dir, exist_ok = True)\n","use_cat0 = False\n","\n","# load generic best tracks CSV file\n","TC_dataset = pd.read_csv(\"best_track/ibtracs.since1980.list.v04r00.csv\", header=0)\n","\n","# load dataframe connecting .nc filename to TC Name and USA_ATCF_ID\n","nc_ID = pd.read_csv(\"SAR_swath_nc/cyclobs_tc_dataframe.csv\", header=0)\n","\n","# gather all .nc files from all categories\n","file_list = glob.glob('SAR_swath_nc/*/*.nc')\n","\n","files = []\n","ids = []\n","cyclob_Vmax_list = []\n","ws_Vmax_list = []\n","best_track_Vmax_list = []\n","tmax_list = []\n","ws_Vmax_masked_list = []\n","\n","cnt=0\n","for single_file in file_list:\n","\n","    nc_filename = os.path.basename(single_file)\n","    df = nc_ID[nc_ID.data == nc_filename]\n","\n","    # cyclob_Vmax is the Vmax value in the request url dataframe from Cyclobs API\n","    cyclob_Vmax = df['vmax (m/s)'].values\n","    id = df.sid.values\n","\n","    ##############################\n","    try:\n","        # READ THE NC PRODUCT\n","        full_info_image = netCDF4.Dataset(single_file, mode='r') \n","\n","        # try to read this feature as it usually gives problems!\n","        feature_cross = full_info_image.variables[\"nrcs_detrend_cross\"][:]\n","\n","        tmax_units = datetime.datetime.strptime(full_info_image.measurementDate, '%Y-%m-%dT%H:%M:%SZ')\n","        tmax = tmax_units.strftime('%Y-%m-%d %H_%M_%S')\n","\n","        # extract wind speed info from .nc product\n","        feature_wind = full_info_image.variables[\"wind_speed\"][:]\n","        feature_wind = feature_wind[0]\n","\n","        # ws_Vmax is the maximum wind speed of the product\n","        ws_Vmax = np.max(feature_wind)\n","        #print(ws_Vmax)\n","\n","        # extract mask from .nc product\n","        mask = full_info_image.variables[\"mask_flag\"][:]\n","        mask = mask[0]\n","        mask[mask != 0] = 1\n","        # dilate mask\n","        kernel = np.ones((11, 11), np.int8)\n","        mask_dilated = cv2.dilate(mask, kernel, iterations = 1)\n","\n","        # extract heterogeneity mask from .nc product\n","        heterogeneity_mask = full_info_image.variables[\"heterogeneity_mask\"][:]\n","        heterogeneity_mask = heterogeneity_mask[0]\n","        #print(np.unique(heterogeneity_mask))\n","\n","        # set all mask pixels diff than 0 to 0\n","        feature_wind[mask_dilated != 0] = 0\n","        #feature_wind[heterogeneity_mask != 0] = 0\n","        feature_wind[heterogeneity_mask == 3] = 0\n","        feature_wind[heterogeneity_mask == 2] = 0\n","\n","        # ws_Vmax_masked is the Vmax of the product after the application of the masks\n","        ws_Vmax_masked = np.max(feature_wind)\n","        if ws_Vmax != ws_Vmax_masked:\n","            cnt +=1\n","        full_info_image.close()\n","    \n","    except KeyError as err:\n","        # Creating KeyError instance for book keeping.\n","        print(\"Error:\", err)\n","        full_info_image.close()\n","        continue\n","    ##############################\n","\n","    # get rows in the TC best track that are taken in the same day of the TC\n","    day = tmax[:10]\n","    img_info = TC_dataset.loc[TC_dataset.ISO_TIME.str.contains(day)]\n","    img_info = img_info.loc[img_info.USA_ATCF_ID == id[0].upper()]\n","\n","    for index, row in img_info.iterrows():\n","        img_info['ISO_TIME'][index] = datetime.datetime.strptime(img_info['ISO_TIME'][index], '%Y-%m-%d %H:%M:%S')\n","    img_info['USA_WIND'] = img_info['USA_WIND'].astype(float)\n","    aux = img_info[['ISO_TIME', 'USA_WIND']]\n","\n","    df2 = pd.DataFrame([[tmax_units, np.nan]], columns=['ISO_TIME', 'USA_WIND'])\n","    #print(df2)\n","    aux = aux.append(df2, ignore_index=True)\n","    aux = aux.sort_values(by='ISO_TIME')\n","    #print(aux)\n","\n","    interp = aux.interpolate()\n","    #print(interp)\n","    best_track_Vmax = interp.USA_WIND.loc[interp.ISO_TIME == tmax_units].values\n","    #print(best_track_Vmax)\n","\n","    files.append(single_file)\n","    ids.append(id[0])\n","    cyclob_Vmax_list.append(cyclob_Vmax[0])\n","    best_track_Vmax_list.append(knots_to_m_sec(best_track_Vmax[0]))\n","    ws_Vmax_list.append(ws_Vmax)\n","    ws_Vmax_masked_list.append(ws_Vmax_masked)\n","    tmax_list.append(tmax)\n","\n","data = {\"nc\": files, \"sid\": ids, \"tmax\": tmax_list, \"Vmax_ws\": ws_Vmax_list, \n","        \"Vmax_ws_masked\": ws_Vmax_masked_list, \"Vmax_cyclob\": cyclob_Vmax_list, \n","        \"Vmax_best_track\": best_track_Vmax_list}\n","data_df = pd.DataFrame(data)\n","\n","print(\"Number of Vmax values that changed by applying the land mask and the heterogeneity mask:\", cnt)\n","\n","# save relevant information in a csv file that could be retrived with pandas in the future\n","data_df.to_csv(\"{}/Vmax_info_masked_both.csv\".format(save_dir), index = False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TqaLj5xGwAAr"},"source":["### 2.1. Drop duplicated rows and timestamps not present in our dataset."]},{"cell_type":"code","metadata":{"id":"P4ENwJLpkPOx"},"source":["#####################################################################################\n","# NOTE: \n","# What reduces the dataset to a size of 227 is dropping duplicates and excluding .nc\n","# products that give an error while trying to read the nrcs_detrend_cross feature\n","#####################################################################################\n","\n","dir = 'SAR_swath_images_VV+VH+WS'\n","\n","dataset = pd.read_csv(\"{}/Vmax_info_masked_both.csv\".format(save_dir))\n","\n","# check indexes with duplicated tmax and keep the last one\n","dataset = dataset.drop_duplicates(subset=[\"tmax\"], keep = 'last')\n","\n","# eliminate rows with timestamps not present in our dataset\n","file_list = glob.glob('{}/*/*.png'.format(dir))\n","\n","times = np.array([item.split(\"/\")[-1][:-4] for item in file_list])\n","df_tmax = dataset.tmax.values\n","\n","# this diff happened because I wasn't using try except nrcs_detrend_cross exception\n","diff = list(list(set(times)-set(df_tmax)) + list(set(df_tmax)-set(times)))\n","if len(diff) > 0:\n","    print(diff)\n","    idxs = []\n","    for item in diff:\n","        df = dataset.loc[dataset.tmax == item]\n","\n","        for idx, row in df.iterrows():\n","            idxs.append(idx)\n","    dataset = dataset.drop(idxs)\n","\n","# create category column\n","dataset[\"category\"] = dataset[\"nc\"].str.split('/', expand=True)[1]\n","\n","if use_cat0:\n","    # load full_dataset.csv to get info on labels\n","    df = pd.read_csv('{}/csv/full_dataset.csv'.format(dir))\n","    df['tmax'] = df['image'].str.split(\"/\").str[-1].str[:-4]\n","    #print(df)\n","    dataset = dataset.merge(df[['tmax', 'label']])\n","    \n","    # drop products where there is no eye\n","    #dataset = dataset[dataset.label != 0]\n","    # instead of dropping no-eye images, we can create \"category0\"\n","    dataset[\"category\"] = np.where((dataset.label == 0), 'category0', dataset[\"category\"])\n","\n","dataset.reset_index(inplace = True, drop = True)\n","dataset.to_csv(\"{}/Vmax_info_masked_both_filtered.csv\".format(save_dir), index = False)\n","\n","print(\"cat0:\", len(dataset[dataset.category == \"category0\"]))\n","print(\"cat1:\", len(dataset[dataset.category == \"category1\"]))\n","print(\"cat2:\", len(dataset[dataset.category == \"category2\"]))\n","print(\"cat3:\", len(dataset[dataset.category == \"category3\"]))\n","print(\"cat4:\", len(dataset[dataset.category == \"category4\"]))\n","print(\"cat5:\", len(dataset[dataset.category == \"category5\"]))\n","dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8X91rnaZE2f0"},"source":["### 2.2. Plot distribution of Vmax across categories for different vars."]},{"cell_type":"code","metadata":{"id":"Lxtdm_02BTNO"},"source":["#main_dir = \"SAR_swath_Vmax/with feature_wind[heterogeneity_mask[0] != 0] = 0 and feature_wind[mask[0] != 0] = 0\"\n","hist_dir = \"{}/histograms\".format(save_dir)\n","os.makedirs(hist_dir, exist_ok = True)\n","save_plots = True\n","n_bins = 50\n","\n","dataset = pd.read_csv(\"{}/Vmax_info_masked_both_filtered.csv\".format(save_dir))\n","\n","#######################\n","###   Vmax_cyclob   ###\n","#######################\n","fig, ax = plt.subplots(figsize =(11,8))\n","rate = 0.2\n","for cat in range(1, 6):\n","    category = 'category'+str(cat)\n","    feature = dataset.where(dataset.category == category).loc[:, \"Vmax_cyclob\"]\n","    feature.hist(ax=ax, label=category)\n","ax.set(title = 'Vmax cyclob', ylabel = 'Count', xlabel = 'Vmax (m/s)')\n","ax.legend(fontsize = 15)\n","ax.grid(axis='y')\n","ax.set_facecolor('#d8dcd6')\n","plt.show()\n","if save_plots:\n","    fig.savefig(\"{}/Vmax_cyclob.png\".format(hist_dir), bbox_inches='tight')\n","\n","\n","##########################\n","###   Vmax_ws_masked   ###\n","##########################\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize =(19,8))\n","\n","for cat in range(1, 6):\n","    category = 'category'+str(cat)\n","    feature = dataset.where(dataset.category == category).loc[:, \"Vmax_ws_masked\"]\n","    feature.hist(ax=ax1, label=category, alpha = rate*cat, bins = n_bins)\n","ax1.set(title = 'Vmax masked unstacked', ylabel = 'Count', xlabel = 'Vmax (m/s)')\n","ax1.legend(fontsize = 15)\n","ax1.grid(axis='y')\n","\n","cats = ['category1', 'category2', 'category3', 'category4', 'category5']\n","cat1 = dataset.where(dataset.category == 'category1').loc[:, \"Vmax_ws_masked\"]\n","cat2 = dataset.where(dataset.category == 'category2').loc[:, \"Vmax_ws_masked\"]\n","cat3 = dataset.where(dataset.category == 'category3').loc[:, \"Vmax_ws_masked\"]\n","cat4 = dataset.where(dataset.category == 'category4').loc[:, \"Vmax_ws_masked\"]\n","cat5 = dataset.where(dataset.category == 'category5').loc[:, \"Vmax_ws_masked\"]\n","\n","ax2.hist([cat1, cat2, cat3, cat4, cat5], stacked=True, label = cats, bins = n_bins)\n","ax2.set(title = 'Vmax masked stacked', ylabel = 'Count', xlabel = 'Vmax (m/s)')\n","ax2.legend(fontsize = 15)\n","ax2.grid(axis='x')\n","ax2.set_facecolor('#d8dcd6')\n","plt.show()\n","\n","if save_plots:\n","    fig.savefig(\"{}/Vmax_ws_masked.png\".format(hist_dir), bbox_inches='tight')\n","\n","\n","###################\n","###   Vmax_ws   ###\n","###################\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize =(19,8))\n","\n","for cat in range(1, 6):\n","    category = 'category'+str(cat)\n","    feature = dataset.where(dataset.category == category).loc[:, \"Vmax_ws\"]\n","    feature.hist(ax=ax1, label=category, alpha = rate*cat, bins = n_bins)\n","ax1.set(title = 'Vmax unmasked unstacked', ylabel = 'Count', xlabel = 'Vmax (m/s)')\n","ax1.legend(fontsize = 15)\n","ax1.grid(axis='y')\n","#ax.set_facecolor('#d8dcd6')\n","\n","cats = ['category1', 'category2', 'category3', 'category4', 'category5']\n","cat1 = dataset.where(dataset.category == 'category1').loc[:, \"Vmax_ws\"]\n","cat2 = dataset.where(dataset.category == 'category2').loc[:, \"Vmax_ws\"]\n","cat3 = dataset.where(dataset.category == 'category3').loc[:, \"Vmax_ws\"]\n","cat4 = dataset.where(dataset.category == 'category4').loc[:, \"Vmax_ws\"]\n","cat5 = dataset.where(dataset.category == 'category5').loc[:, \"Vmax_ws\"]\n","\n","ax2.hist([cat1, cat2, cat3, cat4, cat5], stacked=True, label = cats, bins = n_bins)\n","ax2.set(title = 'Vmax unmasked stacked', ylabel = 'Count', xlabel = 'Vmax (m/s)')\n","ax2.legend(fontsize = 15)\n","ax2.grid(axis='x')\n","ax2.set_facecolor('#d8dcd6')\n","plt.show()\n","\n","if save_plots:\n","    fig.savefig(\"{}/Vmax_ws.png\".format(hist_dir), bbox_inches='tight')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kF5maLYnFBL4"},"source":["## 3. Train regression model"]},{"cell_type":"code","metadata":{"id":"XWAnPRemp0Da"},"source":["# Importing the dataset\n","#main_dir = \"SAR_swath_Vmax/with feature_wind[heterogeneity_mask[0] == 3] = 0\"\n","save_dir = \"SAR_swath_Vmax/with feature_wind[heterogeneity_mask == 2,3] = 0 and feature_wind[dilated_mask != 0] = 0 and cat0\"\n","results_dir = \"{}/regression_results\".format(save_dir)\n","os.makedirs(results_dir, exist_ok= True)\n","save_plots = True\n","\n","dataset = pd.read_csv(\"{}/Vmax_info_masked_both_filtered.csv\".format(save_dir))\n","\n","feature = \"Vmax_ws_masked\"\n","X = dataset.loc[:, feature].values\n","y = dataset.loc[:, \"category\"].str.extract('(\\d+)').astype(int).values.flatten()\n","#X = X/X.max()\n","#print(X)\n","\n","\n","# Splitting the dataset into the Training set and Test set\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n","#print(X_test, y_test)\n","print(y_test)\n","\n","# one hot encode outputs\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)\n","\n","# count number of classes\n","nb_classes = y_test.shape[1]\n","print(\"Number of classes:\", nb_classes)\n","\n","X_train = X_train.reshape(-1, 1)\n","X_test = X_test.reshape(-1, 1)\n","\n","\n","# FEATURE SCALING\n","sc = StandardScaler()\n","X_train = sc.fit_transform(X_train)\n","X_test = sc.transform(X_test)\n","#print(X_train, X_test)\n","\n","\n","# CREATE MODEL\n","model = Sequential()\n","model.add(Dense(32, activation = 'relu', input_dim = 1))    # adding the input layer and the first hidden layer\n","model.add(Dense(units = 32, activation = 'relu'))   # second hidden layer\n","model.add(Dense(units = 32, activation = 'relu'))   # third hidden layer \n","model.add(Dense(nb_classes, activation='softmax'))   # output layer\n","#model.add(Dense(units = 1))\n","\n","# COMPILE MODEL\n","model.compile(optimizer = 'adam', \n","              loss = 'categorical_crossentropy', \n","              metrics = ['accuracy'])\n","#model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n","\n","\n","# FIT THE MODEL\n","history = model.fit(X_train, y_train,\n","                    validation_split = 0.2,\n","                    batch_size = 10,\n","                    epochs = 50,\n","                    callbacks = ModelCheckpoint(results_dir + \"/{}_best_model.h5\".format(feature), verbose = 1, save_best_only = True)\n","                    )\n","\n","\n","# LOAD BEST MODEL\n","time.sleep(5) # guarantees enough time for weights to be saved and loaded afterwards, otherwise gives concurrency problems\n","print(\"Loaded best weights of the training\")\n","model.load_weights(results_dir + \"/{}_best_model.h5\".format(feature))\n","\n","\n","# MAKE PREDICTIONS\n","y_pred = model.predict(X_test)\n","#print(y_pred)\n","predictions = [ np.argmax(t) for t in y_pred ]\n","#print(predictions)\n","y_test_non_category = [ np.argmax(t) for t in y_test ]\n","\n","conf_mat = confusion_matrix(y_test_non_category, predictions, labels = [0,1,2,3,4,5])\n","disp = ConfusionMatrixDisplay(confusion_matrix = conf_mat, display_labels = [0,1,2,3,4,5])\n","conf_mat_display = disp.plot()\n","if save_plots:\n","    plt.savefig(\"{}/{}_confusion_matrix.jpg\".format(results_dir, feature), bbox_inches='tight')\n","\n","\n","# PLOT TRAIN/VALIDATION LOSSES\n","fig, (ax1, ax2) = plt.subplots(nrows = 2, ncols = 1, figsize=(10, 10))\n","ax1.plot(history.history['accuracy'], label='Training Accuracy')\n","ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","ax1.grid(True)\n","ax1.legend(loc='lower right')\n","ax1.set(ylabel = \"Accuracy\",\n","        title = 'Training and Validation Accuracy')\n","\n","ax2.plot(history.history['loss'], label='Training Loss')\n","ax2.plot(history.history['val_loss'], label='Validation Loss')\n","ax2.grid(True)\n","ax2.legend(loc='upper right')\n","ax2.set(xlabel = 'Epoch', \n","      ylabel = 'Categorical Cross Entropy',\n","      title = 'Training and Validation Loss')\n","plt.show()\n","if save_plots:\n","    fig.savefig(\"{}/{}_learning_curves.jpg\".format(results_dir, feature), bbox_inches='tight')\n","\n","# PLOT PREDICTIONS vs LABELS\n","#fig, ax = plt.subplots(figsize=(10, 10))\n","#ax.plot(y_test_non_category, '.', color = 'red', label = 'Real data')\n","#ax.plot(predictions, '.', color = 'blue', label = 'Predicted data')\n","#ax.set(title = 'Prediction')\n","#ax.legend()\n","#plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c2s1hi1pjPBf"},"source":["###3.1. Regression model structures tests."]},{"cell_type":"code","metadata":{"id":"Bm5QP3kGKPXa"},"source":["model = tf.keras.Sequential([\n","  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(1,)),  # input shape required\n","  tf.keras.layers.Dense(10, activation=tf.nn.relu),\n","  tf.keras.layers.Dense(3)\n","])\n","\n","\n","model = Sequential()\n","model.add(Dense(10, activation='relu', input_dim=1))\n","model.add(Dense(10, activation='relu'))\n","model.add(Dense(5, activation='softmax'))\n","\n","# Compile the model\n","model.compile(optimizer='adam', \n","              loss='categorical_crossentropy', \n","              metrics=['accuracy'])\n"],"execution_count":null,"outputs":[]}]}