{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TC_model_comparison.ipynb","provenance":[],"collapsed_sections":["SJHkL9rpnuUJ","K9695aBjozMD","4EkR1bLbnfoT","UtrFW51bpY4h","U90HRPq1bC9p"]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/raquelcarmo/tropical_cyclones/blob/main/src/code/TC_model_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"WJn-0pS7STzM"},"source":["##Imports and configurations"]},{"cell_type":"code","metadata":{"id":"dj7JZ5lPCZvm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625214817201,"user_tz":-120,"elapsed":19409,"user":{"displayName":"Raquel Carmo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUUTZlzR-089p-AQlc4K-YNrw3DWlq1fWmuzLsdfI=s64","userId":"09214858898201854328"}},"outputId":"11b97920-31b7-4899-f6e8-8cd312bc8af5"},"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3d95OmlLCDSj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625214820192,"user_tz":-120,"elapsed":1006,"user":{"displayName":"Raquel Carmo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUUTZlzR-089p-AQlc4K-YNrw3DWlq1fWmuzLsdfI=s64","userId":"09214858898201854328"}},"outputId":"6a59f6d6-fc65-47a1-d0e3-9f399c2f3e4b"},"source":["# getting in the directory \n","%cd /content/drive/My Drive/ESRIN_PhiLab/Tropical_Cyclones/data\n","%ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/ESRIN_PhiLab/Tropical_Cyclones/data\n"," \u001b[0m\u001b[01;34mbest_track\u001b[0m/                           \u001b[01;34mSAR_swath_images_VH+VH+VH\u001b[0m/\n"," categorisation.png                    \u001b[01;34mSAR_swath_images_VV+VH+VH\u001b[0m/\n"," download_files.ipynb                  \u001b[01;34mSAR_swath_images_VV+VH+WS\u001b[0m/\n"," identification.png                    \u001b[01;34mSAR_swath_images_VV+VH+WS_dilated\u001b[0m/\n"," \u001b[01;34mlabels\u001b[0m/                               \u001b[01;34mSAR_swath_images_VV+VH+WS_land\u001b[0m/\n"," \u001b[01;34mLink_ANCHER_IFREMER\u001b[0m/                 \u001b[01;34m'SAR_swath_images_VV+VH+WS_land=1'\u001b[0m/\n"," Mobile_visualization.png              \u001b[01;34mSAR_swath_images_VV+VV+VV\u001b[0m/\n"," \u001b[01;34mmodel_comparisons_between_datasets\u001b[0m/   \u001b[01;34mSAR_swath_images_WS+sWSO+cWSO\u001b[0m/\n","'model_input(x,x,12).png'              \u001b[01;34mSAR_swath_images_WS+WS+WS\u001b[0m/\n"," model.png                             \u001b[01;34mSAR_swath_masks\u001b[0m/\n"," \u001b[01;34mparametric_model\u001b[0m/                     \u001b[01;34mSAR_swath_nc\u001b[0m/\n"," ResNet_visualization.png              \u001b[01;34mSAR_swath_Vmax\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a8tbWAaHayf0","executionInfo":{"status":"ok","timestamp":1625214826408,"user_tz":-120,"elapsed":3719,"user":{"displayName":"Raquel Carmo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUUTZlzR-089p-AQlc4K-YNrw3DWlq1fWmuzLsdfI=s64","userId":"09214858898201854328"}}},"source":["# general imports\n","import random\n","import glob\n","import os\n","import sys\n","sys.stdout.flush()\n","import pandas as pd\n","import numpy as np\n","import cv2\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","import math\n","import imageio\n","import os.path\n","from PIL import Image\n","from mpl_toolkits.axes_grid1 import make_axes_locatable\n","from scipy import ndimage\n","from google.colab.patches import cv2_imshow\n","import random\n","from shapely.geometry import Point\n","import re\n","import pickle\n","import scipy\n","from sklearn.model_selection import StratifiedKFold\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n","import datetime\n","from IPython.display import Image\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.applications import MobileNetV2, ResNet50\n","from tensorflow.keras.layers import concatenate, Dense, GlobalAveragePooling2D\n","import itertools\n","import seaborn as sns"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Uk0cd0xa-Dw"},"source":["## 1. Comparison between X models in one dataset"]},{"cell_type":"code","metadata":{"id":"bksBnI0LgwnY"},"source":["# COMPARISON BETWEEN X MODELS\n","main_dir = \"SAR_swath_images_VV+VH+WS_dilated/\"\n","sub_dir = main_dir + \"classification_results/categorization/test_eye_only/\"\n","\n","# note that the result (in this case the recall) could be null if the classifier is broken \n","# (all recall = 1 because is always classified as positive class)\n","MODEL_1_DIRECTORY = sub_dir + \"ResNet_nu-F_bs-8_416x416_lr-1e-05_ep-30_sp-5_no-zT_cr-u1_ag-T_drp-F/csv\"\n","MODEL_2_DIRECTORY = sub_dir + \"test_new_weights/ResNet_nu-F_bs-8_416x416_lr-1e-05_ep-30_sp-5_no-zT_cr-u1_ag-T_drp-F/csv\"\n","#MODEL_3_DIRECTORY = sub_dir + \"test_new_weights/ResNet_nu-F_bs-8_416x416_lr-1e-05_ep-30_sp-5_no-zT_cr-u1_ag-T_drp-F/csv\"\n","#MODEL_4_DIRECTORY = sub_dir + \"test_rmse/ResNet_nu-F_bs-8_416x416_lr-1e-05_ep-30_sp-5_no-mT_cr-u1_ag-T_drp-F/csv\"\n","#MODEL_5_DIRECTORY = sub_dir + \"ResNet_nu-F_bs-8_416x416_lr-0.0001_ep-35_sp-5_no-zT_cr-u1_ag-T_drp-F_ft-last50/csv\"\n","#MODEL_6_DIRECTORY = sub_dir + \"ResNet_nu-F_bs-8_416x416_lr-0.0001_ep-35_sp-5_no-zT_cr-u1_ag-T_drp-F_ft-last75/csv\"\n","#MODEL_7_DIRECTORY = sub_dir + \"ResNet_nu-F_bs-8_416x416_lr-0.0001_ep-35_sp-5_no-zT_cr-u1_ag-T_drp-F_ft-last125/csv\"\n","#MODEL_8_DIRECTORY = sub_dir + \"ResNet_nu-F_bs-8_416x416_lr-0.0001_ep-35_sp-5_no-zT_cr-u1_ag-T_drp-F_ft-last175/csv\"\n","#MODEL_9_DIRECTORY = sub_dir + \"ResNet_nu-F_bs-8_416x416_lr-1e-05_ep-30_sp-5_no-zT_cr-u1_ag-T_drp-F/csv\"\n","\n","#dir_dict = {1: MODEL_1_DIRECTORY, 2: MODEL_2_DIRECTORY, 3: MODEL_3_DIRECTORY, 4: MODEL_4_DIRECTORY, 5: MODEL_5_DIRECTORY,\n","#            6: MODEL_6_DIRECTORY, 7: MODEL_7_DIRECTORY, 8: MODEL_8_DIRECTORY, 9: MODEL_9_DIRECTORY}\n","#dir_dict = {1: MODEL_1_DIRECTORY, 2: MODEL_2_DIRECTORY, 3: MODEL_3_DIRECTORY, 4: MODEL_4_DIRECTORY, 5: MODEL_5_DIRECTORY}\n","#dir_dict = {1: MODEL_1_DIRECTORY, 2: MODEL_2_DIRECTORY, 3: MODEL_3_DIRECTORY, 4: MODEL_4_DIRECTORY}\n","#dir_dict = {1: MODEL_1_DIRECTORY, 2: MODEL_2_DIRECTORY, 3: MODEL_3_DIRECTORY}\n","dir_dict = {1: MODEL_1_DIRECTORY, 2: MODEL_2_DIRECTORY}\n","\n","# Settings\n","models = len(dir_dict)\n","save_plot = True\n","#title = \"Comparison between dropout rates - ResNet_nu-F_bs-8_384x384_lr-1e-05_ep-30_sp-5_no-mT_cr-u1_ag-T_drp-XXX\"\n","#title = \"Comparison between input sizes - ResNet_nu-F_bs-8_XXX_lr-0.0001_ep-20_sp-5_no-zT_cr-u1_ag-T\"\n","#title = \"Comparison between normalizations\" - ResNet_nu-F_bs-8_384x384_lr-1e-05_ep-30_sp-5_no-XXX_cr-u1_ag-T_drp-F\"\n","#title = \"Comparison between nb crops\"# - ResNet_nu-F_bs-8_XXX_lr-1e-05_ep-30_sp-5_no-mT_cr-uXXX_ag-T_drp-F\"\n","#title = \"Comparison between nb epochs - ResNet_nu-F_bs-8_288x288_lr-0.0001_ep-XXX_sp-5_no-mT_cr-u1_ag-T_ft-165\"\n","#title = \"Comparison between learning rates - ResNet_nu-F_bs-8_384x384_lr-XXX_ep-XXX_sp-5_no-zT_cr-u1_ag-T\"\n","#title = \"Comparison between nb frozen layers - ResNet_nu-F_bs-8_416x416_lr-XXX_ep-XXX_sp-5_no-zT_cr-u1_ag-T_drp-F_ft-XXX\"\n","#title = \"Comparison between CNNs - XXX_nu-F_bs-8_288x288_lr-0.0001_ep-20_sp-5_no-mT_cr-u1_ag-T\"\n","#title = \"Comparison between nb of classes - ResNet_nu-F_bs-8_416x416_lr-XXX_ep-XXX_sp-5_no-XXXT_cr-u1_ag-T\"\n","#title = \"Comparison between rotations - Model 1 has two more augmentation techniques (rot180 and rot270)\"\n","#title = 'Comparison between zeroed land (Model 1) and land (Model 2) - ResNet_nu-F_bs-8_416x416_lr-1e-05_ep-30_sp-5_no-mT_cr-u1_ag-T_drp-F'\n","#title = 'CrossEntropy Loss (Models 1 and 2) and Combined CrossEntropy and RMSE Loss (Models 3 and 4)'\n","title = 'Normal Class weights (Model 1) and Class weights with prior precision (Model 2) - ResNet_nu-F_bs-8_416x416_lr-1e-05_ep-30_sp-5_no-zT_cr-u1_ag-T_drp-F'\n","file_name = title.replace(\" \", \"_\")\n","\n","# Critical t-value to see the difference in two distribution with 5 samples\n","criticalTvalue = 1.533      # 80% confidence\n","#criticalTvalue = 2.132     # 90% confidence\n","#criticalTvalue = 2.776     # 95% confidence"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SJHkL9rpnuUJ"},"source":["###1.1. Load functions"]},{"cell_type":"code","metadata":{"id":"jFN-keiZlUGx","executionInfo":{"status":"ok","timestamp":1625214834504,"user_tz":-120,"elapsed":590,"user":{"displayName":"Raquel Carmo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUUTZlzR-089p-AQlc4K-YNrw3DWlq1fWmuzLsdfI=s64","userId":"09214858898201854328"}}},"source":["def mean_confidence_interval(data, confidence=0.80):\n","    a = 1.0 * np.array(data)\n","    n = len(a)\n","    m, se = np.mean(a), scipy.stats.sem(a)\n","    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n","    return m, m-h, m+h\n","\n","\n","def compute_f1(dir):\n","    precision = np.asarray(pickle.load(open(dir + \"/test_precision.pkl\",  \"rb\")))\n","    recall = np.asarray(pickle.load(open(dir + \"/test_recall.pkl\",  \"rb\")))\n","    f1 = 2 * (precision * recall) / (precision + recall)\n","    f1 = np.nan_to_num(f1)\n","    with open(dir + '/test_f1-score.pkl', 'wb') as handle:\n","        pickle.dump(f1.tolist(), handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","\n","def compute_stats(nb_models, dir_dict, metrics):\n","    inter_dict = {}\n","    mean_dict = {}\n","    for metric in metrics:\n","        metric_dict = {}\n","        for i in range(1, nb_models+1):\n","            load_metric = pickle.load(open(dir_dict[i] + \"/test_{}.pkl\".format(metric),  \"rb\"))\n","            metric_dict['model%d_%s' %(i, metric)] = load_metric\n","\n","            mean, b, u = mean_confidence_interval(load_metric)\n","            #print(mean, b, u)\n","            #conf_dict['model%d_%s' %(i, metric)] = [mean, b, u]\n","\n","            if 'model%d' %(i) in inter_dict:\n","                inter_dict['model%d' %(i)].append(u - mean)\n","                mean_dict['model%d' %(i)].append(mean)\n","            else:\n","                inter_dict['model%d' %(i)] = [u - mean]\n","                mean_dict['model%d' %(i)] = [mean]\n","\n","        #print(inter_dict)\n","        #print(mean_dict)\n","        print(\"---------------- {} analysis ----------------\".format(metric.upper()))\n","        for x, y in itertools.combinations(metric_dict.keys(), 2):\n","            t, p = scipy.stats.ttest_ind(metric_dict[x], metric_dict[y])\n","            print(\"t value {} between {} and {}: {}\".format(metric, x, y, t))\n","            if t > criticalTvalue or t < -criticalTvalue :\n","                print(\"Null hypothesis rejected -> there is a significant difference in the two {} distributions\".format(metric))\n","            else:\n","                print(\"Null hypothesis not rejected -> I assume that each difference in {} is due to the randomness\".format(metric))\n","        print(\"-------------------------------------------------\")\n","    return mean_dict, inter_dict\n","\n","\n","def plot_model_comparisons(nb_models, metrics, dir_dict, mean_dict, inter_dict, sub_dir, title, save_plot):\n","    # get colors\n","    colors = sns.color_palette(\"hls\", nb_models)\n","\n","    fig, ax = plt.subplots(figsize=(15,8))\n","    index = np.arange(len(metrics))\n","    bar_width = 0.1\n","    opacity = 0.8\n","\n","    for i in range(1, nb_models+1):\n","        rects = plt.bar(index + (i-1)*bar_width, mean_dict['model%d' %(i)], bar_width, yerr = inter_dict['model%d' %(i)],\n","                        capsize = 5, alpha = opacity, color = colors[i-1],\n","                        label = dir_dict[i].split(sep='/')[-2])\n","\n","    font_size = 15\n","    font_size_ticks = 13\n","    plt.ylabel('Scores', fontsize = font_size)\n","    plt.title(title, fontsize = font_size)\n","    plt.xticks(index + (nb_models/2-0.5)*bar_width, tuple([name.capitalize() for name in metrics]), fontsize = font_size)\n","    plt.yticks(fontsize = font_size_ticks)\n","    #plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True, ncol=2, prop={'size': font_size-3})\n","    plt.legend(loc = 'lower left', prop={'size': font_size})\n","    plt.grid(True)\n","    #ax.set_ylim(ymin=0.75)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # SAVE PLOT\n","    if save_plot:\n","        dir = sub_dir + \"model_comparisons/\"\n","        os.makedirs(dir, exist_ok=True)\n","        fig.savefig(\"{}/{}.jpg\".format(dir, title.replace(\" \", \"_\")), bbox_inches='tight')\n","\n","\n","def plot_frozen_layers(nb_models, metrics, dir_dict, mean_dict, inter_dict, sub_dir, title, save_plot):\n","    # get colors\n","    colors = sns.color_palette(\"hls\", nb_models)\n","    a = np.arange(nb_models)\n","    x = [0, 165, 170, 172, 174]\n","    assert nb_models == len(x)\n","\n","    fig, ax = plt.subplots(figsize=(15,8))\n","\n","    for j in range(len(metrics)):\n","        metric_mean = []\n","        metric_yerr = []\n","        for i in range(1, nb_models+1):\n","            #ax.plot(a, mean_dict['model%d' %(i)], color = colors[i-1], label = dir_dict[i].split(sep='/')[-2])\n","            metric_mean.append(mean_dict['model%d' %(i)][j])\n","            metric_yerr.append(inter_dict['model%d' %(i)][j])\n","\n","        ax.errorbar(a, metric_mean, yerr = metric_yerr, capsize=5, capthick=2,\n","                    fmt='-o', color = colors[j], label = metrics[j].capitalize())\n","        bellow = np.asarray(metric_mean)-np.asarray(metric_yerr)\n","        up = np.asarray(metric_mean)+np.asarray(metric_yerr)\n","        ax.fill_between(a, bellow.tolist(), up.tolist(), color=colors[j], alpha=.1)\n","    \n","    font_size = 15\n","    font_size_ticks = 13\n","    plt.xlabel('Number of frozen layers in backbone', fontsize = font_size)\n","    plt.ylabel('Scores', fontsize = font_size)\n","    plt.title(title, fontsize = font_size)\n","    ax.xaxis.set_ticks(a) #set the ticks to be a\n","    ax.xaxis.set_ticklabels(x, fontsize = font_size_ticks) # change the ticks' names to x\n","    #ax.set_ylim(ymin=0.25)\n","    #plt.xticks(x, fontsize = font_size_ticks)\n","    start, end = ax.get_ylim()\n","    ax.yaxis.set_ticks(np.arange(np.round(start, 1), np.round(end, 1), 0.05))\n","    plt.yticks(fontsize = font_size_ticks)\n","    plt.legend(loc = 'lower left', prop={'size': font_size})\n","    plt.grid(True)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # SAVE PLOT\n","    if save_plot:\n","        dir = sub_dir + \"model_comparisons/\"\n","        os.makedirs(dir, exist_ok=True)\n","        fig.savefig(\"{}/{}.jpg\".format(dir, title.replace(\" \", \"_\")), bbox_inches='tight')"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bZ417Sp1nyaE"},"source":["###1.2. Metric analysis and plotting"]},{"cell_type":"code","metadata":{"id":"27PGdZGuszTa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625217488927,"user_tz":-120,"elapsed":5132,"user":{"displayName":"Raquel Carmo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUUTZlzR-089p-AQlc4K-YNrw3DWlq1fWmuzLsdfI=s64","userId":"09214858898201854328"}},"outputId":"b2b6b336-bd8b-4713-a39e-91c839a4fe56"},"source":["#\"SAR_swath_images_VV+VH+WS_dilated/classification_results/categorization/test_eye_only/ResNet_nu-F_bs-8_416x416_lr-1e-05_ep-30_sp-5_no-zT_cr-u1_ag-T_drp-F/csv\"\n","dirs = [\"SAR_swath_images_VV+VH+WS/classification_results/identification/ResNet_Numeric-F_BatchSize-8_lr-0.0001_Epochs-20_Folds-5_Norm-T/csv\", \n","        \"SAR_swath_images_WS+WS+WS/classification_results/identification/ResNet_Numeric-F_BatchSize-8_lr-0.0001_Epochs-20_Folds-5_Norm-T/csv\",\n","        \"SAR_swath_images_VV+VH+VH/classification_results/identification/ResNet_Numeric-F_BatchSize-8_lr-0.0001_Epochs-20_Folds-5_Norm-T/csv\",\n","        \"SAR_swath_images_WS+sWSO+cWSO/classification_results/identification/ResNet_Numeric-F_BatchSize-8_lr-0.0001_Epochs-20_Folds-5_Norm-T/csv\"]\n","\n","for check_dir in dirs:\n","    print(check_dir.split('/')[0])\n","    file = glob.glob(check_dir + '/test_f1-score.pkl')\n","\n","    if file == []:\n","        compute_f1(check_dir)\n","    #check_dir = dir_dict[9]\n","\n","    acc = np.asarray(pickle.load(open(check_dir + \"/test_accuracy.pkl\",  \"rb\")))\n","    print(\"Accuracy:\", acc, np.mean(acc), np.std(acc))\n","\n","    prec = np.asarray(pickle.load(open(check_dir + \"/test_precision.pkl\",  \"rb\")))\n","    print(\"Precision:\", prec, np.mean(prec), np.std(prec))\n","\n","    rec = np.asarray(pickle.load(open(check_dir + \"/test_recall.pkl\",  \"rb\")))\n","    print(\"Recall:\", rec, np.mean(rec), np.std(rec))\n","\n","    f1 = np.asarray(pickle.load(open(check_dir + \"/test_f1-score.pkl\",  \"rb\")))\n","    print(\"F1-score:\", f1, np.mean(f1), np.std(f1))\n","    print('\\n')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["SAR_swath_images_VV+VH+WS\n","Accuracy: [0.95652175 0.95652175 0.97777778 1.         0.97777778] 0.9737198114395141 0.01621807129762225\n","Precision: [1.         0.93333334 1.         1.         1.        ] 0.9866666674613953 0.026666665077209474\n","Recall: [0.9285714  1.         0.96428573 1.         0.96296299] 0.9711640238761902 0.026801963866203593\n","F1-score: [0.96296295 0.96551724 0.98181819 1.         0.98113209] 0.9782860940616118 0.013340999999519544\n","\n","\n","SAR_swath_images_WS+WS+WS\n","Accuracy: [0.93478262 1.         0.97777778 1.         0.97777778] 0.9780676364898682 0.023815194552875604\n","Precision: [0.96296299 1.         1.         1.         1.        ] 0.9925925970077515 0.014814805984497071\n","Recall: [0.9285714  1.         0.96428573 1.         0.96296299] 0.9711640238761902 0.026801963866203593\n","F1-score: [0.94545454 1.         0.98181819 1.         0.98113209] 0.9816809640858487 0.019919075511699975\n","\n","\n","SAR_swath_images_VV+VH+VH\n","Accuracy: [0.9130435  0.97826087 0.97777778 1.         1.        ] 0.973816430568695 0.03193729229259011\n","Precision: [0.96153843 1.         1.         1.         1.        ] 0.9923076868057251 0.015384626388549807\n","Recall: [0.89285713 0.96428573 0.96428573 1.         1.        ] 0.9642857193946839 0.03912304293101864\n","F1-score: [0.92592591 0.98181819 0.98181819 1.         1.        ] 0.9779124579767 0.027235380803345477\n","\n","\n","SAR_swath_images_WS+sWSO+cWSO\n","Accuracy: [0.9130435  0.89130437 0.95555556 0.97777778 0.95555556] 0.9386473536491394 0.031622397374501815\n","Precision: [0.9285714  0.92592591 0.96428573 0.96428573 0.96296299] 0.9492063522338867 0.0179543888254935\n","Recall: [0.9285714  0.89285713 0.96428573 1.         0.96296299] 0.949735450744629 0.03632151712863789\n","F1-score: [0.9285714  0.9090909  0.96428573 0.98181819 0.96296299] 0.9493458414511247 0.026514592951919966\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zNja6m1MX6cx"},"source":["# compute F1-score if not already computed previously\n","for i in range(1, models+1):\n","    dir = dir_dict[i]\n","    file = glob.glob(dir + '/test_f1-score.pkl')\n","    if file == []:\n","        compute_f1(dir)\n","\n","# metrics to use for analysis\n","metrics = [\"accuracy\", \"precision\", \"recall\", \"f1-score\"]\n","# compute mean and confidence intervals for each metric and each model\n","mean_dict, inter_dict = compute_stats(models, dir_dict, metrics)\n","# plot model comparison\n","plot_model_comparisons(models, metrics, dir_dict, mean_dict, inter_dict, sub_dir, title, save_plot)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K9695aBjozMD"},"source":["###1.3. Plot Scores vs. Nb of frozen layers"]},{"cell_type":"code","metadata":{"id":"QcXw8M2ks9d5"},"source":["# compute F1-score if not already computed previously\n","for i in range(1, models+1):\n","    dir = dir_dict[i]\n","    file = glob.glob(dir + '/test_f1-score.pkl')\n","    if file == []:\n","        print(\"No f1-score file found.\")\n","        compute_f1(dir)\n","\n","# metrics to use for analysis\n","metrics = [\"accuracy\", \"precision\", \"recall\", \"f1-score\"]\n","# compute mean and confidence intervals for each metric and each model\n","mean_dict, inter_dict = compute_stats(models, dir_dict, metrics)\n","# plot model comparison\n","plot_frozen_layers(models, metrics, dir_dict, mean_dict, inter_dict, sub_dir, title, save_plot)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4EkR1bLbnfoT"},"source":["###1.4. [DEPRECATED] Metric analysis and plotting"]},{"cell_type":"code","metadata":{"id":"MAopwYgkSJdH"},"source":["#[DEPRECATED]\n","# ACCURACY analysis\n","model1_accuracy = pickle.load(open(MODEL_1_DIRECTORY + \"/test_accuracy.pkl\",  \"rb\" ))\n","model2_accuracy = pickle.load(open(MODEL_2_DIRECTORY + \"/test_accuracy.pkl\",  \"rb\" ))\n","\n","model1_accuracy_mean, b_model1_acc, u_model1_acc = mean_confidence_interval(model1_accuracy)\n","model2_accuracy_mean, b_model2_acc, u_model2_acc = mean_confidence_interval(model2_accuracy)\n","\n","if models > 2:\n","\n","    model3_accuracy = pickle.load(open(MODEL_3_DIRECTORY + \"/test_accuracy.pkl\",  \"rb\" ))\n","    model4_accuracy = pickle.load(open(MODEL_4_DIRECTORY + \"/test_accuracy.pkl\",  \"rb\" ))\n","\n","    model3_accuracy_mean, b_model3_acc, u_model3_acc = mean_confidence_interval(model3_accuracy)\n","    model4_accuracy_mean, b_model4_acc, u_model4_acc = mean_confidence_interval(model4_accuracy)\n","\n","    acc_dict = {\"model1_accuracy\": model1_accuracy, \"model2_accuracy\": model2_accuracy, \n","                \"model3_accuracy\": model3_accuracy, \"model4_accuracy\": model4_accuracy}\n","    print(\"---------------- ACCURACY analysis ----------------\")\n","    for x, y in itertools.combinations(acc_dict.keys(), 2):\n","        t, p = scipy.stats.ttest_ind(acc_dict[x], acc_dict[y])\n","        print(\"t value accuracy between {} and {}: {}\".format(x, y, t))\n","        if t > criticalTvalue or t < -criticalTvalue :\n","            print(\"Null hypothesis rejected -> there is a significant difference in the two accuracy distributions\")\n","        else:\n","            print(\"Null hypothesis not rejected -> I assume that each difference in accuracy is due to the randomness\")\n","    print(\"-------------------------------------------------\")\n","\n","else:\n","    print(\"---------------- ACCURACY analysis ----------------\")\n","    t, p = scipy.stats.ttest_ind(model1_accuracy, model2_accuracy)\n","    print(\"t value accuracy: {}\".format(t))\n","\n","    if t > criticalTvalue or t < -criticalTvalue :\n","        print(\"Null hypothesis rejected -> there is a significant difference in the two accuracy distributions\")\n","    else:\n","        print(\"Null hypothesis not rejected -> I assume that each difference in accuracy is due to the randomness\")\n","    print(\"-------------------------------------------------\")\n","\n","\n","# PRECISION analysis\n","model1_precision = pickle.load(open(MODEL_1_DIRECTORY + \"/test_precision.pkl\",  \"rb\" ))\n","model2_precision = pickle.load(open(MODEL_2_DIRECTORY + \"/test_precision.pkl\",  \"rb\" ))\n","\n","model1_precision_mean, b_model1_p, u_model1_p = mean_confidence_interval(model1_precision)\n","model2_precision_mean, b_model2_p, u_model2_p = mean_confidence_interval(model2_precision)\n","\n","if models > 2:\n","\n","    model3_precision = pickle.load(open(MODEL_3_DIRECTORY + \"/test_precision.pkl\",  \"rb\" ))\n","    model4_precision = pickle.load(open(MODEL_4_DIRECTORY + \"/test_precision.pkl\",  \"rb\" ))\n","\n","    model3_precision_mean, b_model3_p, u_model3_p = mean_confidence_interval(model3_precision)\n","    model4_precision_mean, b_model4_p, u_model4_p = mean_confidence_interval(model4_precision)\n","\n","    prec_dict = {\"model1_precision\": model1_precision, \"model2_precision\": model2_precision, \n","                 \"model3_precision\": model3_precision, \"model4_precision\": model4_precision}\n","    print(\"---------------- PRECISION analysis ----------------\")\n","    for x, y in itertools.combinations(prec_dict.keys(), 2):\n","        t, p = scipy.stats.ttest_ind(prec_dict[x], prec_dict[y])\n","        print(\"t value precision between {} and {}: {}\".format(x, y, t))\n","        if t > criticalTvalue or t < -criticalTvalue :\n","            print(\"Null hypothesis rejected -> there is a significant difference in the two precision distributions\")\n","        else:\n","            print(\"Null hypothesis not rejected -> I assume that each difference in precision is due to the randomness\")\n","    print(\"-------------------------------------------------\")\n","\n","else:\n","    print(\"---------------- PRECISION analysis ----------------\")\n","    t, p = scipy.stats.ttest_ind(model1_precision, model2_precision)\n","    print(\"t value precision: {}\".format(t))\n","\n","    if t > criticalTvalue or t < -criticalTvalue :\n","        print(\"Null hypothesis rejected -> there is a significant difference in the two precision distributions\")\n","    else:\n","        print(\"Null hypothesis not rejected -> I assume that each difference in precision is due to the randomness\")\n","    print(\"-------------------------------------------------\")\n","    \n","\n","# RECALL analysis\n","model1_recall = pickle.load(open(MODEL_1_DIRECTORY + \"/test_recall.pkl\",  \"rb\" ))\n","model2_recall = pickle.load(open(MODEL_2_DIRECTORY + \"/test_recall.pkl\",  \"rb\" ))\n","\n","model1_recall_mean, b_model1_r, u_model1_r = mean_confidence_interval(model1_recall)\n","model2_recall_mean, b_model2_r, u_model2_r = mean_confidence_interval(model2_recall)\n","\n","if models > 2:\n","\n","    model3_recall = pickle.load(open(MODEL_3_DIRECTORY + \"/test_recall.pkl\",  \"rb\" ))\n","    model4_recall = pickle.load(open(MODEL_4_DIRECTORY + \"/test_recall.pkl\",  \"rb\" ))\n","\n","    model3_recall_mean, b_model3_r, u_model3_r = mean_confidence_interval(model3_recall)\n","    model4_recall_mean, b_model4_r, u_model4_r = mean_confidence_interval(model4_recall)\n","\n","    rec_dict = {\"model1_recall\": model1_recall, \"model2_recall\": model2_recall, \n","                \"model3_recall\": model3_recall, \"model4_recall\": model4_recall}\n","    print(\"---------------- RECALL analysis ----------------\")\n","    for x, y in itertools.combinations(rec_dict.keys(), 2):\n","        t, p = scipy.stats.ttest_ind(rec_dict[x], rec_dict[y])\n","        print(\"t value recall between {} and {}: {}\".format(x, y, t))\n","        if t > criticalTvalue or t < -criticalTvalue :\n","            print(\"Null hypothesis rejected -> there is a significant difference in the two recall distributions\")\n","        else:\n","            print(\"Null hypothesis not rejected -> I assume that each difference in recall is due to the randomness\")\n","    print(\"-------------------------------------------------\")\n","\n","else:\n","    print(\"---------------- RECALL analysis ----------------\")\n","    t, p = scipy.stats.ttest_ind(model1_recall, model2_recall)\n","    print(\"t value recall: {}\".format(t))\n","\n","    if t > criticalTvalue or t < -criticalTvalue :\n","        print(\"Null hypothesis rejected -> there is a significant difference in the two recall distributions\")\n","    else:\n","        print(\"Null hypothesis not rejected -> I assume that each difference in recall is due to the randomness\")\n","    print(\"-------------------------------------------------\")\n","\n","\n","# COMPUTE CONFIDENCE INTERVALS\n","confidence_model1 = [u_model1_acc - b_model1_acc, u_model1_p - b_model1_p, u_model1_r - b_model1_r]\n","confidence_model2 = [u_model2_acc - b_model2_acc, u_model2_p - b_model2_p, u_model2_r - b_model2_r]\n","confidence_model3 = [u_model3_acc - b_model3_acc, u_model3_p - b_model3_p, u_model3_r - b_model3_r]\n","confidence_model4 = [u_model4_acc - b_model4_acc, u_model4_p - b_model4_p, u_model4_r - b_model4_r]\n","\n","# DATA TO PLOT\n","n_groups = 3\n","means_model1 = (model1_accuracy_mean, model1_precision_mean, model1_recall_mean)\n","means_model2 = (model2_accuracy_mean, model2_precision_mean, model2_recall_mean)\n","means_model3 = (model3_accuracy_mean, model3_precision_mean, model3_recall_mean)\n","means_model4 = (model4_accuracy_mean, model4_precision_mean, model4_recall_mean)\n","\n","# CREATE PLOT\n","fig, ax = plt.subplots(figsize=(15,8))\n","index = np.arange(n_groups)\n","bar_width = 0.1\n","opacity = 0.8\n","\n","rects1 = plt.bar(index, means_model1, bar_width, yerr = confidence_model1, capsize = 5, alpha = opacity, color = 'g',\n","                 label = MODEL_1_DIRECTORY.split(sep='/')[-2])\n","\n","rects2 = plt.bar(index + bar_width, means_model2, bar_width, yerr = confidence_model2, capsize = 5, alpha = opacity, color = 'r',\n","                 label = MODEL_2_DIRECTORY.split(sep='/')[-2])\n","\n","rects3 = plt.bar(index + 2 * bar_width, means_model3, bar_width, yerr = confidence_model3, capsize = 5, alpha = opacity, color = 'b',\n","                 label = MODEL_3_DIRECTORY.split(sep='/')[-2])\n","\n","rects4 = plt.bar(index + 3 * bar_width, means_model4, bar_width, yerr = confidence_model4, capsize = 5, alpha = opacity, color = 'y',\n","                 label = MODEL_4_DIRECTORY.split(sep='/')[-2])\n","\n","font_size = 15\n","font_size_ticks = 15\n","plt.ylabel('Scores', fontsize = font_size)\n","plt.title(title, fontsize = font_size)\n","plt.xticks(index + 1.5*bar_width, ('Accuracy', 'Precision', 'Recall'), fontsize = font_size_ticks)\n","plt.yticks(fontsize = font_size_ticks)\n","plt.legend(loc = 'lower left', prop={'size': font_size})\n","plt.grid(True)\n","#ax.set_ylim(ymin=0.75)\n","plt.tight_layout()\n","plt.show()\n","\n","# SAVE PLOT\n","if save_plot:\n","    dir = sub_dir + \"model_comparisons/\"\n","    os.makedirs(dir, exist_ok=True)\n","    fig.savefig(\"{}/{}.jpg\".format(dir, title), bbox_inches='tight')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UtrFW51bpY4h"},"source":["###1.5. [DEPRECATED] Plot confusion matrices"]},{"cell_type":"code","metadata":{"id":"C50SOvWvSL6x"},"source":["#COMPARE CONFUSION MATRIX\n","TP_model1 = pickle.load(open(MODEL_1_DIRECTORY + \"/test_tp.pkl\",  \"rb\" ))\n","FP_model1 = pickle.load(open(MODEL_1_DIRECTORY + \"/test_fp.pkl\",  \"rb\" ))\n","TN_model1 = pickle.load(open(MODEL_1_DIRECTORY + \"/test_tn.pkl\",  \"rb\" ))\n","FN_model1 = pickle.load(open(MODEL_1_DIRECTORY + \"/test_fn.pkl\",  \"rb\" ))\n","\n","TP_model2 = pickle.load(open(MODEL_2_DIRECTORY + \"/test_tp.pkl\",  \"rb\" ))\n","FP_model2 = pickle.load(open(MODEL_2_DIRECTORY + \"/test_fp.pkl\",  \"rb\" ))\n","TN_model2 = pickle.load(open(MODEL_2_DIRECTORY + \"/test_tn.pkl\",  \"rb\" ))\n","FN_model2 = pickle.load(open(MODEL_2_DIRECTORY + \"/test_fn.pkl\",  \"rb\" ))\n","\n","fold = 0\n","for tp_1, fp_1, tn_1, fn_1, tp_2, fp_2, tn_2, fn_2 in zip(TP_model1, FP_model1, TN_model1, FN_model1, TP_model2, FP_model2, TN_model2, FN_model2):\n","  fold += 1\n","\n","  print(\"Model 1 confusion matrix\")\n","  fig, ax = plt.subplots()  \n","  my_confusion_matrix = [[tp_1, fn_1], [fp_1, tn_1]]\n","  plt.imshow(my_confusion_matrix, cmap=plt.cm.Blues)\n","  plt.ylabel(\"Not present                 Present\")\n","  plt.xticks([], [])\n","  plt.yticks([], [])\n","  plt.title(\"Present                  Not present\")\n","  #plt.colorbar() \n","  ax.text(0, 0, int(tp_1), ha=\"center\", va=\"center\", color=\"black\")\n","  ax.text(1, 0, int(fn_1), ha=\"center\", va=\"center\", color=\"black\")\n","  ax.text(0, 1, int(fp_1), ha=\"center\", va=\"center\", color=\"black\")\n","  ax.text(1, 1, int(tn_1), ha=\"center\", va=\"center\", color=\"black\")\n","  plt.show()\n","\n","  fig.savefig(MODEL_1_DIRECTORY + \"/confusion_matrix_{}.jpg\".format(fold))\n","\n","  print(\"Model 2 confusion matrix\")\n","  fig, ax = plt.subplots()  \n","  my_confusion_matrix = [[tp_2, fn_2], [fp_2, tn_2]]\n","  plt.imshow(my_confusion_matrix, cmap=plt.cm.Blues)\n","  plt.ylabel(\"Not present                 Present\")\n","  plt.xticks([], [])\n","  plt.yticks([], [])\n","  plt.title(\"Present                  Not present\")\n","  #plt.colorbar() \n","  ax.text(0, 0, int(tp_2), ha=\"center\", va=\"center\", color=\"black\")\n","  ax.text(1, 0, int(fn_2), ha=\"center\", va=\"center\", color=\"black\")\n","  ax.text(0, 1, int(fp_2), ha=\"center\", va=\"center\", color=\"black\")\n","  ax.text(1, 1, int(tn_2), ha=\"center\", va=\"center\", color=\"black\")\n","  plt.show()\n","\n","  fig.savefig(MODEL_2_DIRECTORY + \"/confusion_matrix_{}.jpg\".format(fold))\n","\n","  \"\"\"\n","  print(\"Fold {}:    Model 1                     Model 2\".format(fold))\n","  fig, (ax1, ax2) = plt.subplots(1, 2)\n","  my_confusion_matrix = [[tp_1, fn_1], [fp_1, tn_1]]\n","  ax1.imshow(my_confusion_matrix, cmap=plt.cm.Blues)\n","  ax1.set(ylabel = \"Not present                 Present\",\n","          title = \"   Present          Not present\")\n","  ax1.set_xticks([])\n","  ax1.set_yticks([])\n","  #plt.colorbar() \n","  ax1.text(0, 0, int(tp_1), ha=\"center\", va=\"center\", color=\"black\")\n","  ax1.text(1, 0, int(fn_1), ha=\"center\", va=\"center\", color=\"black\")\n","  ax1.text(0, 1, int(fp_1), ha=\"center\", va=\"center\", color=\"black\")\n","  ax1.text(1, 1, int(tn_1), ha=\"center\", va=\"center\", color=\"black\")\n","\n","  my_confusion_matrix = [[tp_2, fn_2], [fp_2, tn_2]]\n","  ax2.imshow(my_confusion_matrix, cmap=plt.cm.Blues)\n","  ax2.set(ylabel = \"Not present                 Present\",\n","          title = \"   Present           Not present\")\n","  ax2.set_xticks([])\n","  ax2.set_yticks([])\n","  #plt.colorbar() \n","  ax2.text(0, 0, int(tp_2), ha=\"center\", va=\"center\", color=\"black\")\n","  ax2.text(1, 0, int(fn_2), ha=\"center\", va=\"center\", color=\"black\")\n","  ax2.text(0, 1, int(fp_2), ha=\"center\", va=\"center\", color=\"black\")\n","  ax2.text(1, 1, int(tn_2), ha=\"center\", va=\"center\", color=\"black\")\n","  plt.tight_layout()\n","  plt.show()\n","  \"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U90HRPq1bC9p"},"source":["## 2. Comparison between 6 models across datasets"]},{"cell_type":"code","metadata":{"id":"x9AOLrxYbFQM"},"source":["# COMPARISON BETWEEN FOUR MODELS\n","NETWORK = \"Mobile\"\n","models = 6\n","MODEL_1_DIRECTORY = \"SAR_swath_images_VV+VH+WS/classification_results/categorization/{}_Numeric-F_BatchSize-8_lr-0.0001_Epochs-20_Folds-5_Norm-T_Aug-T/csv\".format(NETWORK)\n","MODEL_2_DIRECTORY = \"SAR_swath_images_VV+VH+VH/classification_results/categorization/{}_Numeric-F_BatchSize-8_lr-0.0001_Epochs-20_Folds-5_Norm-T_Aug-T/csv\".format(NETWORK)\n","MODEL_3_DIRECTORY = \"SAR_swath_images_WS+WS+WS/classification_results/categorization/{}_Numeric-F_BatchSize-8_lr-0.0001_Epochs-20_Folds-5_Norm-T_Aug-T/csv\".format(NETWORK)\n","MODEL_4_DIRECTORY = \"SAR_swath_images_WS+sWSO+cWSO/classification_results/categorization/{}_Numeric-F_BatchSize-8_lr-0.0001_Epochs-20_Folds-5_Norm-T_Aug-T/csv\".format(NETWORK)\n","MODEL_5_DIRECTORY = \"SAR_swath_images_VV+VV+VV/classification_results/categorization/{}_Numeric-F_BatchSize-8_700x400_lr-0.0001_Epochs-20_Folds-5_Norm-T_Aug-T/csv\".format(NETWORK)\n","MODEL_6_DIRECTORY = \"SAR_swath_images_VH+VH+VH/classification_results/categorization/{}_Numeric-F_BatchSize-8_700x400_lr-0.0001_Epochs-20_Folds-5_Norm-T_Aug-T/csv\".format(NETWORK)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e9nfHkfGOfiY"},"source":["# Critical t-value to see the difference in two distribution with 5 samples\n","# 80% confidence\n","criticalTvalue = 1.533\n","\n","# 90% confidence\n","#criticalTvalue = 2.132\n","\n","# 95% confidence\n","#criticalTvalue = 2.776\n","\n","\n","def mean_confidence_interval(data, confidence=0.80):\n","    a = 1.0 * np.array(data)\n","    n = len(a)\n","    m, se = np.mean(a), scipy.stats.sem(a)\n","    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n","    return m, m-h, m+h\n","\n","\n","# ACCURACY analysis\n","model1_accuracy = pickle.load(open(MODEL_1_DIRECTORY + \"/test_accuracy.pkl\",  \"rb\" ))\n","model2_accuracy = pickle.load(open(MODEL_2_DIRECTORY + \"/test_accuracy.pkl\",  \"rb\" ))\n","model3_accuracy = pickle.load(open(MODEL_3_DIRECTORY + \"/test_accuracy.pkl\",  \"rb\" ))\n","model4_accuracy = pickle.load(open(MODEL_4_DIRECTORY + \"/test_accuracy.pkl\",  \"rb\" ))\n","model5_accuracy = pickle.load(open(MODEL_5_DIRECTORY + \"/test_accuracy.pkl\",  \"rb\" ))\n","model6_accuracy = pickle.load(open(MODEL_6_DIRECTORY + \"/test_accuracy.pkl\",  \"rb\" ))\n","\n","model1_accuracy_mean, b_model1_acc, u_model1_acc = mean_confidence_interval(model1_accuracy)\n","model2_accuracy_mean, b_model2_acc, u_model2_acc = mean_confidence_interval(model2_accuracy)\n","model3_accuracy_mean, b_model3_acc, u_model3_acc = mean_confidence_interval(model3_accuracy)\n","model4_accuracy_mean, b_model4_acc, u_model4_acc = mean_confidence_interval(model4_accuracy)\n","model5_accuracy_mean, b_model5_acc, u_model5_acc = mean_confidence_interval(model5_accuracy)\n","model6_accuracy_mean, b_model6_acc, u_model6_acc = mean_confidence_interval(model6_accuracy)\n","\n","import itertools\n","acc_dict = {\"model1_accuracy\": model1_accuracy, \"model2_accuracy\": model2_accuracy, \"model3_accuracy\": model3_accuracy,\n","            \"model4_accuracy\": model4_accuracy, \"model5_accuracy\": model5_accuracy, \"model6_accuracy\": model6_accuracy}\n","print(\"---------------- ACCURACY analysis ----------------\")\n","for x, y in itertools.combinations(acc_dict.keys(), 2):\n","    t, p = scipy.stats.ttest_ind(acc_dict[x], acc_dict[y])\n","    print(\"t value accuracy between {} and {}: {}\".format(x, y, t))\n","    if t > criticalTvalue or t < -criticalTvalue :\n","        print(\"Null hypothesis rejected -> there is a significant difference in the two accuracy distributions\")\n","    else:\n","        print(\"Null hypothesis not rejected -> I assume that each difference in accuracy is due to the randomness\")\n","print(\"-------------------------------------------------\")\n","\n","\n","# PRECISION analysis\n","model1_precision = pickle.load(open(MODEL_1_DIRECTORY + \"/test_precision.pkl\",  \"rb\" ))\n","model2_precision = pickle.load(open(MODEL_2_DIRECTORY + \"/test_precision.pkl\",  \"rb\" ))\n","model3_precision = pickle.load(open(MODEL_3_DIRECTORY + \"/test_precision.pkl\",  \"rb\" ))\n","model4_precision = pickle.load(open(MODEL_4_DIRECTORY + \"/test_precision.pkl\",  \"rb\" ))\n","model5_precision = pickle.load(open(MODEL_5_DIRECTORY + \"/test_precision.pkl\",  \"rb\" ))\n","model6_precision = pickle.load(open(MODEL_6_DIRECTORY + \"/test_precision.pkl\",  \"rb\" ))\n","\n","model1_precision_mean, b_model1_p, u_model1_p = mean_confidence_interval(model1_precision)\n","model2_precision_mean, b_model2_p, u_model2_p = mean_confidence_interval(model2_precision)\n","model3_precision_mean, b_model3_p, u_model3_p = mean_confidence_interval(model3_precision)\n","model4_precision_mean, b_model4_p, u_model4_p = mean_confidence_interval(model4_precision)\n","model5_precision_mean, b_model5_p, u_model5_p = mean_confidence_interval(model5_precision)\n","model6_precision_mean, b_model6_p, u_model6_p = mean_confidence_interval(model6_precision)\n","\n","prec_dict = {\"model1_precision\": model1_precision, \"model2_precision\": model2_precision, \"model3_precision\": model3_precision,\n","            \"model4_precision\": model4_precision, \"model5_precision\": model5_precision, \"model6_precision\": model6_precision}\n","print(\"---------------- PRECISION analysis ----------------\")\n","for x, y in itertools.combinations(prec_dict.keys(), 2):\n","    t, p = scipy.stats.ttest_ind(prec_dict[x], prec_dict[y])\n","    print(\"t value precision between {} and {}: {}\".format(x, y, t))\n","    if t > criticalTvalue or t < -criticalTvalue :\n","        print(\"Null hypothesis rejected -> there is a significant difference in the two precision distributions\")\n","    else:\n","        print(\"Null hypothesis not rejected -> I assume that each difference in precision is due to the randomness\")\n","print(\"-------------------------------------------------\")\n","\n","\n","# RECALL analysis\n","model1_recall = pickle.load(open(MODEL_1_DIRECTORY + \"/test_recall.pkl\",  \"rb\" ))\n","model2_recall = pickle.load(open(MODEL_2_DIRECTORY + \"/test_recall.pkl\",  \"rb\" ))\n","model3_recall = pickle.load(open(MODEL_3_DIRECTORY + \"/test_recall.pkl\",  \"rb\" ))\n","model4_recall = pickle.load(open(MODEL_4_DIRECTORY + \"/test_recall.pkl\",  \"rb\" ))\n","model5_recall = pickle.load(open(MODEL_5_DIRECTORY + \"/test_recall.pkl\",  \"rb\" ))\n","model6_recall = pickle.load(open(MODEL_6_DIRECTORY + \"/test_recall.pkl\",  \"rb\" ))\n","\n","model1_recall_mean, b_model1_r, u_model1_r = mean_confidence_interval(model1_recall)\n","model2_recall_mean, b_model2_r, u_model2_r = mean_confidence_interval(model2_recall)\n","model3_recall_mean, b_model3_r, u_model3_r = mean_confidence_interval(model3_recall)\n","model4_recall_mean, b_model4_r, u_model4_r = mean_confidence_interval(model4_recall)\n","model5_recall_mean, b_model5_r, u_model5_r = mean_confidence_interval(model5_recall)\n","model6_recall_mean, b_model6_r, u_model6_r = mean_confidence_interval(model6_recall)\n","\n","rec_dict = {\"model1_recall\": model1_recall, \"model2_recall\": model2_recall, \"model3_recall\": model3_recall,\n","            \"model4_recall\": model4_recall, \"model5_recall\": model5_recall, \"model6_recall\": model6_recall}\n","print(\"---------------- RECALL analysis ----------------\")\n","for x, y in itertools.combinations(rec_dict.keys(), 2):\n","    t, p = scipy.stats.ttest_ind(rec_dict[x], rec_dict[y])\n","    print(\"t value recall between {} and {}: {}\".format(x, y, t))\n","    if t > criticalTvalue or t < -criticalTvalue :\n","        print(\"Null hypothesis rejected -> there is a significant difference in the two recall distributions\")\n","    else:\n","        print(\"Null hypothesis not rejected -> I assume that each difference in recall is due to the randomness\")\n","print(\"-------------------------------------------------\")\n","\n","\n","# COMPUTE CONFIDENCE INTERVALS\n","confidence_model1 = [u_model1_acc - b_model1_acc, u_model1_p - b_model1_p, u_model1_r - b_model1_r]\n","confidence_model2 = [u_model2_acc - b_model2_acc, u_model2_p - b_model2_p, u_model2_r - b_model2_r]\n","confidence_model3 = [u_model3_acc - b_model3_acc, u_model3_p - b_model3_p, u_model3_r - b_model3_r]\n","confidence_model4 = [u_model4_acc - b_model4_acc, u_model4_p - b_model4_p, u_model4_r - b_model4_r]\n","confidence_model5 = [u_model5_acc - b_model5_acc, u_model5_p - b_model5_p, u_model5_r - b_model5_r]\n","confidence_model6 = [u_model6_acc - b_model6_acc, u_model6_p - b_model6_p, u_model6_r - b_model6_r]\n","\n","# DATA TO PLOT\n","n_groups = 3\n","means_model1 = (model1_accuracy_mean, model1_precision_mean, model1_recall_mean)\n","means_model2 = (model2_accuracy_mean, model2_precision_mean, model2_recall_mean)\n","means_model3 = (model3_accuracy_mean, model3_precision_mean, model3_recall_mean)\n","means_model4 = (model4_accuracy_mean, model4_precision_mean, model4_recall_mean)\n","means_model5 = (model5_accuracy_mean, model5_precision_mean, model5_recall_mean)\n","means_model6 = (model6_accuracy_mean, model6_precision_mean, model6_recall_mean)\n","\n","# CREATE PLOT\n","fig, ax = plt.subplots(figsize=(15,8))\n","index = np.arange(n_groups)\n","bar_width = 0.1\n","opacity = 0.8\n","\n","rects1 = plt.bar(index, means_model1, bar_width, yerr = confidence_model1, capsize = 5, alpha = opacity, color = 'g',\n","                 #label = MODEL_1_DIRECTORY.split(sep='/')[0]\n","                 label = \"Dataset VV+VH+WS\")\n","\n","rects2 = plt.bar(index + bar_width, means_model2, bar_width, yerr = confidence_model2, capsize = 5, alpha = opacity, color = 'r',\n","                 #label = MODEL_2_DIRECTORY.split(sep='/')[0]\n","                 label = \"Dataset VV+VH+VH\")\n","\n","rects3 = plt.bar(index + 2 * bar_width, means_model3, bar_width, yerr = confidence_model3, capsize = 5, alpha = opacity, color = 'b',\n","                 #label = MODEL_3_DIRECTORY.split(sep='/')[0]\n","                 label = \"Dataset WS+WS+WS\")\n","\n","rects4 = plt.bar(index + 3 * bar_width, means_model4, bar_width, yerr = confidence_model4, capsize = 5, alpha = opacity, color = 'y',\n","                 #label = MODEL_4_DIRECTORY.split(sep='/')[0]\n","                 label = \"Dataset WS+sWSO+cWSO\")\n","\n","rects5 = plt.bar(index + 4 * bar_width, means_model5, bar_width, yerr = confidence_model5, capsize = 5, alpha = opacity, color = 'c',\n","                 #label = MODEL_5_DIRECTORY.split(sep='/')[0]\n","                 label = \"Dataset VV+VV+VV\")\n","\n","rects6 = plt.bar(index + 5 * bar_width, means_model6, bar_width, yerr = confidence_model6, capsize = 5, alpha = opacity, color = 'm',\n","                 #label = MODEL_6_DIRECTORY.split(sep='/')[0]\n","                 label = \"Dataset VH+VH+VH\")\n","\n","font_size = 15\n","font_size_ticks = 15\n","plt.ylabel('Scores', fontsize = font_size)\n","plt.title(MODEL_1_DIRECTORY.split(sep='/')[-2], fontsize = font_size)\n","#plt.title(\"{} across datasets\".format(NETWORK), fontsize = font_size)\n","plt.xticks(index + 1.5*bar_width, ('Accuracy', 'Precision', 'Recall'), fontsize = font_size_ticks)\n","plt.yticks(fontsize = font_size_ticks)\n","plt.legend(loc = 'lower left', prop={'size': font_size})\n","plt.grid(True)\n","#ax.set_ylim(ymin=0.75)\n","plt.tight_layout()\n","plt.show()\n","\n","# SAVE PLOT\n","save_plot = True\n","if save_plot:\n","    dir = \"model_comparisons_between_datasets/categorization/\"\n","    os.makedirs(dir, exist_ok=True)\n","    fig.savefig(dir + \"{}_across_{}_datasets.jpg\".format(MODEL_1_DIRECTORY.split(sep='/')[-2], models), bbox_inches='tight')\n","    #fig.savefig(dir + \"{}_across_{}_datasets.jpg\".format(NETWORK, models), bbox_inches='tight')"],"execution_count":null,"outputs":[]}]}