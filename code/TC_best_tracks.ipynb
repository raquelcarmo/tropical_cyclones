{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TC_best_tracks.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/raquelcarmo/tropical_cyclones/blob/main/src/code/TC_best_tracks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"_WA-kK-Hbjsv"},"source":["##TC Best Tracks\n","\n","This script associates the images dataset to the best track of each tropical cyclone and allows for data retrieval to be inserted in the parametric model."]},{"cell_type":"code","metadata":{"id":"Zx-SNat0aIDE"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DyQfiH37aRQk"},"source":["# getting in the directory \n","#%ls\n","# insert your path\n","%cd /content/drive/My\\ Drive/ESRIN_PhiLab/Tropical_Cyclones/data\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"60OzF-DhaVDO"},"source":["#!pip install imgaug==0.2.6\n","#!pip uninstall shapely\n","#!pip install shapely --no-binary shapely\n","#!apt-get install libproj-dev proj-data proj-bin  \n","#!apt-get install libgeos-dev  \n","#!pip install cython  \n","#!pip install cartopy  \n","#!pip install geoviews\n","#!pip install rasterio\n","!pip install netcdf4\n","#!pip install rioxarray"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vH87DLZVaXhK"},"source":["# general import\n","import bokeh.io\n","bokeh.io.output_notebook()\n","#import geoviews as gv\n","#import geoviews.feature as gf\n","#gv.extension('bokeh','matplotlib')\n","import pandas as pd\n","import numpy as np\n","import xarray as xr\n","#import rasterio as rio\n","#import rioxarray # geospatial extension for xarray\n","import os\n","import matplotlib.pyplot as plt\n","#import cartopy.crs as ccrs\n","#import cartopy\n","#from tqdm.auto import tqdm\n","#import netCDF4\n","import glob\n","import argparse\n","import cv2\n","from google.colab.patches import cv2_imshow\n","import math \n","from math import radians, cos, sin, asin, sqrt\n","from dateutil import parser\n","import datetime"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y_wdiinKakEm"},"source":["General settings to be changed accordingly."]},{"cell_type":"code","metadata":{"id":"UOYlE5miaaSf"},"source":["# insert here category, mission and feature you want to extract\n","CATEGORY = \"cat1\" # either 1, 2, 3, 4, 5\n","MISSION = \"sar\" # either sar, smap or smos\n","if MISSION == \"sar\":\n","  KEY = \"s1\"\n","  FEATURE_TO_SAVE = \"nrcs_detrend\"\n","  CHANNELS = [\"nrcs_detrend_cross\", \"nrcs_detrend_co\"]\n","elif MISSION == \"smap\":\n","  KEY = \"*smap\"\n","  FEATURE_TO_SAVE = \"wind_speed\"\n","else: # smos\n","  KEY = \"SM\"\n","  FEATURE_TO_SAVE = \"wind_speed\"\n","\n","# set download path\n","download_path = CATEGORY   # Tropical_Cyclones/data/cat1\n","os.makedirs(download_path, exist_ok=True)\n","\n","# set saving directory accordingly\n","SAVING_DIRECTORY = \"{}/{}_{}_{}\".format(CATEGORY, FEATURE_TO_SAVE, MISSION, CATEGORY)   # cat1/nrcs_detrend_sar_cat1\n","os.makedirs(SAVING_DIRECTORY, exist_ok=True)\n","\n","BEST_TRACK_PATH = \"{}/{}_{}_best_tracks\".format(CATEGORY, MISSION, CATEGORY)    # cat1/sar_cat1_best_tracks\n","os.makedirs(BEST_TRACK_PATH, exist_ok=True)\n","\n","# set format of output images\n","SAVE_FORMAT = \"png\"\n","\n","BB_WIDTH = 75 # pixels\n","TIME_DELTA = 10 # hours\n","THRESHOLD_DISTANCE = 200 # Km"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zk6-23hhaziV"},"source":["Script to associate the information relative to the TC best track with the images from Ifremer. Information is stored in CSV files."]},{"cell_type":"code","metadata":{"id":"VeSLMH9ua0Az"},"source":["# load generic best tracks CSV file\n","TC_dataset = pd.read_csv(\"best_track/ibtracs.since1980.list.v04r00.csv\", header=0)\n","iso_column = TC_dataset[\"ISO_TIME\"]\n","\n","file_list = glob.glob('{}/*.{}'.format(SAVING_DIRECTORY, SAVE_FORMAT))    # cat1/nrcs_detrend_sar_cat1/*.png\n","\n","for single_file in file_list:\n","  # get iso date and time of TC from the filename\n","  file_name = os.path.basename(single_file)[:-4]\n","\n","  # get the portion of the filename corresponding to the iso date\n","  file_name_iso = file_name[0:10]\n","\n","  # get rows in the TC best track that are taken in the same day of the TC\n","  img_info = TC_dataset.loc[TC_dataset['ISO_TIME'].str.contains(file_name_iso)]\n","\n","  # save relevant information in a csv file that could be retrived with pandas in the future\n","  img_info.to_csv(\"{}/{}.csv\".format(BEST_TRACK_PATH, file_name_iso))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_nJORAEkAGRW"},"source":["# Helper functions\n","def point_distance(lon1, lat1, lon2, lat2):\n","  \"\"\"\n","  Calculate the great circle distance between two points \n","  on the earth (specified in decimal degrees)\n","  \"\"\"\n","  # convert decimal degrees to radians \n","  lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n","  # haversine formula \n","  dlon = lon2 - lon1 \n","  dlat = lat2 - lat1 \n","  a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n","  c = 2 * asin(sqrt(a)) \n","  # Radius of earth in kilometers is 6371\n","  km = 6371* c\n","  return km\n","\n","def knots_to_m_sec(kts):\n","  \"\"\" Converts knots (kt) to meters/second (m/s) \"\"\"\n","  if np.isnan(kts):\n","    return kts\n","  else:\n","    return kts * 0.514444\n","\n","def nmiles_to_km(nm):\n","  \"\"\" Converts Nautical Miles (nm) to kilometers (km) \"\"\"\n","  if np.isnan(nm):\n","    return nm\n","  else:\n","    return nm / 0.53995680"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iUYwBoUGa2QY"},"source":["# construct the argument parser and parse the arguments\n","image_name_list = glob.glob(\"{}/*.{}\".format(SAVING_DIRECTORY, SAVE_FORMAT))   # cat1/nrcs_detrend_sar_cat1/*.png\n","nc_name_list = glob.glob(\"{}/{}*.nc\".format(CATEGORY, KEY))    # cat1/s1*.nc\n","best_track_name_list = glob.glob(\"{}/*.csv\".format(BEST_TRACK_PATH))    # cat1/sar_cat1_best_tracks/*.csv\n","\n","np.set_printoptions(threshold=1000)\n","\n","\n","for image_name in image_name_list:\n","  # ensure to have the right image, nc and csv files\n","  best_track_name = \"empty\"\n","  for best_track_name_i in best_track_name_list:\n","    if os.path.basename(image_name)[:-13] == os.path.basename(best_track_name_i)[:-4]:\n","      best_track_name = best_track_name_i\n","      break\n","\n","  if best_track_name == \"empty\":\n","    print(\"best track not found\")\n","    continue\n","\n","  nc_image = \"empty\"\n","  for nc_name_i in nc_name_list:\n","    nc_image_i = netCDF4.Dataset(nc_name_i, mode='r') \n","    tmax_units  = nc_image_i.variables['time'].units\n","    if os.path.basename(image_name)[:-4] == tmax_units[11:]:\n","        nc_image = nc_image_i\n","        break\n","\n","  if nc_image == \"empty\":\n","    print(\"nc not found\")\n","    continue\n","\n","  # extract the longitude and latidute in decimal degrees from the nc file\n","  referement_lon_matrix = nc_image.variables[\"lon\"][:]\n","  referement_lat_matrix  = nc_image.variables[\"lat\"][:]\n","  #print(\"coordinate referement lon matrix: {}\".format(referement_lon_matrix))\n","\n","  # get only the element in the middle as reference\n","  referement_lon = referement_lon_matrix[len(referement_lon_matrix)//2]\n","  referement_lat = referement_lat_matrix[len(referement_lat_matrix)//2]\n","\n","  # extract estimated latitude and longitude\n","  best_track_info = pd.read_csv(best_track_name)\n","\n","  # deleting element with time too distant wrt our reference\n","  referement_date = parser.parse(os.path.basename(image_name)[:-3])\n","\n","  best_track_info[\"ISO_TIME\"] =  best_track_info[\"ISO_TIME\"].apply(lambda x: parser.parse(x)) \n","\n","  best_track_info[\"ISO_TIME_DIFF\"] = (referement_date - best_track_info[\"ISO_TIME\"])\n","  best_track_info[\"ISO_TIME_DIFF\"] = best_track_info[\"ISO_TIME_DIFF\"].apply(lambda x: x.total_seconds()/3600)\n","\n","  best_track_info = best_track_info[best_track_info.ISO_TIME_DIFF < TIME_DELTA]\n","\n","  # delete null elements\n","  estimated_lon_df = pd.to_numeric(best_track_info.USA_LON, errors='coerce').dropna()\n","  estimated_lat_df  = pd.to_numeric(best_track_info.USA_LAT, errors='coerce').dropna()\n","\n","  concat_lon_lat = pd.concat([estimated_lon_df, estimated_lat_df], axis=1)\n","\n","  # compute distance between reference and estimated\n","  concat_lon_lat[\"distance_with_my_TC\"] = concat_lon_lat.apply(lambda x: point_distance(x.USA_LON, x.USA_LAT, referement_lon, referement_lat), axis=1, result_type = \"expand\")\n","  #print(concat_lon_lat)\n","  \n","  # prune the values that have distance too large wrt our nc file's coodinates\n","  concat_lon_lat = concat_lon_lat[concat_lon_lat[\"distance_with_my_TC\"] < THRESHOLD_DISTANCE]\n","  #print(\"after pruning: {}\".format(concat_lon_lat))\n","\n","  # get my estimated center\n","  estimated_lon = concat_lon_lat.USA_LON.mean()   \n","  estimated_lat = concat_lon_lat.USA_LAT.mean()   \n","\n","  # get the indexes of the closest longitude and latitude in the image wrt the estimated values\n","  key_lon = min(range(len(referement_lon_matrix)), key=lambda i: abs(referement_lon_matrix[i]-estimated_lon))\n","  key_lat = min(range(len(referement_lat_matrix)), key=lambda i: abs(referement_lat_matrix[i]-estimated_lat))\n","\n","  if key_lon == 0 or key_lon == len(referement_lon_matrix) or key_lat == 0 or key_lat == len(referement_lat_matrix):\n","    print(\"TC not found\")\n","    continue\n","  #print(\"estimated: {}, closest: {}, next: {}\".format(estimated_lon,referement_lon_matrix[key_lon],referement_lon_matrix[key_lon+1]))\n","\n","  ############################################\n","  # plot image and bounding box around the eye\n","  image = cv2.imread(image_name)\n","\n","  # draw a bounding box square and label on the image\n","  color = (0,0,255)\n","  #cv2.rectangle(image, (key_lon, key_lat), (key_lon + BB_WIDTH, key_lat + BB_WIDTH), color, 2)\n","  cv2.rectangle(image, (key_lon - BB_WIDTH, key_lat - BB_WIDTH), (key_lon + BB_WIDTH, key_lat + BB_WIDTH), color, 2)\n","  cv2_imshow(image)\n","\n","  #cv2.imwrite(\"./BB_TRIALS/{}\".format(os.path.basename(image_name)), image)\n","  #cv2.imwrite(\"./{}/{}\".format(IMAGE_DATA_DIRECTORY, os.path.basename(image_name)), image)\n","\n","  # here I have to create a .txt file for each image\n","  # add empty txt files for the background images\n","  #object label x_center y_center width[0,1] height[0,1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ApAr2mibB97"},"source":["# load dataframe connecting .nc filename to TC Name and USA_ATCF_ID\n","nc_ID = pd.read_csv(\"best_track/Cyclobs_info_names.csv\", header=0)\n","\n","for CAT in range(1, 6):\n","  # change directories accordingly\n","  BEST_TRACK_DIRECTORY = \"{}/{}_{}_best_tracks\".format(\"cat\" + str(CAT), MISSION, \"cat\" + str(CAT))\n","  IMAGE_DIRECTORY = \"{}/{}_{}_{}\".format(\"cat\" + str(CAT), FEATURE_TO_SAVE, MISSION, \"cat\" + str(CAT))\n","\n","  best_track_file_list = glob.glob(\"{}/*.csv\".format(BEST_TRACK_DIRECTORY))\n","  nc_file_list = glob.glob(\"{}/{}*.nc\".format(\"cat\" + str(CAT), KEY))    # cat1/s1*.nc\n","  image_file_list = glob.glob(\"{}/*.{}\".format(IMAGE_DIRECTORY, SAVE_FORMAT))   # cat1/nrcs_detrend_sar_cat1/*.png\n","\n","  # initialize list of values for each parameter\n","  Vt = list()   # Translation Speed\n","  Dt = list()   # Translation Direction\n","  R_Vm = list()   # Maximum Radius Winds\n","  Pcs = list()    # Central Pressure\n","  Pns = list()    # External Pressure\n","  Phi = list()    # Absolute Latitude\n","  Vmax = list()   # Maximum sutained wind speed\n","\n","  if (best_track_file_list == [] or nc_file_list == [] or image_file_list == []):\n","    print(\"No files found.\")\n","    continue\n","\n","  for image_name in image_file_list:\n","    # ensure to have the right image, nc and csv files\n","    best_track_name = None\n","    for best_track_filename in best_track_file_list:\n","      if os.path.basename(image_name)[:-13] == os.path.basename(best_track_filename)[:-4]:  # 2019-06-12\n","        best_track_path = best_track_filename\n","        best_track_name = os.path.basename(best_track_path)\n","        break\n","\n","    if best_track_name is None:\n","      print(\"best track not found!\")\n","      continue\n","\n","    nc_name = None\n","    for nc_filename in nc_file_list:\n","      nc_info = netCDF4.Dataset(nc_filename, mode='r') \n","\n","      if MISSION == \"sar\":\n","        tmax_units  = nc_info.variables['time'].units\n","      else: # smap or smos\n","        tmax_units  = nc_info.measurement_start_date\n","\n","      if os.path.basename(image_name)[:-4] == tmax_units[11:]:  # 2019-06-12 01:18:27\n","          np_path = nc_filename\n","          nc_name = os.path.basename(np_path)\n","          break\n","\n","    if nc_name is None:\n","      print(\"nc file not found!\")\n","      continue\n","\n","    print(nc_name, best_track_name)\n","    row = nc_ID.loc[nc_ID['data_url'] == nc_name]\n","    ID = np.unique(row['sid'])[0].upper()   # uppercase letters in ID to match the ones in best_track\n","\n","    # load best track dataframe\n","    best_track_info = pd.read_csv(best_track_path)\n","\n","    # filter dataframe to ID of TC and replace white spaces for np.nan\n","    df = best_track_info.loc[best_track_info['USA_ATCF_ID'] == ID].replace(r'^\\s*$', np.nan, regex=True)\n","    if df.empty:\n","      continue\n","    \n","    STORM_SPEED = pd.to_numeric(df['STORM_SPEED'], errors='coerce').dropna()\n","    STORM_DIR = pd.to_numeric(df['STORM_DIR'], errors='coerce').dropna()\n","    USA_RMW = pd.to_numeric(df['USA_RMW'], errors='coerce').dropna()\n","    TOKYO_PRES = pd.to_numeric(df['TOKYO_PRES'], errors='coerce').dropna()\n","    USA_POCI = pd.to_numeric(df['USA_POCI'], errors='coerce').dropna()\n","    USA_LAT = pd.to_numeric(df['USA_LAT'], errors='coerce').dropna()\n","    USA_WIND = pd.to_numeric(df['USA_WIND'], errors='coerce').dropna()\n","    \n","    # append mean of parameters' values to corresponding arrays\n","    Vt.append(knots_to_m_sec(STORM_SPEED.mean()))\n","    Dt.append(STORM_DIR.mean())\n","    R_Vm.append(nmiles_to_km(USA_RMW.mean()))\n","    Pcs.append(TOKYO_PRES.mean())  # mb = hPa\n","    Pns.append(USA_POCI.mean())   # mb = hPa\n","    Phi.append(USA_LAT.mean())\n","    Vmax.append(knots_to_m_sec(USA_WIND.mean()))\n","    # parameters missing: dPcst\n","\n","  # save lists of values for each parameter for each category\n","  filename = \"{}/parametric_values.csv\".format(BEST_TRACK_DIRECTORY)   # cat1/sar_cat1_best_tracks/parametric_values.csv\n","  parameters_df = pd.DataFrame(data={\"Vt (m/s)\": Vt, \"Dt (degrees)\": Dt, \"R_Vm (km)\": R_Vm, \"Pcs (hPa)\": Pcs, \"Pns (hPa)\": Pns, \"Phi (degrees)\": Phi, \"Vmax (m/s)\": Vmax})\n","  parameters_df.to_csv(filename, sep=',', index = False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9WXQ0OeXL7vc"},"source":["from scipy.interpolate import interp1d\n","\n","# load generic best tracks CSV file\n","TC_dataset = pd.read_csv(\"best_track/ibtracs.since1980.list.v04r00.csv\", header=0)\n","\n","# load dataframe connecting .nc filename to TC Name and USA_ATCF_ID\n","nc_ID = pd.read_csv(\"SAR_swath_nc/tc_dataframe.csv\", header=0)\n","\n","file_list = glob.glob('SAR_swath_nc/*/*.nc')\n","files = []\n","ids = []\n","cyclob_Vmax_list = []\n","ws_Vmax_list = []\n","best_track_Vmax_list = []\n","tmax_list=[]\n","\n","for single_file in file_list:\n","\n","  nc_filename = os.path.basename(single_file)\n","  df = nc_ID[nc_ID.data == nc_filename]\n","  cyclob_Vmax = df['vmax (m/s)'].values\n","  id = df.sid.values\n","\n","  full_info_image = netCDF4.Dataset(single_file, mode='r') \n","  tmax_units = datetime.datetime.strptime(full_info_image.measurementDate, '%Y-%m-%dT%H:%M:%SZ')\n","  tmax = tmax_units.strftime('%Y-%m-%d %H_%M_%S')\n","\n","  feature_wind = full_info_image.variables[\"wind_speed\"][:]\n","  ws_Vmax = np.max(feature_wind[0])\n","  #print(ws_Vmax)\n","\n","  day = tmax[:10]\n","\n","  # get iso date and time of TC from the filename\n","  #file_name = os.path.basename(single_file)[:-4]\n","\n","  # get the portion of the filename corresponding to the iso date\n","  #file_name_iso = file_name[0:10]\n","\n","  # get rows in the TC best track that are taken in the same day of the TC\n","  img_info = TC_dataset.loc[TC_dataset.ISO_TIME.str.contains(day)]\n","  img_info = img_info.loc[img_info.USA_ATCF_ID == id[0].upper()]\n","\n","  for index, row in img_info.iterrows():\n","    img_info['ISO_TIME'][index] = datetime.datetime.strptime(img_info['ISO_TIME'][index], '%Y-%m-%d %H:%M:%S')\n","  img_info['USA_WIND'] = img_info['USA_WIND'].astype(float)\n","  aux = img_info[['ISO_TIME', 'USA_WIND']]\n","\n","  df2 = pd.DataFrame([[tmax_units, np.nan]], columns=['ISO_TIME', 'USA_WIND'])\n","  #print(df2)\n","  aux = aux.append(df2, ignore_index=True)\n","  aux = aux.sort_values(by='ISO_TIME')\n","  #print(aux)\n","\n","  interp = aux.interpolate()\n","  #print(interp)\n","  best_track_Vmax = interp.USA_WIND.loc[interp.ISO_TIME == tmax_units].values\n","  #print(best_track_Vmax)\n","\n","  files.append(single_file)\n","  ids.append(id[0])\n","  cyclob_Vmax_list.append(cyclob_Vmax[0])\n","  best_track_Vmax_list.append(knots_to_m_sec(best_track_Vmax[0]))\n","  ws_Vmax_list.append(ws_Vmax)\n","  tmax_list.append(tmax)\n","\n","data = {\"nc\": files, \"sid\": ids, \"tmax\": tmax_list, \"Vmax_ws\": ws_Vmax_list, \"Vmax_cyclob\": cyclob_Vmax_list, \"Vmax_best_track\": best_track_Vmax_list}\n","final_df = pd.DataFrame(data)\n","final_df.to_csv(\"SAR_swath_Vmax/Vmax_info.csv\", index = False)\n","\n","# save relevant information in a csv file that could be retrived with pandas in the future\n","#img_info.to_csv(\"{}/{}.csv\".format(BEST_TRACK_PATH, file_name_iso))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m3K_6BJ4eV17"},"source":["request_url1=\"https://cyclobs.ifremer.fr/app/api/getData?cat_min=cat-1&cat_max=cat-2&mission=S1B,S1A&product_type=swath&include_cols=all\"\n","request_url2=\"https://cyclobs.ifremer.fr/app/api/getData?cat_min=cat-2&cat_max=cat-3&mission=S1B,S1A&product_type=swath&include_cols=all\"\n","request_url3=\"https://cyclobs.ifremer.fr/app/api/getData?cat_min=cat-3&cat_max=cat-4&mission=S1B,S1A&product_type=swath&include_cols=all\"\n","request_url4=\"https://cyclobs.ifremer.fr/app/api/getData?cat_min=cat-4&cat_max=cat-5&mission=S1B,S1A&product_type=swath&include_cols=all\"\n","request_url5=\"https://cyclobs.ifremer.fr/app/api/getData?cat_min=cat-5&mission=S1B,S1A&product_type=swath&include_cols=all\"\n","df1 = pd.read_csv(request_url1)\n","df1[\"request_url\"] = request_url1\n","df2 = pd.read_csv(request_url2)\n","df2[\"request_url\"] = request_url2\n","df3 = pd.read_csv(request_url3)\n","df3[\"request_url\"] = request_url3\n","df4 = pd.read_csv(request_url4)\n","df4[\"request_url\"] = request_url4\n","df5 = pd.read_csv(request_url5)\n","df5[\"request_url\"] = request_url5\n","#print(df_request.columns)\n","#for index, row in df_request.iterrows():\n","#  df_request[\"data_url\"][index] = df_request[\"data_url\"][index].split('/')[-1]\n","\n","frames = [df1, df2, df3, df4, df5]\n","result = pd.concat(frames)#.drop_duplicates(keep='last')\n","\n","result[\"data\"] = result[\"data_url\"].str.split('/', expand=True).iloc[:,-1:]\n","\n","#new_df = result[[\"data\", \"sid\", \"eye_in_acq\", \"acquisition_start_time\", \"maximum_cyclone_category\", \"vmax (m/s)\"]]\n","new_df = result[[\"request_url\", \"data_url\", \"sid\", \"eye_in_acq\", \"acquisition_start_time\", \"maximum_cyclone_category\", \"vmax (m/s)\"]]\n","url = new_df[\"data_url\"]\n","duplicates = new_df[url.isin(url[url.duplicated()])].sort_values(by=['data_url'])\n","#duplicates\n","duplicates.to_csv(\"SAR_swath_nc/duplicated_nc_dataframe.csv\", index = False)\n","\n","\n","#single_nc = new_df.iloc[3]\n","#path = 'SAR_swath_nc/category5/'\n","#full_info_image = netCDF4.Dataset(path + single_nc.data, mode='r')\n","\n","#tmax_units = datetime.datetime.strptime(full_info_image.measurementDate, '%Y-%m-%dT%H:%M:%SZ')\n","#tmax_units = tmax_units.strftime('%Y-%m-%d %H_%M_%S')\n","\n","#single_nc['tmax'] = tmax_units\n","#print(single_nc)\n","\n","#np.load(\"SAR_swath_Vmax/category5/\" + tmax_units+\"_Vmax.npy\")\n","#feature_co = full_info_image.variables[\"nrcs_detrend_co\"][:]\n","#bbox = single_nc.bounding_box\n","\n","#import re\n","#points = re.findall(\"[-+]?[.]?[\\d]+(?:,\\d\\d\\d)*[\\.]?\\d*(?:[eE][-+]?\\d+)?\", bbox)\n","#points = np.array(points).astype(np.float).reshape((5,2))\n","#print(points)\n","#import matplotlib\n","#fig = plt.figure(figsize=(10,10))\n","#ax = fig.add_subplot(111)\n","#plt.imshow(feature_co[0])\n","#ax.add_patch(matplotlib.patches.Polygon(points, closed = True, color = 'red'))\n","#print(np.max(feature_co[0]), np.min(feature_co[0]))\n","#print(\"feature_co\")\n","#plt.show()"],"execution_count":null,"outputs":[]}]}