{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TC_downloader_&_converter.ipynb","provenance":[],"collapsed_sections":["_PaVuZ_RgZmY","gzs88t_ymkwf","ScgK25TlVd2-","QzJBBp6HZdAh","xRfjWIAmi3cK","ygyOuX_Abug0","zSyR90FIb3tY","xYSJQrLgHugS"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"M0uQMbru4Gy9"},"source":["#TC Data Download and Convertion\n","\n","This script downloads the SMOS / SMAP / SAR data from the Cyclobs API and converts the resulting .nc files into .jpg files."]},{"cell_type":"markdown","metadata":{"id":"_PaVuZ_RgZmY"},"source":["##Imports and configurations"]},{"cell_type":"code","metadata":{"id":"mlDk9bA0dQty"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P7oC6PTgdZX2"},"source":["# getting in the directory \n","%ls\n","# insert your path\n","%cd /content/drive/My\\ Drive/ESRIN_PhiLab/Tropical_Cyclones/data\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mvdPqu4bevdo"},"source":["#!pip install imgaug==0.2.6\n","#!pip uninstall shapely\n","#!pip install shapely --no-binary shapely\n","#!apt-get install libproj-dev proj-data proj-bin  \n","#!apt-get install libgeos-dev  \n","#!pip install cython  \n","#!pip install cartopy  \n","#!pip install geoviews\n","#!pip install rasterio\n","!pip install netcdf4\n","#!pip install rioxarray"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2NjRUdQbjica"},"source":["# general import\n","#import bokeh.io\n","#bokeh.io.output_notebook()\n","#import geoviews as gv\n","#import geoviews.feature as gf\n","#gv.extension('bokeh','matplotlib')\n","import pandas as pd\n","import numpy as np\n","import xarray as xr\n","#import rasterio as rio\n","#import rioxarray # geospatial extension for xarray\n","import os\n","import matplotlib.pyplot as plt\n","#import cartopy.crs as ccrs\n","#import cartopy\n","from tqdm.auto import tqdm\n","import netCDF4\n","import glob\n","import argparse\n","import cv2\n","from google.colab.patches import cv2_imshow\n","import math \n","from math import radians, cos, sin, asin, sqrt\n","from dateutil import parser\n","import datetime\n","import sys\n","import cv2\n","import re\n","import random"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bJHxQz5Qat70"},"source":["[DEPRECATED] General settings to be changes accordingly."]},{"cell_type":"code","metadata":{"id":"wJ5hbTlmjUqJ"},"source":["\"\"\"\n","# insert here category, mission and feature you want to extract\n","CATEGORY = \"cat1\" # either 1, 2, 3, 4, 5\n","MISSION = \"smap\" # either sar, smap or smos\n","if MISSION == \"sar\":\n","    KEY = \"s1\"\n","    FEATURE_TO_SAVE = \"nrcs_detrend\"\n","    CHANNELS = [\"nrcs_detrend_cross\", \"nrcs_detrend_co\"]\n","elif MISSION == \"smap\":\n","    KEY = \"*smap\"\n","    FEATURE_TO_SAVE = \"wind_speed\"\n","else: # smos\n","    KEY = \"SM\"\n","    FEATURE_TO_SAVE = \"wind_speed\"\n","\n","# set download path\n","download_path = CATEGORY   # Tropical_Cyclones/data/cat1\n","os.makedirs(download_path, exist_ok=True)\n","\n","# set saving directory accordingly\n","SAVING_DIRECTORY = \"{}/{}_{}_{}\".format(CATEGORY, FEATURE_TO_SAVE, MISSION, CATEGORY)   # cat1/nrcs_detrend_sar_cat1\n","os.makedirs(SAVING_DIRECTORY, exist_ok=True)\n","\n","# set format of output images\n","SAVE_FORMAT = \"png\"\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gzs88t_ymkwf"},"source":["##1.Download the .nc files from the Cyclobs API"]},{"cell_type":"code","metadata":{"id":"AlFB4WXadK9X"},"source":["# doanload path\n","download_path = \"SAR_swath_nc/category56\"\n","os.makedirs(download_path, exist_ok=True)\n","\n","# make the resquest using cyclobs API and store the result in a pandas dataframe\n","#request_url=\"https://cyclobs.ifremer.fr/app/api/getData?cat_min=cat-2&mission=SMAP,SMOS,S1B,S1A&include_cols=all\"\n","#request_url=\"https://cyclobs.ifremer.fr/app/api/getData?sid=al122017&instrument=C-Band_SAR&product_type=gridded&include_cols=all\"\n","#request_url=\"https://cyclobs.ifremer.fr/app/api/getData?cat_min=cat-3&mission=SMAP,SMOS,S1B,S1A&include_cols=all\"\n","#request_url=\"https://cyclobs.ifremer.fr/app/api/getData?mission=S1B,S1A&product_type=swath&include_cols=all\"\n","#request_url=\"https://cyclobs.ifremer.fr/app/api/getData?cat_min=cat-4&cat_max=cat-5&mission=S1B,S1A&product_type=swath&include_cols=all\"\n","request_url=\"https://cyclobs.ifremer.fr/app/api/getData?cat_min=cat-5&mission=S1B,S1A&product_type=swath&include_cols=all\"\n","df_request = pd.read_csv(request_url)\n","\n","# add download path\n","df_request['path'] = df_request['data_url'].map(lambda x : os.path.join(download_path,os.path.basename(x)))\n","#df_request\n","\n","# download 'data_url' to 'path' with wget, and read files\n","projection=ccrs.Mercator()\n","datasets = []\n","for idx,entry in tqdm(df_request.iterrows(),total=df_request.shape[0]):\n","    ret = os.system('cd %s ; wget -N  %s' % (os.path.dirname(entry['path']),entry['data_url']))\n","\n","    if ret == 0 : \n","        ds = xr.open_dataset(entry['path'])\n","        datasets.append(ds)\n","        #datasets.append(ds.rio.reproject(projection.proj4_params))\n","        \n","    else:\n","        datasets.append(None) # error fetching file\n","\n","#print(datasets)\n","df_request['dataset'] = datasets\n","\n","\"\"\"\n","df_request['dataset'].iloc[0]['wind_speed']\n","\n","gv_list=[gf.coastline.opts(projection=projection)]\n","for ds in df_request['dataset']:\n","    print(ds)\n","    gv_list.append(gv.Image(ds['wind_speed'].squeeze()[::5,::5],crs=projection).opts(cmap='jet',tools=['hover']))\n","    \n","gv.Overlay(gv_list).options(width=800, height=500)\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X-bUNkg-Zn2C"},"source":["###1.1. Download data_url, sid and name of each Tropical Cyclone"]},{"cell_type":"code","metadata":{"id":"akpdtj2JZjy6"},"source":["download_path = \"TC_5_try_additional_info\"\n","os.makedirs(download_path, exist_ok=True)\n","\n","request_url = \"https://cyclobs.ifremer.fr/app/api/getData?cat_min=cat-5&mission=SMAP,SMOS,S1B,S1A&include_cols=all\"\n","df_request = pd.read_csv(request_url)\n","\n","# add download path\n","df_request['path'] = df_request['data_url'].map(lambda x : os.path.join(download_path,os.path.basename(x)))\n","TC_CyclObs_information = pandas.DataFrame(data = {'data_url': df_request['data_url'], 'sid': df_request[\"sid\"], 'TC_name': df_request[\"cyclone_name\"]})\n","TC_CyclObs_information[\"data_url\"] = TC_CyclObs_information[\"data_url\"].apply(lambda x: os.path.basename(x))\n","print(TC_CyclObs_information)\n","TC_CyclObs_information.to_csv('{}/Cyclobs_info_names.csv'.format(download_path), index=False, header=True)  \n","\n","# download 'data_url' to 'path' with wget, and read files\n","#projection=ccrs.Mercator()\n","#datasets = []\n","#for idx,entry in tqdm(df_request.iterrows(),total=df_request.shape[0]):\n","#    ret = os.system('cd %s ; wget -N  %s' % (os.path.dirname(entry['path']),entry['data_url']))\n","#    if ret == 0 : \n","#        ds = xr.open_dataset(entry['path'])\n","#        datasets.append( ds)\n","        #datasets.append( ds.rio.reproject(projection.proj4_params))\n","#    else:\n","#        datasets.append(None) # error fetching file"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xEA3rOcemsDT"},"source":["###1.2. [DEPRECATED] Convert the information we need to SAVE_FORMAT to be able to use computer vison algorithms"]},{"cell_type":"code","metadata":{"id":"R1IHtVKX3OJh"},"source":["# convert information relative to SAR mission\n","if MISSION != \"sar\":\n","    sys.exit('The script is only for SAR!')\n","\n","nc_list = glob.glob(\"{}/{}*.nc\".format(CATEGORY, KEY))\n","\n","for nc_image in nc_list:\n","    for FEATURE_TO_SAVE in CHANNELS:\n","        # read nc image\n","        full_info_image = netCDF4.Dataset(nc_image, mode='r') \n","    \n","        # getting the information of the feature you want\n","        try:\n","            feature_extracted = full_info_image.variables[FEATURE_TO_SAVE][:]\n","            tmax_units  = full_info_image.variables['time'].units\n","            feature_extracted = feature_extracted[0]\n","            full_info_image.close()\n","        except:\n","            print(FEATURE_TO_SAVE + \" data not available\")\n","            full_info_image.close()\n","            continue\n","      \n","    # save image in SAVE_FORMAT\n","    if FEATURE_TO_SAVE == \"nrcs_detrend_cross\":\n","        plt.imsave(\"{}/{}_cross.{}\".format(SAVING_DIRECTORY, tmax_units[11:], SAVE_FORMAT), \n","                   feature_extracted, format = SAVE_FORMAT)\n","    else:\n","        plt.imsave(\"{}/{}.{}\".format(SAVING_DIRECTORY, tmax_units[11:], SAVE_FORMAT), \n","                   feature_extracted, format = SAVE_FORMAT)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MATJ8Dtcs1vj"},"source":["# convert information relative to SMOS and SMAP missions, substitute KEY with *smap*.nc or *OPER_MIR*.nc\n","nc_list = glob.glob(\"{}/{}*.nc\".format(CATEGORY, KEY))\n","\n","for nc_image in nc_list:\n","    # read nc image\n","    full_info_image = netCDF4.Dataset(nc_image, mode='r') \n","\n","    # getting the information of the feature you want\n","    feature_extracted = full_info_image.variables[FEATURE_TO_SAVE][:]\n","    tmax_units  = full_info_image.measurement_start_date\n","    full_info_image.close()\n","\n","    # save image in SAVE_FORMAT\n","    plt.imsave(\"{}/{}.{}\".format(SAVING_DIRECTORY, tmax_units[11:], SAVE_FORMAT), \n","               feature_extracted, format = SAVE_FORMAT)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ScgK25TlVd2-"},"source":["##2.Save the features we need into an image (3 channels = 3 features)"]},{"cell_type":"code","metadata":{"id":"lDV-bjPaVaoL"},"source":["with_land_mask = True\n","categories = [\"category1\", \"category2\", \"category3\", \"category4\", \"category5\"]\n","\n","\n","for category in categories:\n","    #category = 'category1'\n","    # directory where .nc files are\n","    saved_dir = \"SAR_swath_nc/{}\".format(category)\n","\n","    # directory where corresponding images will be saved\n","    images_dir = \"SAR_swath_images_VV+VH+WS_land=1/{}\".format(category)\n","    #images_dir = \"SAR_swath_Vmax/{}\".format(category)\n","    #images_dir = \"SAR_swath_masks/{}\".format(category)\n","    os.makedirs(images_dir, exist_ok=True)\n","\n","    #lon_dir = images_dir + \"/lon\"\n","    #os.makedirs(lon_dir, exist_ok=True)\n","    #lat_dir = images_dir + \"/lat\"\n","    #os.makedirs(lat_dir, exist_ok=True)\n","\n","    nc_list = glob.glob(\"{}/*.nc\".format(saved_dir))\n","    count=0\n","\n","    for nc_image in nc_list:\n","        # read nc image\n","        full_info_image = netCDF4.Dataset(nc_image, mode='r') \n","\n","        try:\n","            count += 1\n","            # getting the information of the feature you want\n","            if with_land_mask:\n","                # extract mask flag\n","                mask = full_info_image.variables[\"mask_flag\"][:]\n","                #mask[mask != 0] = 1\n","\n","                # dilate mask\n","                #kernel = np.ones((11, 11), np.int8)\n","                #mask_dilated = cv2.dilate(mask[0], kernel, iterations = 1)\n","                #print(np.unique(mask[0]))\n","                #plt.imshow(mask[0])\n","                #plt.show()\n","\n","            feature_co = full_info_image.variables[\"nrcs_detrend_co\"][:]\n","            if with_land_mask:\n","                # mask out the land values\n","                feature_co[0][mask[0] != 0] = 0\n","                feature_co[0][mask[0] == 1] = 1\n","                #feature_co[0][mask_dilated != 0] = 0\n","            feature_co = (feature_co[0] - np.min(feature_co[0]))/(np.max(feature_co[0]) - np.min(feature_co[0]))\n","            #print(\"feature_co normalised:\", np.max(feature_co), np.min(feature_co))\n","            if count <5:\n","                print(\"feature_co\")\n","                plt.imshow(feature_co)\n","                plt.show()\n","\n","            feature_cross = full_info_image.variables[\"nrcs_detrend_cross\"][:]\n","            if with_land_mask:\n","                # mask out the land values\n","                feature_cross[0][mask[0] != 0] = 0\n","                feature_cross[0][mask[0] == 1] = 1\n","                #feature_cross[0][mask_dilated != 0] = 0 \n","            feature_cross = (feature_cross[0] - np.min(feature_cross[0]))/(np.max(feature_cross[0]) - np.min(feature_cross[0]))\n","            #print(\"feature_cross normalised:\", np.max(feature_cross), np.min(feature_cross))\n","            if count <5:\n","                print(\"feature_cross\")\n","                plt.imshow(feature_cross)\n","                plt.show()\n","\n","            feature_wind = full_info_image.variables[\"wind_speed\"][:]\n","            if with_land_mask:\n","                # mask out the land values\n","                feature_wind[0][mask[0] != 0] = 0\n","                feature_wind[0][mask[0] == 1] = 1\n","                #feature_wind[0][mask_dilated != 0] = 0 \n","            feature_wind = (feature_wind[0] - np.min(feature_wind[0]))/(np.max(feature_wind[0]) - np.min(feature_wind[0]))\n","            #print(\"feature_wind normalised:\", np.max(feature_wind), np.min(feature_wind))\n","            if count <5:\n","                print(\"feature_wind\")\n","                plt.imshow(feature_wind)\n","                plt.show()\n","\n","            feature_wso = full_info_image.variables[\"wind_streaks_orientation\"][:]\n","            #print(np.nanmax(feature_wso[0]), np.nanmin(feature_wso[0]))\n","            #if np.isnan(feature_wso).any():\n","            #    count +=1\n","            #print(\"feature_wso\")\n","            #plt.imshow(feature_wso[0])\n","            #plt.show()\n","            #feature_wso = np.nan_to_num(feature_wso[0])\n","            #sin_feature_wso = np.sin(feature_wso * np.pi/180.)   # values from -1 to 1\n","            #sin_feature_wso = (sin_feature_wso - np.min(sin_feature_wso))/(np.max(sin_feature_wso) - np.min(sin_feature_wso))\n","            #cos_feature_wso = np.cos(feature_wso * np.pi/180.)   # values from -1 to 1\n","            #cos_feature_wso = (cos_feature_wso - np.min(cos_feature_wso))/(np.max(cos_feature_wso) - np.min(cos_feature_wso))\n","\n","            # extract longitude and latitude features\n","            feature_lon = full_info_image.variables[\"lon\"][:]\n","            feature_lat = full_info_image.variables[\"lat\"][:]\n","\n","            # extract time registered\n","            tmax_units = datetime.datetime.strptime(full_info_image.measurementDate, '%Y-%m-%dT%H:%M:%SZ')\n","            tmax_units = tmax_units.strftime('%Y-%m-%d %H_%M_%S')\n","\n","            # stack matrices along 3rd axis (depth-wise)\n","            output_image = np.dstack((feature_co, feature_cross, feature_wind))\n","            #output_image = np.dstack((feature_wind[0], np.sin(feature_wso[0] * np.pi/180.), np.cos(feature_wso[0] * np.pi/180.)))\n","\n","            if (output_image < 0).any():\n","                output_image = np.clip(output_image, a_min=0, a_max=None)\n","            #print(output_image.shape)\n","            #print(\"output_image:\", np.max(output_image), np.min(output_image))\n","            #print(\"output:\", np.nanmax(output_image), np.nanmin(output_image))\n","            #print(\"first output image\")\n","            if count <5:\n","                print(\"output image\")\n","                plt.imshow(output_image)\n","                plt.show()\n","\n","            full_info_image.close()\n","        except KeyError as err:\n","            # creating KeyError instance for book keeping\n","            print(\"Error:\", err)\n","            full_info_image.close()\n","            continue\n","        \n","        # [DEPRECATED] mask out the land values\n","        #output_image[mask[0] != 0] = 0\n","        #print(\"output image masked\")\n","        #print(\"after masking:\", np.max(output_image), np.min(output_image))\n","        #plt.imshow(output_image)\n","        #plt.show()\n","\n","        # [DEPRECATED] normalise values between [0 ... 1]\n","        #norm_image = output_image/np.nanmax(output_image)\n","        #print(\"after normalising:\", np.max(norm_image), np.min(norm_image))\n","        #print(\"output image normalised\")\n","        #plt.imshow(norm_image)\n","        #plt.show()\n","\n","        # save information extracted\n","        #cv2.imwrite(\"{}/{}.png\".format(images_dir, tmax_units), output_image)\n","        plt.imsave(\"{}/{}.png\".format(images_dir, tmax_units), output_image, format = 'png')\n","        #np.save(\"{}/{}_mask.npy\".format(images_dir, tmax_units), np.array(mask[0]))\n","        #np.save(\"{}/{}_Vmax.npy\".format(images_dir, tmax_units), np.max(feature_wind[0]))\n","        #np.save(\"{}/{}.npy\".format(lon_dir, tmax_units), np.mean(feature_lon[0]))\n","        #np.save(\"{}/{}.npy\".format(lat_dir, tmax_units), np.mean(feature_lat[0]))\n","    #print(count)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QzJBBp6HZdAh"},"source":["###2.1. Test features"]},{"cell_type":"code","metadata":{"id":"CbXExCrt1vqq"},"source":["# directory where nc files are\n","saved_dir = \"SAR_swath_nc/category1\"\n","\n","# directory where corresponding images will be saved\n","#images_dir = \"SAR_swath_images/category1\"\n","#images_dir = \"SAR_swath_masks/category5\"\n","#os.makedirs(images_dir, exist_ok=True)\n","#lon_dir = images_dir + \"/lon\"\n","#os.makedirs(lon_dir, exist_ok=True)\n","#lat_dir = images_dir + \"/lat\"\n","#os.makedirs(lat_dir, exist_ok=True)\n","test_dir = 'test'\n","os.makedirs(test_dir, exist_ok=True)\n","\n","nc_list = glob.glob(\"{}/*.nc\".format(saved_dir))\n","count=0\n","\n","for nc_image in nc_list[:5]:\n","    # read nc image\n","    full_info_image = netCDF4.Dataset(nc_image, mode='r') \n","\n","    # getting the information of the feature you want\n","    try:\n","        # extract mask flag\n","        mask = full_info_image.variables[\"mask_flag\"][:]\n","\n","        feature_co = full_info_image.variables[\"nrcs_detrend_co\"][:]\n","        #print(\"feature_co: \", np.max(feature_co[0]), np.min(feature_co[0]))\n","        #plt.imshow(feature_co[0])\n","        #plt.show()\n","        test = feature_co[0]\n","        test[mask[0] != 0] = 0\n","        feature_co[0][mask[0] != 0] = 0\n","        print(\"feature_co masked:\", np.max(feature_co[0]), np.min(feature_co[0]))\n","        print(\"test masked:\", np.max(test), np.min(test))\n","\n","        feature_co = (feature_co[0] - np.min(feature_co[0]))/(np.max(feature_co[0]) - np.min(feature_co[0]))\n","        print(\"feature_co normalised:\", np.max(feature_co), np.min(feature_co))\n","        #feature_co[mask[0] != 0] = 0\n","        #print(\"feature_co masked:\", np.max(feature_co), np.min(feature_co))\n","        plt.imshow(feature_co)\n","        plt.show()\n","    \n","  \n","        feature_cross = full_info_image.variables[\"nrcs_detrend_cross\"][:]\n","        #print(\"feature_cross:\", np.max(feature_cross[0]), np.min(feature_cross[0]))\n","        #plt.imshow(feature_cross[0])\n","        #plt.show()\n","\n","        feature_cross = (feature_cross[0] - np.min(feature_cross[0]))/(np.max(feature_cross[0]) - np.min(feature_cross[0]))\n","        print(\"feature_cross normalised:\", np.max(feature_cross), np.min(feature_cross))\n","        feature_cross[mask[0] != 0] = 0\n","        print(\"feature_cross masked:\", np.max(feature_cross), np.min(feature_cross))\n","        plt.imshow(feature_cross)\n","        plt.show()\n","\n","\n","        feature_wind = full_info_image.variables[\"wind_speed\"][:]\n","        #print(\"feature_wind:\", np.max(feature_wind[0]), np.min(feature_wind[0]))\n","        #plt.imshow(feature_wind[0])\n","        #plt.show()\n","\n","\n","        feature_wind = (feature_wind[0] - np.min(feature_wind[0]))/(np.max(feature_wind[0]) - np.min(feature_wind[0]))\n","        print(\"feature_wind normalised:\", np.max(feature_wind), np.min(feature_wind))\n","        feature_wind[mask[0] != 0] = 0\n","        print(\"feature_wind masked:\", np.max(feature_wind), np.min(feature_wind))\n","        plt.imshow(feature_wind)\n","        plt.show()\n","\n","\n","        feature_wso = full_info_image.variables[\"wind_streaks_orientation\"][:]\n","        #if np.isnan(feature_wso).any():\n","        #  count +=1\n","        #print(np.nanmax(feature_wso[0]), np.nanmin(feature_wso[0]))\n","        #print(\"feature_wso\")\n","        #plt.imshow(feature_wso[0])\n","        #plt.show()\n","\n","        feature_lon = full_info_image.variables[\"lon\"][:]\n","        feature_lat = full_info_image.variables[\"lat\"][:]\n","\n","        tmax_units = datetime.datetime.strptime(full_info_image.measurementDate, '%Y-%m-%dT%H:%M:%SZ')\n","        tmax_units = tmax_units.strftime('%Y-%m-%d %H_%M_%S')\n","\n","        # stack matrices along 3rd axis (depth-wise)\n","        output_image = np.dstack((feature_co, feature_cross, feature_wind))\n","        if (output_image < 0).any():\n","            output_image = np.clip(output_image, a_min=0, a_max=None)\n","        #print(output_image.shape)\n","        print(\"output:\", np.max(output_image), np.min(output_image))\n","        #print(\"first output image\")\n","        #plt.imshow(output_image)\n","        #plt.show()\n","\n","        full_info_image.close()\n","\n","    except KeyError as err:\n","        # creating KeyError instance for book keeping\n","        print(\"Error:\", err)\n","        full_info_image.close()\n","        continue\n","    \n","    # mask out the land values\n","    #output_image[mask[0] != 0] = 0\n","    #print(\"output image masked\")\n","    #print(\"after masking:\", np.max(output_image), np.min(output_image))\n","    #plt.imshow(output_image)\n","    #plt.show()\n","\n","    # normalise values between [0 ... 1]\n","    #norm_image = output_image/np.nanmax(output_image)\n","    #print(\"after normalising:\", np.max(norm_image), np.min(norm_image))\n","    #print(\"output image normalised\")\n","    #plt.imshow(norm_image)\n","    #plt.show()\n","\n","    # save information extracted\n","    #cv2.imwrite(\"{}/{}.png\".format(images_dir, tmax_units), output_image)\n","    plt.imsave(\"{}/{}.png\".format(test_dir, tmax_units), output_image, format = 'png')\n","    #plt.imsave(\"{}/{}_mask.png\".format(images_dir, tmax_units), mask, format = 'png')\n","    #np.save(\"{}/{}_mask.npy\".format(images_dir, tmax_units), np.array(mask[0]))\n","    #np.save(\"{}/{}.npy\".format(lon_dir, tmax_units), np.mean(feature_lon[0]))\n","    #np.save(\"{}/{}.npy\".format(lat_dir, tmax_units), np.mean(feature_lat[0]))\n","\n","    print(\"--------------------- Test -----------------------\")\n","    a = cv2.imread(\"{}/{}.png\".format(test_dir, tmax_units))\n","    #print(\"cv2_imshow\")\n","    #cv2_imshow(a)\n","    print(\"plt.imshow: all\")\n","    plt.imshow(a)\n","    plt.show()\n","\n","    #print(\"feature_co\")\n","    #print(\"cv2_imshow\")\n","    #cv2_imshow(a[:,:,0])\n","\n","    print(\"plt.imshow\")\n","    plt.imshow(a[:,:,0])\n","    plt.show()\n","    plt.imshow(a[:,:,1])\n","    plt.show()\n","    plt.imshow(a[:,:,2])\n","    plt.show()\n","\n","    print(np.min(a), np.max(a))\n","\n","#print(count)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pt8Beq3siRKl"},"source":["a = cv2.imread(\"SAR_swath_images_VV+VH+WS/category1/2016-09-01 20_30_16.png\")\n","print(\"cv2_imshow\")\n","cv2_imshow(a)\n","print(\"plt.imshow\")\n","plt.imshow(a)\n","plt.show()\n","\n","print(\"feature_co\")\n","print(\"cv2_imshow\")\n","cv2_imshow(a[:,:,0])\n","\n","print(\"plt.imshow\")\n","plt.imshow(a[:,:,0])\n","plt.show()\n","\n","print(np.min(a), np.max(a))\n","b = a/np.max(a)\n","print(np.min(b), np.max(b))\n","print(\"cv2_imshow\")\n","cv2_imshow(b)\n","print(\"plt.imshow\")\n","plt.imshow(b)\n","plt.show()\n","\n","#c = cv2.imread(\"SAR_swath_images_VV+VH+VH/category1/2016-09-15 14_12_58.png\")\n","#cv2_imshow(c)\n","#print(np.min(c), np.max(c))\n","#d = c/np.max(c)\n","#cv2_imshow(d)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NN7E1KlE50hf"},"source":["a = cv2.imread(\"SAR_swath_images/category1/2016-09-01 20_30_16.png\")\n","print(\"cv2_imshow\")\n","cv2_imshow(a)\n","print(\"plt.imshow\")\n","plt.imshow(a)\n","plt.show()\n","\n","print(\"feature_co\")\n","print(\"cv2_imshow\")\n","cv2_imshow(a[:,:,0])\n","\n","print(\"plt.imshow\")\n","plt.imshow(a[:,:,0])\n","plt.show()\n","plt.imshow(a[:,:,1])\n","plt.show()\n","plt.imshow(a[:,:,2])\n","plt.show()\n","\n","print(np.min(a), np.max(a))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RqsH9xIIqEen"},"source":["a = cv2.imread(\"SAR_swath_images_VV+VH+WS/category1/2016-09-01 20_30_16.png\")\n","print(\"cv2_imshow\")\n","cv2_imshow(a)\n","print(\"plt.imshow\")\n","plt.imshow(a)\n","plt.show()\n","\n","print(\"feature_co\")\n","print(\"cv2_imshow\")\n","cv2_imshow(a[:,:,0])\n","\n","print(\"plt.imshow\")\n","plt.imshow(a[:,:,0])\n","plt.show()\n","plt.imshow(a[:,:,1])\n","plt.show()\n","plt.imshow(a[:,:,2])\n","plt.show()\n","\n","print(np.min(a), np.max(a))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XU6C1ag2qVI2"},"source":["a = cv2.imread(\"SAR_swath_images_VV+VH+VH/category1/2016-09-01 20_30_16.png\")\n","print(\"cv2_imshow\")\n","cv2_imshow(a)\n","print(\"plt.imshow\")\n","plt.imshow(a)\n","plt.show()\n","\n","print(\"feature_co\")\n","print(\"cv2_imshow\")\n","cv2_imshow(a[:,:,0])\n","\n","print(\"plt.imshow\")\n","plt.imshow(a[:,:,0])\n","plt.show()\n","plt.imshow(a[:,:,1])\n","plt.show()\n","plt.imshow(a[:,:,2])\n","plt.show()\n","\n","print(np.min(a), np.max(a))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EJcOBpUVqsIo"},"source":["a = cv2.imread(\"SAR_swath_images_WS+WS+WS/category1/2016-09-01 20_30_16.png\")\n","print(\"cv2_imshow\")\n","cv2_imshow(a)\n","print(\"plt.imshow\")\n","plt.imshow(a)\n","plt.show()\n","\n","print(\"feature_co\")\n","print(\"cv2_imshow\")\n","cv2_imshow(a[:,:,1])\n","\n","print(\"plt.imshow\")\n","plt.imshow(a[:,:,0])\n","plt.show()\n","plt.imshow(a[:,:,1])\n","plt.show()\n","plt.imshow(a[:,:,2])\n","plt.show()\n","\n","print(np.min(a), np.max(a))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zImvZtOtqz2A"},"source":["a = cv2.imread(\"SAR_swath_images_WS+sWSO+cWSO/category1/2016-09-01 20_30_16.png\")\n","print(\"cv2_imshow\")\n","cv2_imshow(a)\n","print(\"plt.imshow\")\n","plt.imshow(a)\n","plt.show()\n","\n","print(\"feature_co\")\n","print(\"cv2_imshow\")\n","cv2_imshow(a[:,:,0])\n","\n","print(\"plt.imshow\")\n","plt.imshow(a[:,:,0])\n","plt.show()\n","plt.imshow(a[:,:,1])\n","plt.show()\n","plt.imshow(a[:,:,2])\n","plt.show()\n","\n","print(np.min(a), np.max(a))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xRfjWIAmi3cK"},"source":["###2.2. Delete doubled files"]},{"cell_type":"code","metadata":{"id":"982q-y86fr4z"},"source":["folder_to_delete = \"/content/drive/My Drive/ESRIN_PhiLab/Tropical_Cyclones/data/SAR_swath_images_VV+VH+WS_land=1/category2\"\n","files_to_delete = glob.glob(folder_to_delete + '/*.png')\n","\n","source_folder = \"/content/drive/My Drive/ESRIN_PhiLab/Tropical_Cyclones/data/SAR_swath_images_VV+VH+WS_land=1/category1\"\n","source_files = glob.glob(source_folder + '/*.png')\n","\n","for f in files_to_delete:\n","    f = f.split('/')[-1]\n","    for s in source_files:\n","        s = s.split('/')[-1]\n","        if f == s:\n","          print('File removed:', s)\n","          os.remove(source_folder + '/' + s)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fEHpWS_ztZdb"},"source":["source_folder = \"/content/drive/My Drive/ESRIN_PhiLab/Tropical_Cyclones/data/SAR_swath_images_VV+VH+WS_land=1/category5\"\n","source_files = glob.glob(source_folder + '/*.png')\n","len(source_files)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ygyOuX_Abug0"},"source":["##3.Split data between train, validation and test csv files"]},{"cell_type":"code","metadata":{"id":"VDlmC0MOCITr"},"source":["LABELS_PATH = \"/content/drive/My Drive/ESRIN_PhiLab/Tropical_Cyclones/data/labels/via_project_11Jan2021_12h22m_csv.csv\"\n","#labels_path_file = glob.glob('{}/*.csv'.format(LABELS_PATH))\n","## in case there is no txt file for that image\n","#if labels_path_file == []:\n","#  print(\"No labels file found.\")\n","#  sys.exit(0)\n","\n","#df = pd.read_csv(labels_path_file[0])\n","df = pd.read_csv(LABELS_PATH)\n","cols = df.columns.tolist()\n","for idx, row in df.iterrows():\n","    # retrieve the coordinates of the bounding boxes\n","    df['region_shape_attributes'][idx] = re.findall(r'\\d+', df['region_shape_attributes'][idx])\n","\n","images_dir = \"SAR_swath_images_VV+VH+WS_land=1/\"\n","category = [\"category1/\", \"category2/\", \"category3/\", \"category4/\", \"category5/\"]\n","\n","# use a seed so to obtain the same splitting every time\n","random.seed(20)\n","\n","df_train = [] \n","df_val = [] \n","df_test = [] \n","df_full = []\n","\n","for cat in category:\n","    dir = images_dir + cat\n","    lon_dir = dir + \"lon\"\n","    lat_dir = dir + \"lat\"\n","\n","    image_list = glob.glob('{}/*.png'.format(dir))\n","\n","    for image_path in image_list:\n","        #print(image_path)\n","        image_name = os.path.basename(image_path)\n","\n","        lon_list = glob.glob('{}/{}.npy'.format(lon_dir, image_name.split('.')[0]))\n","        lat_list = glob.glob('{}/{}.npy'.format(lat_dir, image_name.split('.')[0]))\n","\n","        lon = lon_list[0] if lon_list != [] else None\n","        lat = lat_list[0] if lat_list != [] else None\n","\n","        aux = [image_path]\n","        aux.extend([lat, lon])\n","        #print(df[df.filename == image_name])\n","        index = df[df.filename == image_name].index[0]\n","        bb_present = 1 if df['region_count'][index] == 1 else 0\n","        aux.append(bb_present)\n","        aux.append(df['region_shape_attributes'][index])\n","\n","        df_full.append(aux)\n","        rand = random.randint(0, 100)\n","        # split used 60, 20, 20\n","        if rand < 60:\n","            df_train.append(aux)\n","        elif rand < 80:\n","            df_val.append(aux)\n","        else:\n","            df_test.append(aux) \n","\n","col_names = ['image', 'lat', 'lon', 'label', 'bbox_shape']\n","df_train = pd.DataFrame(df_train, columns = col_names)\n","df_val = pd.DataFrame(df_val, columns = col_names)\n","df_test = pd.DataFrame(df_test, columns = col_names)\n","df_full = pd.DataFrame(df_full, columns = col_names)\n","\n","if (df_full['lat'].values == None).all() and (df_full['lon'].values == None).all():\n","    print(\"lat and lon variables not considered.\")\n","    df_full.drop(columns=['lat', 'lon'], inplace=True)\n","\n","# create path to save the csv files\n","CSV_PATH = images_dir + 'csv'\n","os.makedirs(CSV_PATH, exist_ok=True)\n","\n","if df_train.empty == False:\n","    df_train.to_csv('{}/training.csv'.format(CSV_PATH), index = False)\n","\n","if df_val.empty == False:\n","    df_val.to_csv('{}/val.csv'.format(CSV_PATH), index = False)\n","\n","if df_test.empty == False:\n","    df_test.to_csv('{}/test.csv'.format(CSV_PATH), index = False)\n","\n","if df_full.empty == False:\n","    df_full.to_csv('{}/full_dataset.csv'.format(CSV_PATH), index = False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zSyR90FIb3tY"},"source":["##4.Test the location of the bounding boxes around the eye"]},{"cell_type":"code","metadata":{"id":"6mWfx5ZVHTYm"},"source":["import matplotlib\n","df = pd.read_csv('SAR_swath_images_VV+VH+WS_land=1/csv/full_dataset.csv', converters={'bbox_shape': eval})\n","\n","for idx, row in df.iterrows():\n","    print(\"Image:\", df['image'][idx], \"label:\", df['label'][idx])\n","    img = cv2.imread(df['image'][idx])\n","\n","    b = img/np.max(img)\n","    fig, ax = plt.subplots(figsize=(5,8))\n","    plt.imshow(b)\n","\n","    if df['bbox_shape'][idx] != []:\n","        bbox = df['bbox_shape'][idx]\n","\n","        # read dimensions of bbox_eye\n","        cX = (int)(bbox[0])\n","        cY = (int)(bbox[1])\n","        bb_width = (int)(bbox[2])\n","        bb_height = (int)(bbox[3])\n","\n","        ax.add_patch(matplotlib.patches.Rectangle((cX, cY), bb_width, bb_height, color ='green', fc='none'))\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MniuThrHiD6s"},"source":["##5.Link .nc files to ARCHER products"]},{"cell_type":"code","metadata":{"id":"yXptg6vj-T5P"},"source":["main_dir = \"Link_ANCHER_IFREMER\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uf9dcHNYbpGi"},"source":["###5.1. Read all txt files from ARCHER and convert them to csv files"]},{"cell_type":"code","metadata":{"id":"kB4u7if4n4eV"},"source":["# directory where ARCHER products are stored\n","archer_dir = main_dir + \"/ARCHER_products\"\n","folders = glob.glob(\"{}/*\".format(archer_dir))\n","cnt = 0\n","\n","for folder in folders:\n","    print(folder)\n","    files = glob.glob(\"{}/*\".format(folder))\n","    file = \"{}/archerTrackTable.txt\".format(folder)\n","\n","    if file in files:\n","        cnt+=1\n","        try:\n","            # read .txt file as pandas df\n","            cols = [\"Date/Time (UTC)\", \"Source Sensor\", \"Vmax (kts)\", \"t_off (hrs)\", \"Geo-ref Lat\",\n","                    \"Lon\", \"Opr Ctr Lat\", \"Lon2\", \"50% cert rad\", \"95% cert rad\", \"Eye diam (deg)\", \"% cert eye\", \"50% cert fxa\"]\n","            df = pd.read_csv(file, sep=\"\\s{2,}\", engine='python', skiprows=1, names = cols, index_col=False)\n","            df[\"Date/Time (UTC)\"] = pd.to_datetime(df[\"Date/Time (UTC)\"], format='%Y-%m-%d %H:%M:%S')\n","            df.to_csv(\"{}/archerTrackTable.csv\".format(folder), index=False)\n","\n","        except ValueError as err:\n","            print(\"Error:\", err)\n","            file = \"{}/summaryTable.txt\".format(folder)\n","            if file in files:\n","                try:\n","                    # read .txt file as pandas df\n","                    cols = [\"Date/Time (UTC)\", \"Source Sensor\", \"Vmax (kts)\", \"ARCHER Lat\", \"Lon\", \"Geo-ref Lat\",\n","                            \"Lon2\", \"50% cert rad\", \"95% cert rad\", \"Eye diam (deg)\", \"% cert eye\", \"50% cert fxa\"]\n","                    df = pd.read_csv(file, sep=\"\\s{2,}\", engine='python', skiprows=1, names = cols, index_col=False)\n","                    df[\"Date/Time (UTC)\"] = df[\"Date/Time (UTC)\"].apply(lambda date: date if \" *\" not in date else date.replace(\" *\", \"\"))\n","                    df[\"Date/Time (UTC)\"] = pd.to_datetime(df[\"Date/Time (UTC)\"], format='%Y-%m-%d %H:%M:%S')\n","                    df.to_csv(\"{}/summaryTable.csv\".format(folder), index=False)\n","                \n","                except ValueError as err:\n","                    print(\"Second Error:\", err)\n","                    continue\n","print(cnt)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yn0ED6AEbyL-"},"source":["###5.2. Link each .nc file to an ARCHER csv file"]},{"cell_type":"markdown","metadata":{"id":"KURETo41rBMG"},"source":["####5.2.1. Gather info from .nc files"]},{"cell_type":"code","metadata":{"id":"JVu0FRlQiGsc"},"source":["categories = [\"category1\", \"category2\", \"category3\", \"category4\", \"category5\"]\n","#category = 'category4'\n","\n","nc_files = []\n","date_time = []\n","extreme_coords = []\n","for category in categories:\n","    print(category)\n","    # directory where .nc files are\n","    saved_dir = \"SAR_swath_nc/{}\".format(category)\n","\n","    # directory where corresponding images will be saved\n","    images_dir = \"SAR_swath_images_VV+VH+WS_dilated/{}\".format(category)\n","\n","    if category == 'category5':\n","        # get .nc files\n","        nc_list = glob.glob(\"{}/*.nc\".format(saved_dir))\n","    else:\n","        cat = int(re.findall(\"\\d\", category)[0])\n","        categories_higher = [i for i in range(cat+1, 6)]\n","        nc_list_higher_all = []\n","        for cat_h in categories_higher:\n","            nc_list_higher_all.extend(glob.glob('SAR_swath_nc/category{}/*.nc'.format(cat_h)))\n","\n","        nc_list_higher = [os.path.basename(x) for x in nc_list_higher_all]\n","        nc_list = [file for file in glob.glob(\"{}/*.nc\".format(saved_dir)) if os.path.basename(file) not in nc_list_higher]\n","        \n","    cnt = 0\n","    for nc_image in nc_list:\n","        # read nc image\n","        full_info_image = netCDF4.Dataset(nc_image, mode='r') \n","\n","        # getting the information of the feature you want\n","        try:\n","            # try to read this feature as it usually gives problems\n","            feature_cross = full_info_image.variables[\"nrcs_detrend_cross\"][:]\n","\n","            # extract time registered\n","            tmax_units = datetime.datetime.strptime(full_info_image.measurementDate, '%Y-%m-%dT%H:%M:%SZ')\n","            #tmax_units = tmax_units.strftime('%Y-%m-%d %H_%M_%S')\n","\n","            # extract longitude and latitude features\n","            feature_lon = full_info_image.variables[\"lon\"][:]\n","            #print(\"feature_lon:\", np.max(feature_lon[0]), np.min(feature_lon[0]))\n","            feature_lat = full_info_image.variables[\"lat\"][:]\n","            #print(\"feature_lat:\", np.max(feature_lat[0]), np.min(feature_lat[0]))\n","\n","            coords = [np.min(feature_lat[0]), np.max(feature_lat[0]), np.min(feature_lon[0]), np.max(feature_lon[0])]\n","\n","            nc_files.append(nc_image)\n","            date_time.append(tmax_units)\n","            extreme_coords.append(coords)\n","\n","            full_info_image.close()\n","        except KeyError as err:\n","            # creating KeyError instance for book keeping\n","            print(\"Error:\", err)\n","            full_info_image.close()\n","            continue\n","        cnt +=1\n","    print(cnt)\n","\n","df = pd.DataFrame({\"nc_files\": nc_files, \"date_time\": date_time, \"extreme_coords\": extreme_coords})\n","df[\"image\"] = df[\"date_time\"].apply(lambda i: i.strftime('%Y-%m-%d %H_%M_%S'+'.png'))\n","df[\"ARCHER product\"] = np.empty((len(df), 0)).tolist()\n","df.to_csv(main_dir + \"/IFREMER_info.csv\", index=False)\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X0KbhnQeegjE"},"source":["df.dtypes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s0D33UPLrHIt"},"source":["####5.2.2. Link .nc files to ARCHER"]},{"cell_type":"code","metadata":{"id":"sTUfJEJj6Rcv"},"source":["df = pd.read_csv(main_dir + \"/IFREMER_info.csv\", converters={'ARCHER product': eval, 'extreme_coords': eval})\n","df[\"date_time\"] = pd.to_datetime(df[\"date_time\"])\n","\n","# directory where ARCHER products are stored\n","archer_dir = main_dir + \"/ARCHER_products\"\n","folders = glob.glob(\"{}/*\".format(archer_dir))\n","cnt = 0\n","\n","def link_ARCHER_IFREMER(x, EndDate, StartDate, lat_arr, lon_arr):\n","    lat_ok = False\n","    lon_ok = False\n","    x['extreme_coords'] = np.array(x['extreme_coords'], dtype=np.float)\n","    print(\"StartDate:\", StartDate, \", EndDate:\", EndDate, \", SAR datetime:\", x['date_time'])\n","\n","\n","    if StartDate <= x['date_time'] <= EndDate:\n","        print(\"Inside datetime!\")\n","        print(x['extreme_coords'])\n","\n","        print(lat_arr)\n","        if any((x['extreme_coords'][0] <= lat_arr) & (lat_arr <= x['extreme_coords'][1])):\n","            print(\"LAT OK!\")\n","            lat_ok = True\n","        #for item in lat_arr:\n","        #    print(type(item), type(x['extreme_coords'][0]), type(x['extreme_coords'][1]))\n","        #    if x['extreme_coords'][0] <= item <= x['extreme_coords'][1]:\n","        #        print(\"LAT OK! Value: {}\".format(item))\n","        #        lat_ok = True\n","        #        break\n","\n","        print(lon_arr)\n","        if any((x['extreme_coords'][2] <= lon_arr) & (lon_arr <= x['extreme_coords'][3])):\n","            print(\"LON OK!\")\n","            lon_ok = True\n","        #for item in lon_arr:\n","        #    if x['extreme_coords'][2] <= item <= x['extreme_coords'][3]:\n","        #        print(\"LON OK! Value: {}\".format(item))\n","        #        lon_ok = True\n","        #        break\n","\n","        if lat_ok and lon_ok:\n","            #sys.exit()\n","            return True\n","        else:\n","            return False\n","    else:\n","        return False\n","\n","for folder in folders:\n","    cnt +=1\n","    print(folder)\n","    csv_files = glob.glob(\"{}/*.csv\".format(folder))\n","\n","    if len(csv_files) > 1:\n","        csv_files = [s for s in csv_files if \"new\" in s]\n","\n","    for csv_file in csv_files:\n","\n","        archer_df = pd.read_csv(csv_file, header=0)\n","        archer_df[\"Date/Time (UTC)\"] = pd.to_datetime(archer_df[\"Date/Time (UTC)\"])\n","        archer_df = archer_df.dropna()\n","\n","        StartDate = np.min(archer_df[\"Date/Time (UTC)\"])\n","        EndDate = np.max(archer_df[\"Date/Time (UTC)\"])\n","        if (EndDate - StartDate ) / np.timedelta64(1, 'D') > 30:    # time difference of more than 30 days\n","            print(folder, \"- StartDate:\", StartDate, \"EndDate:\", EndDate)\n","        \n","        inTime_df = df[(df.date_time >= StartDate) & (df.date_time <= EndDate)]\n","\n","        if not inTime_df.empty:\n","            lat = archer_df[\"Geo-ref Lat\"].dropna().to_numpy()\n","            lat_arr = np.delete(lat, np.argwhere(lat == \"***\"))\n","            lat_arr = lat_arr.astype(np.float)\n","\n","            lon_col = \"Lon\" if \"archerTrackTable\" in os.path.basename(csv_file) else \"Lon2\"\n","            lon = archer_df[lon_col].dropna().to_numpy()\n","            lon_arr = np.delete(lon, np.argwhere(lon == \"***\"))\n","            lon_arr = lon_arr.astype(np.float)\n","\n","            #df[\"ARCHER product\"] = df[\"ARCHER product\"].apply(lambda i: i if not (df.date_time < EndDate & df.date_time > StartDate) else i.append(folder)\n","            df.apply(lambda x: x[\"ARCHER product\"].append(csv_file) if link_ARCHER_IFREMER(x, EndDate, StartDate, lat_arr, lon_arr) else x[\"ARCHER product\"], axis = 1)\n","    \n","df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bRhBx1BkYRCV"},"source":["df.to_csv(main_dir + \"/ARCHER_IFREMER_link_by_date_and_coords.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dok9QDdTY_tX"},"source":["####5.2.3. Deal with problematic ARCHER products"]},{"cell_type":"code","metadata":{"id":"3ywGnPNELsU9"},"source":["# folders with time_diffs between StartDate and EndDate higher than 30 days (problematic)\n","# Link_ANCHER_IFREMER/ARCHER_products/2016_30W - StartDate: 2016-12-21 05:30:00 EndDate: 2017-11-15 11:30:00\n","# Link_ANCHER_IFREMER/ARCHER_products/2018_03S - StartDate: 2018-01-02 17:30:00 EndDate: 2018-11-11 17:30:00\n","# Link_ANCHER_IFREMER/ARCHER_products/2018_09P - StartDate: 2018-02-13 06:00:00 EndDate: 2019-01-07 21:00:00\n","# Link_ANCHER_IFREMER/ARCHER_products/2020_07S - StartDate: 2020-01-11 11:30:00 EndDate: 2020-12-30 17:30:00\n","# Link_ANCHER_IFREMER/ARCHER_products/2020_10S - StartDate: 2020-01-26 17:30:00 EndDate: 2021-01-19 11:30:00\n","\n","folder = \"Link_ANCHER_IFREMER/ARCHER_products/2020_10S\"\n","csv_files = glob.glob(\"{}/*.csv\".format(folder))\n","\n","for csv_file in csv_files:\n","\n","    archer_df = pd.read_csv(csv_file, header=0)\n","    archer_df[\"Date/Time (UTC)\"] = pd.to_datetime(archer_df[\"Date/Time (UTC)\"])\n","    archer_df = archer_df.sort_values(by=[\"Date/Time (UTC)\"], ignore_index=True).dropna()\n","\n","    # check time differences between consecutive obervations\n","    time_diffs = archer_df['Date/Time (UTC)'].diff().dropna()\n","    # get index for when time diff is above 2 days\n","    index = np.where(time_diffs / np.timedelta64(1, 'D') > 5)[0][0]\n","\n","    df1 = archer_df.iloc[:index+1]\n","    df2 = archer_df.iloc[index+1:]\n","    print(df1)\n","    print(df2)\n","\n","    input(\"Press Enter to continue...\")\n","\n","    df1.to_csv(\"{}/{}_new1.csv\".format(folder, os.path.basename(csv_file)[:-4]), index=False)\n","    df2.to_csv(\"{}/{}_new2.csv\".format(folder, os.path.basename(csv_file)[:-4]), index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FkQ99UuaeH01"},"source":["###5.3. Test location of eye according to ARCHER and best track"]},{"cell_type":"code","metadata":{"id":"Y4_T7Bbd6aYt"},"source":["def find_nearest(array, value):\n","    #array = np.asarray(array)\n","    X = np.abs(array - value)\n","    idx = np.where(X == X.min())\n","    print(\"Index:\", idx)\n","    #print(\"Value wanted: {} and closest value in X: {}\".format(value, array[idx[0], idx[1]]))\n","    #return array[idx[0], idx[1]], idx\n","    flat_index = np.argmin(np.abs(array - value))\n","    alt_idx = np.unravel_index(flat_index, array.shape)\n","    print(\"Alternatively:\", alt_idx)\n","    print(\"Value wanted: {} and closest value in X: {}\".format(value, array[alt_idx[0], alt_idx[1]]))\n","    return array[alt_idx[0], alt_idx[1]], alt_idx"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XiNnvIdpHD06"},"source":["request_url = \"https://cyclobs.ifremer.fr/app/api/getData?cat_min=cat-1&cat_max=cat-5&mission=S1B,S1A&product_type=swath&include_cols=all\"\n","df_request = pd.read_csv(request_url)\n","\n","nc_CyclObs_info = pd.DataFrame(data = {'data_url': df_request['data_url'], 'sid': df_request[\"sid\"], 'TC_name': df_request[\"cyclone_name\"]})\n","nc_CyclObs_info[\"data_url\"] = nc_CyclObs_info[\"data_url\"].apply(lambda x: os.path.basename(x))\n","nc_CyclObs_info"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"05-V8N05Dmdk"},"source":["df = pd.read_csv(main_dir + \"/ARCHER_IFREMER_link_by_date_and_coords.csv\", converters={'ARCHER product': eval})\n","df[\"date_time\"] = pd.to_datetime(df[\"date_time\"])\n","\n","best_track_df = pd.read_csv(\"best_track/ibtracs.since1980.list.v04r00.csv\", header=0, skiprows=range(1, 2))\n","best_track_df[\"ISO_TIME\"] = pd.to_datetime(best_track_df[\"ISO_TIME\"])\n","best_track_df['ISO_TIME_str'] = best_track_df['ISO_TIME'].apply(lambda i: datetime.datetime.strftime(i, \"%Y-%m-%d\"))\n","\n","time_diffs = []\n","\n","cnt = 0\n","for index, row in df.iterrows():\n","    print(row)\n","    cnt+=1\n","\n","    full_info_image = netCDF4.Dataset(row[\"nc_files\"], mode='r') \n","\n","    mask = full_info_image.variables[\"mask_flag\"][:]\n","    mask[mask != 0] = 1\n","    # dilate mask\n","    kernel = np.ones((11, 11), np.int8)\n","    mask_dilated = cv2.dilate(mask[0], kernel, iterations = 1)\n","\n","    feature_co = full_info_image.variables[\"nrcs_detrend_co\"][:]\n","    feature_co[0][mask_dilated != 0] = 0 # mask out the land values\n","    feature_co = (feature_co[0] - np.min(feature_co[0]))/(np.max(feature_co[0]) - np.min(feature_co[0]))\n","\n","    feature_cross = full_info_image.variables[\"nrcs_detrend_cross\"][:]\n","    feature_cross[0][mask_dilated != 0] = 0 # mask out the land values\n","    feature_cross = (feature_cross[0] - np.min(feature_cross[0]))/(np.max(feature_cross[0]) - np.min(feature_cross[0]))\n","\n","    feature_wind = full_info_image.variables[\"wind_speed\"][:]\n","    feature_wind[0][mask_dilated != 0] = 0 # mask out the land values\n","    feature_wind = (feature_wind[0] - np.min(feature_wind[0]))/(np.max(feature_wind[0]) - np.min(feature_wind[0]))\n","\n","    # stack matrices along 3rd axis (depth-wise)\n","    output_image = np.dstack((feature_co, feature_cross, feature_wind))\n","\n","    # extract longitude and latitude features\n","    feature_lon = full_info_image.variables[\"lon\"][:]\n","    #feature_lon = feature_lon[0]\n","    feature_lat = full_info_image.variables[\"lat\"][:]\n","    #feature_lat = feature_lat[0]\n","\n","    nc_info = nc_CyclObs_info.loc[nc_CyclObs_info[\"data_url\"] == os.path.basename(row[\"nc_files\"])]\n","    nc_sid = nc_info[\"sid\"].values[0].upper()\n","    print(nc_sid)\n","\n","    # prepare plot of coordinates\n","    fig, ax = plt.subplots()\n","    ax.imshow(output_image)    \n","\n","    # filter best_track_df by date\n","    aux = best_track_df.loc[best_track_df['ISO_TIME_str'] == datetime.datetime.strftime(row[\"date_time\"], \"%Y-%m-%d\")][[\"USA_ATCF_ID\", \"ISO_TIME\", \"LAT\", \"LON\"]]\n","    #print(aux)\n","    if not aux.empty:\n","        best_track = aux.loc[aux['USA_ATCF_ID'] == nc_sid] if len(np.unique(aux['USA_ATCF_ID'])) > 1 else aux\n","\n","        if not best_track.empty:\n","            best_track = best_track.append({\"ISO_TIME\": row[\"date_time\"], \"LAT\": np.nan, \"LON\": np.nan}, ignore_index=True)\n","            best_track = best_track.sort_values(by=[\"ISO_TIME\"], ignore_index=True)\n","            interp_best_track = best_track.interpolate(method='linear')\n","            bt_lat_value = interp_best_track.loc[interp_best_track[\"ISO_TIME\"] == row[\"date_time\"]][\"LAT\"]\n","            bt_lon_value = interp_best_track.loc[interp_best_track[\"ISO_TIME\"] == row[\"date_time\"]][\"LON\"]\n","            print(interp_best_track)\n","            print(bt_lat_value.values[0], bt_lon_value.values[0])\n","\n","            _, bt_idx_lat = find_nearest(feature_lat[0], bt_lat_value.values[0])\n","            _, bt_idx_lon = find_nearest(feature_lon[0], bt_lon_value.values[0])\n","            bt_x = bt_idx_lon[1]\n","            bt_y = bt_idx_lat[0]\n","            print(\"BT Center:\", bt_x, bt_y)\n","\n","            ax.plot(bt_x, bt_y, '^', mfc='none', label = \"Best track\")\n","\n","    for item in row['ARCHER product']:\n","        if row['ARCHER product'] == []:\n","            continue\n","        print(item)\n","        archer_df = pd.read_csv(item)\n","        archer_df[\"Date/Time (UTC)\"] = pd.to_datetime(archer_df[\"Date/Time (UTC)\"])\n","\n","        lon_col = \"Lon\" if \"archerTrackTable\" in os.path.basename(item) else \"Lon2\"\n","        coords = archer_df[[\"Date/Time (UTC)\", \"Geo-ref Lat\", lon_col]].dropna()\n","        coords_df = coords[coords[\"Geo-ref Lat\"] != \"***\"]\n","        coords_df[\"Geo-ref Lat\"] = coords_df[\"Geo-ref Lat\"].astype(float)\n","        coords_df[lon_col] = coords_df[lon_col].astype(float)\n","        coords_df = coords_df.append({\"Date/Time (UTC)\": row[\"date_time\"], \"Geo-ref Lat\": np.nan, lon_col: np.nan}, ignore_index=True)\n","        coords_df = coords_df.sort_values(by=[\"Date/Time (UTC)\"], ignore_index=True)\n","        print(coords_df)\n","\n","        # Check time difference between ARCHER records and SAR measuremente Date\n","        index = coords_df.index[coords_df[\"Date/Time (UTC)\"] == row[\"date_time\"]].tolist()[0]\n","        test_diff = coords_df.iloc[index-1:index+2] if index > 0 else coords_df.iloc[index:index+2]\n","        print(min(test_diff['Date/Time (UTC)'].diff().dropna().tolist()))\n","        time_diffs.append(min(test_diff['Date/Time (UTC)'].diff().dropna().tolist()))\n","\n","        interp_coords_df = coords_df.interpolate(method='linear')\n","        lat_value = interp_coords_df.loc[interp_coords_df[\"Date/Time (UTC)\"] == row[\"date_time\"]][\"Geo-ref Lat\"]\n","        lon_value = interp_coords_df.loc[interp_coords_df[\"Date/Time (UTC)\"] == row[\"date_time\"]][lon_col]\n","        #print(interp_coords_df.loc[interp_coords_df[\"Date/Time (UTC)\"] == row[\"date_time\"]])\n","    \n","        lat, idx_lat = find_nearest(feature_lat[0], lat_value.values[0])\n","        lon, idx_lon = find_nearest(feature_lon[0], lon_value.values[0])\n","        x = idx_lon[1]\n","        y = idx_lat[0]\n","        print(\"ARCHER Center:\", x, y)\n","\n","        ax.plot(x, y, 'o', mfc='none', label = \"ARCHER\")\n","        \n","    ax.legend(title = \"Source\", bbox_to_anchor=(1.05, 1), loc='upper left')\n","    plt.show()\n","    #plots_dir = main_dir + \"/plot_coords\"\n","    #os.makedirs(plots_dir, exist_ok=True)\n","    #fig.savefig(\"{}/TC_center_by_source_{}.jpg\".format(plots_dir, cnt), bbox_inches='tight')\n","\n","    #if cnt == 3:\n","    #    break\n","\n","time_diffs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B-O68dWgThnV"},"source":["time_diff_df = pd.DataFrame(time_diffs, columns=['Date'])\n","#print(time_diff_df)\n","\n","hours = time_diff_df.Date / np.timedelta64(1, 'h')\n","#hours = hours[hours < 48]\n","\n","fig,ax = plt.subplots(1, 1, figsize=(15,8))\n","ax.set(xlabel = 'Time difference (hours)', title='Time difference between SAR and ARCHER products')\n","hours.plot.hist(bins=100)\n","plt.grid(True, axis='y')\n","plt.show()\n","\n","#fig.savefig(main_dir + \"/time_diff_hist.png\", bbox_inches = 'tight')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xYSJQrLgHugS"},"source":["##6.Test black pixels in image"]},{"cell_type":"code","metadata":{"id":"HT8lqJwrHxok"},"source":["# read the image path\n","image_path = \"SAR_swath_images_VV+VH+WS_dilated/category2/2018-10-24 01_06_05.png\"\n","im = cv2.imread(image_path) # loads images as BGR in float32\n","image = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)   # BGR -> RGB\n","cv2_imshow(image)\n","print(image.size)\n","print(image.shape)\n","print(image.shape[0]*image.shape[1])\n","print(cv2.countNonZero(image[:,:,0]))\n","print(cv2.countNonZero(image[:,:,1]))\n","print(cv2.countNonZero(image[:,:,2]))\n","percentage = (image.shape[0]*image.shape[1] - cv2.countNonZero(image[:,:,0]))/(image.shape[0]*image.shape[1]) * 100\n","percentage"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vd-td7MaJjEK"},"source":["# read the image path\n","image_path = \"SAR_swath_images_VV+VH+WS_dilated/category2/2020-09-22 10_17_34.png\"\n","im = cv2.imread(image_path) # loads images as BGR in float32\n","image = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)   # BGR -> RGB\n","cv2_imshow(image)\n","print(image.size)\n","print(image.shape)\n","print(image.shape[0]*image.shape[1])\n","print(cv2.countNonZero(image[:,:,0]))\n","percentage = (image.shape[0]*image.shape[1] - cv2.countNonZero(image[:,:,0]))/(image.shape[0]*image.shape[1]) * 100\n","percentage"],"execution_count":null,"outputs":[]}]}