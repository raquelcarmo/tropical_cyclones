{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/raquelcarmo/tropical_cyclones/blob/main/src/code/TC_Vmax_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jnxs1L1PWjeL"
   },
   "source": [
    "# Maximum Wind Speed (Vmax) Regression\n",
    "Script to execute training of a regression model using the Maximum Wind Speed feature of the Tropical Cyclone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4p-mSrzKWqUb"
   },
   "outputs": [],
   "source": [
    "# Insert your desired path to work on\n",
    "import os\n",
    "os.chdir('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZu5fkuXWxdC"
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import netCDF4\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load functions to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kADmGNs14dNr"
   },
   "outputs": [],
   "source": [
    "def knots_to_m_sec(kts):\n",
    "    \"\"\"Converts knots (kt) to meters per second (m/s).\"\"\"\n",
    "    if np.isnan(kts):\n",
    "        return kts\n",
    "    else:\n",
    "        return kts*0.514444"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set folder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'in_nc': 'nc',\n",
    "    'out_vmax': 'Vmax'\n",
    "}\n",
    "\n",
    "# List comprehension for the folder structure code\n",
    "[os.makedirs(val) for key, val in config.items() if not os.path.exists(val)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhLZr2Y-vwkk"
   },
   "source": [
    "## 1. Test dilation of land mask on category 4 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KT8eOkQxPFd"
   },
   "outputs": [],
   "source": [
    "# Set directory to save results\n",
    "save_dir = f\"{config['out_vmax']}/hm==23_dm!=0\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "use_cat0 = False\n",
    "\n",
    "# Retrieve nc files only of category 4\n",
    "file_list_cat4 = glob(f\"{config['in_nc']}/category4/*.nc\")\n",
    "file_list_cat5 = [os.path.basename(x) for x in glob(f\"{config['in_nc']}/category5/*.nc\")]\n",
    "list_cat4 = [file for file in file_list_cat4 if os.path.basename(file) not in file_list_cat5]\n",
    "print(len(list_cat4))\n",
    "\n",
    "#file_list_cat1 = glob(f\"{config['in_nc']}/category1/*.nc\")\n",
    "#file_list_cat2 = [os.path.basename(x) for x in glob(f\"{config['in_nc']}/category2/*.nc\")]\n",
    "#list_cat1 = [file for file in file_list_cat1 if os.path.basename(file) not in file_list_cat2]\n",
    "\n",
    "values = []\n",
    "for single_file in list_cat4:\n",
    "    # Read .nc file\n",
    "    full_info_image = netCDF4.Dataset(single_file, mode='r') \n",
    "\n",
    "    # Get wind_speed feature from .nc product\n",
    "    feature_wind = full_info_image.variables[\"wind_speed\"][:]\n",
    "    feature_wind = feature_wind[0]\n",
    "\n",
    "    # Plot feature\n",
    "    plt.imshow(feature_wind)\n",
    "    plt.title(\"Original wind feature\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    print(\"First Vmax value:\", np.max(feature_wind))\n",
    "    ind = np.unravel_index(np.argmax(feature_wind, axis=None), feature_wind.shape)\n",
    "    print(\"Pixel for the Vmax found:\", ind)\n",
    "\n",
    "    # Get mask from .nc product\n",
    "    mask = full_info_image.variables[\"mask_flag\"][:]\n",
    "    mask = mask[0]\n",
    "    mask[mask != 0] = 1\n",
    "    #np.int8(mask != 0)\n",
    "\n",
    "    # Dilate mask\n",
    "    kernel = np.ones((11, 11), np.int8)\n",
    "    dilation = cv2.dilate(mask,kernel,iterations = 1)\n",
    "    \n",
    "    #plt.imshow(dilation)\n",
    "    #plt.colorbar()\n",
    "    #plt.show()    \n",
    "    \n",
    "    # Apply dilated mask to feature\n",
    "    feature_wind[dilation != 0] = 0\n",
    "\n",
    "    plt.imshow(feature_wind)\n",
    "    plt.title(\"Wind feature with land mask\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    print(\"Intermediate Vmax value:\", np.max(feature_wind))\n",
    "\n",
    "    # Get heterogeneity mask from .nc product\n",
    "    heterogeneity_mask = full_info_image.variables[\"heterogeneity_mask\"][:]\n",
    "    heterogeneity_mask = heterogeneity_mask[0]\n",
    "\n",
    "    plt.imshow(heterogeneity_mask)\n",
    "    plt.title(\"heterogeneity mask\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    print(\"heterogeneity mask unique values:\", np.unique(heterogeneity_mask))\n",
    "\n",
    "    # Test feature with both masks\n",
    "    feature_wind[heterogeneity_mask == 3] = 0\n",
    "    feature_wind[heterogeneity_mask == 2] = 0\n",
    "    plt.imshow(feature_wind)\n",
    "    plt.title(\"Wind feature with both masks\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    ind = np.unravel_index(np.argmax(feature_wind, axis=None), feature_wind.shape)\n",
    "    ws_Vmax = np.max(feature_wind)\n",
    "    print(os.path.basename(single_file))\n",
    "    print(\"Final Vmax value:\", ws_Vmax)\n",
    "    print(\"Pixel for the Vmax found:\", ind)\n",
    "\n",
    "    values.append(ws_Vmax)\n",
    "    #plt.imshow(feature_wind)\n",
    "    #plt.title(\"Final image\")\n",
    "    #plt.colorbar()\n",
    "    #plt.show()\n",
    "    print(\"---------------------------------------\")\n",
    "print(\"Values:\", values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gk0SR8lr1yZs"
   },
   "outputs": [],
   "source": [
    "plt.hist(values)\n",
    "plt.title(\"Category 4 Vmax distribution\")\n",
    "plt.savefig(f\"{save_dir}/Category4_Vmax_dist.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQMbpy9_v2dB"
   },
   "source": [
    "## 2. Create dataframe with all Vmax values for all categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZSD06Ni1ggp"
   },
   "outputs": [],
   "source": [
    "# Load generic best tracks CSV file\n",
    "TC_dataset = pd.read_csv(\"best_track/ibtracs.since1980.list.v04r00.csv\", header=0)\n",
    "\n",
    "# Load dataframe connecting .nc filename to TC Name and USA_ATCF_ID\n",
    "nc_ID = pd.read_csv(f\"{config['in_nc']}/cyclobs_tc_dataframe.csv\", header=0)\n",
    "\n",
    "# Gather all .nc files from all categories\n",
    "file_list = glob(f\"{config['in_nc']}/*/*.nc\")\n",
    "\n",
    "files = []\n",
    "ids = []\n",
    "cyclob_Vmax_list = []\n",
    "ws_Vmax_list = []\n",
    "best_track_Vmax_list = []\n",
    "tmax_list = []\n",
    "ws_Vmax_masked_list = []\n",
    "\n",
    "cnt=0\n",
    "for single_file in file_list:\n",
    "    nc_filename = os.path.basename(single_file)\n",
    "    df = nc_ID[nc_ID.data == nc_filename]\n",
    "\n",
    "    # cyclob_Vmax is the Vmax value in the request url dataframe from Cyclobs API\n",
    "    cyclob_Vmax = df['vmax (m/s)'].values\n",
    "    id = df.sid.values\n",
    "\n",
    "    ##############################\n",
    "    try:\n",
    "        # Read the .nc product\n",
    "        full_info_image = netCDF4.Dataset(single_file, mode='r') \n",
    "\n",
    "        # Try to read \"nrcs_detrend_cross\" feature (may give problems!)\n",
    "        feature_cross = full_info_image.variables[\"nrcs_detrend_cross\"][:]\n",
    "\n",
    "        tmax_units = datetime.strptime(full_info_image.measurementDate, '%Y-%m-%dT%H:%M:%SZ')\n",
    "        tmax = tmax_units.strftime('%Y-%m-%d %H_%M_%S')\n",
    "\n",
    "        # Extract wind speed info from .nc product\n",
    "        feature_wind = full_info_image.variables[\"wind_speed\"][:]\n",
    "        feature_wind = feature_wind[0]\n",
    "\n",
    "        # ws_Vmax is the maximum wind speed of the product\n",
    "        ws_Vmax = np.max(feature_wind)\n",
    "        #print(ws_Vmax)\n",
    "\n",
    "        # Extract mask from .nc product\n",
    "        mask = full_info_image.variables[\"mask_flag\"][:]\n",
    "        mask = mask[0]\n",
    "        mask[mask != 0] = 1\n",
    "        \n",
    "        # Dilate mask\n",
    "        kernel = np.ones((11, 11), np.int8)\n",
    "        mask_dilated = cv2.dilate(mask, kernel, iterations = 1)\n",
    "\n",
    "        # Extract heterogeneity mask from .nc product\n",
    "        heterogeneity_mask = full_info_image.variables[\"heterogeneity_mask\"][:]\n",
    "        heterogeneity_mask = heterogeneity_mask[0]\n",
    "        #print(np.unique(heterogeneity_mask))\n",
    "\n",
    "        # Set all mask pixels different than 0 to 0\n",
    "        feature_wind[mask_dilated != 0] = 0\n",
    "        #feature_wind[heterogeneity_mask != 0] = 0\n",
    "        feature_wind[heterogeneity_mask == 3] = 0\n",
    "        feature_wind[heterogeneity_mask == 2] = 0\n",
    "\n",
    "        # ws_Vmax_masked is the Vmax of the product after applying the masks\n",
    "        ws_Vmax_masked = np.max(feature_wind)\n",
    "        if ws_Vmax != ws_Vmax_masked:\n",
    "            cnt +=1\n",
    "        full_info_image.close()\n",
    "    \n",
    "    except KeyError as err:\n",
    "        # Creating KeyError instance for book keeping\n",
    "        print(\"Error:\", err)\n",
    "        full_info_image.close()\n",
    "        continue\n",
    "    ##############################\n",
    "\n",
    "    # Get rows in the TC best track that are taken in the same day of the TC\n",
    "    day = tmax[:10]\n",
    "    img_info = TC_dataset.loc[TC_dataset.ISO_TIME.str.contains(day)]\n",
    "    img_info = img_info.loc[img_info.USA_ATCF_ID == id[0].upper()]\n",
    "\n",
    "    for index, row in img_info.iterrows():\n",
    "        img_info['ISO_TIME'][index] = datetime.strptime(img_info['ISO_TIME'][index], '%Y-%m-%d %H:%M:%S')\n",
    "    img_info['USA_WIND'] = img_info['USA_WIND'].astype(float)\n",
    "    aux = img_info[['ISO_TIME', 'USA_WIND']]\n",
    "\n",
    "    df2 = pd.DataFrame([[tmax_units, np.nan]], columns=['ISO_TIME', 'USA_WIND'])\n",
    "    #print(df2)\n",
    "    aux = aux.append(df2, ignore_index=True)\n",
    "    aux = aux.sort_values(by='ISO_TIME')\n",
    "    #print(aux)\n",
    "\n",
    "    interp = aux.interpolate()\n",
    "    #print(interp)\n",
    "    best_track_Vmax = interp.USA_WIND.loc[interp.ISO_TIME == tmax_units].values\n",
    "    #print(best_track_Vmax)\n",
    "\n",
    "    files.append(single_file)\n",
    "    ids.append(id[0])\n",
    "    cyclob_Vmax_list.append(cyclob_Vmax[0])\n",
    "    best_track_Vmax_list.append(knots_to_m_sec(best_track_Vmax[0]))\n",
    "    ws_Vmax_list.append(ws_Vmax)\n",
    "    ws_Vmax_masked_list.append(ws_Vmax_masked)\n",
    "    tmax_list.append(tmax)\n",
    "\n",
    "data = {\"nc\": files, \"sid\": ids, \"tmax\": tmax_list, \"Vmax_ws\": ws_Vmax_list, \n",
    "        \"Vmax_ws_masked\": ws_Vmax_masked_list, \"Vmax_cyclob\": cyclob_Vmax_list, \n",
    "        \"Vmax_best_track\": best_track_Vmax_list}\n",
    "data_df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Number of Vmax values that changed by applying the land mask and the heterogeneity mask:\", cnt)\n",
    "\n",
    "# Save relevant information in a csv file\n",
    "data_df.to_csv(f\"{save_dir}/Vmax_info_masked_both.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqaLj5xGwAAr"
   },
   "source": [
    "### 2.1. Drop duplicated rows and timestamps not present in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4ENwJLpkPOx"
   },
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# NOTE: \n",
    "# What reduces the dataset to a size of 227 is dropping duplicates and excluding .nc\n",
    "# products that give an error while trying to read the nrcs_detrend_cross feature\n",
    "#####################################################################################\n",
    "\n",
    "dir = 'VV_VH_WS'\n",
    "\n",
    "dataset = pd.read_csv(f\"{save_dir}/Vmax_info_masked_both.csv\")\n",
    "\n",
    "# Check indexes with duplicated tmax and keep the last one\n",
    "dataset = dataset.drop_duplicates(subset=[\"tmax\"], keep = 'last')\n",
    "\n",
    "# Eliminate rows with timestamps not present in our dataset\n",
    "file_list = glob(f\"{dir}/*/*.png\")\n",
    "\n",
    "times = np.array([item.split(os.sep)[-1][:-4] for item in file_list])\n",
    "df_tmax = dataset.tmax.values\n",
    "\n",
    "# This diff happened because I wasn't using try except nrcs_detrend_cross exception\n",
    "diff = list(list(set(times)-set(df_tmax)) + list(set(df_tmax)-set(times)))\n",
    "if len(diff) > 0:\n",
    "    print(diff)\n",
    "    idxs = []\n",
    "    for item in diff:\n",
    "        df = dataset.loc[dataset.tmax == item]\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            idxs.append(idx)\n",
    "    dataset = dataset.drop(idxs)\n",
    "\n",
    "# Create category column\n",
    "dataset[\"category\"] = dataset[\"nc\"].str.split(os.sep, expand=True)[1]\n",
    "\n",
    "if use_cat0:\n",
    "    # Load full_dataset.csv to get info on labels\n",
    "    df = pd.read_csv(f\"{dir}/csv/full_dataset.csv\")\n",
    "    df['tmax'] = df['image'].str.split(os.sep).str[-1].str[:-4]\n",
    "    #print(df)\n",
    "    dataset = dataset.merge(df[['tmax', 'label']])\n",
    "    \n",
    "    # Drop products where there is no eye\n",
    "    #dataset = dataset[dataset.label != 0]\n",
    "    # Instead of dropping no-eye images, we can create \"category0\"\n",
    "    dataset[\"category\"] = np.where((dataset.label == 0), 'category0', dataset[\"category\"])\n",
    "\n",
    "dataset.reset_index(inplace=True, drop=True)\n",
    "dataset.to_csv(f\"{save_dir}/Vmax_info_masked_both_filtered.csv\", index=False)\n",
    "\n",
    "print(\"cat0:\", len(dataset[dataset.category == \"category0\"]))\n",
    "print(\"cat1:\", len(dataset[dataset.category == \"category1\"]))\n",
    "print(\"cat2:\", len(dataset[dataset.category == \"category2\"]))\n",
    "print(\"cat3:\", len(dataset[dataset.category == \"category3\"]))\n",
    "print(\"cat4:\", len(dataset[dataset.category == \"category4\"]))\n",
    "print(\"cat5:\", len(dataset[dataset.category == \"category5\"]))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8X91rnaZE2f0"
   },
   "source": [
    "### 2.2. Plot distribution of Vmax across categories for different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lxtdm_02BTNO"
   },
   "outputs": [],
   "source": [
    "hist_dir = f\"{save_dir}/histograms\"\n",
    "os.makedirs(hist_dir, exist_ok=True)\n",
    "save_plots = True\n",
    "n_bins = 50\n",
    "\n",
    "dataset = pd.read_csv(f\"{save_dir}/Vmax_info_masked_both_filtered.csv\")\n",
    "\n",
    "#######################\n",
    "###   Vmax_cyclob   ###\n",
    "#######################\n",
    "fig, ax = plt.subplots(figsize =(11,8))\n",
    "rate = 0.2\n",
    "for cat in range(1, 6):\n",
    "    category = 'category'+str(cat)\n",
    "    feature = dataset.where(dataset.category == category).loc[:, \"Vmax_cyclob\"]\n",
    "    feature.hist(ax=ax, label=category)\n",
    "ax.set(title='Vmax cyclob', ylabel='Count', xlabel='Vmax (m/s)')\n",
    "ax.legend(fontsize = 15)\n",
    "ax.grid(axis='y')\n",
    "ax.set_facecolor('#d8dcd6')\n",
    "plt.show()\n",
    "if save_plots:\n",
    "    fig.savefig(f\"{hist_dir}/Vmax_cyclob.png\", bbox_inches='tight')\n",
    "\n",
    "\n",
    "##########################\n",
    "###   Vmax_ws_masked   ###\n",
    "##########################\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize =(19,8))\n",
    "\n",
    "for cat in range(1, 6):\n",
    "    category = 'category'+str(cat)\n",
    "    feature = dataset.where(dataset.category == category).loc[:, \"Vmax_ws_masked\"]\n",
    "    feature.hist(ax=ax1, label=category, alpha = rate*cat, bins = n_bins)\n",
    "ax1.set(title='Vmax masked unstacked', ylabel='Count', xlabel='Vmax (m/s)')\n",
    "ax1.legend(fontsize = 15)\n",
    "ax1.grid(axis='y')\n",
    "\n",
    "cats = ['category1', 'category2', 'category3', 'category4', 'category5']\n",
    "cat1 = dataset.where(dataset.category == 'category1').loc[:, \"Vmax_ws_masked\"]\n",
    "cat2 = dataset.where(dataset.category == 'category2').loc[:, \"Vmax_ws_masked\"]\n",
    "cat3 = dataset.where(dataset.category == 'category3').loc[:, \"Vmax_ws_masked\"]\n",
    "cat4 = dataset.where(dataset.category == 'category4').loc[:, \"Vmax_ws_masked\"]\n",
    "cat5 = dataset.where(dataset.category == 'category5').loc[:, \"Vmax_ws_masked\"]\n",
    "\n",
    "ax2.hist([cat1, cat2, cat3, cat4, cat5], stacked=True, label = cats, bins = n_bins)\n",
    "ax2.set(title='Vmax masked stacked', ylabel='Count', xlabel='Vmax (m/s)')\n",
    "ax2.legend(fontsize = 15)\n",
    "ax2.grid(axis='x')\n",
    "ax2.set_facecolor('#d8dcd6')\n",
    "plt.show()\n",
    "if save_plots:\n",
    "    fig.savefig(f\"{hist_dir}/Vmax_ws_masked.png\", bbox_inches='tight')\n",
    "\n",
    "\n",
    "###################\n",
    "###   Vmax_ws   ###\n",
    "###################\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize =(19,8))\n",
    "\n",
    "for cat in range(1, 6):\n",
    "    category = 'category'+str(cat)\n",
    "    feature = dataset.where(dataset.category == category).loc[:, \"Vmax_ws\"]\n",
    "    feature.hist(ax=ax1, label=category, alpha = rate*cat, bins = n_bins)\n",
    "ax1.set(title='Vmax unmasked unstacked', ylabel='Count', xlabel='Vmax (m/s)')\n",
    "ax1.legend(fontsize = 15)\n",
    "ax1.grid(axis='y')\n",
    "#ax.set_facecolor('#d8dcd6')\n",
    "\n",
    "cats = ['category1', 'category2', 'category3', 'category4', 'category5']\n",
    "cat1 = dataset.where(dataset.category == 'category1').loc[:, \"Vmax_ws\"]\n",
    "cat2 = dataset.where(dataset.category == 'category2').loc[:, \"Vmax_ws\"]\n",
    "cat3 = dataset.where(dataset.category == 'category3').loc[:, \"Vmax_ws\"]\n",
    "cat4 = dataset.where(dataset.category == 'category4').loc[:, \"Vmax_ws\"]\n",
    "cat5 = dataset.where(dataset.category == 'category5').loc[:, \"Vmax_ws\"]\n",
    "\n",
    "ax2.hist([cat1, cat2, cat3, cat4, cat5], stacked=True, label = cats, bins = n_bins)\n",
    "ax2.set(title='Vmax unmasked stacked', ylabel='Count', xlabel='Vmax (m/s)')\n",
    "ax2.legend(fontsize = 15)\n",
    "ax2.grid(axis='x')\n",
    "ax2.set_facecolor('#d8dcd6')\n",
    "plt.show()\n",
    "if save_plots:\n",
    "    fig.savefig(f\"{hist_dir}/Vmax_ws.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kF5maLYnFBL4"
   },
   "source": [
    "## 3. Train Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XWAnPRemp0Da"
   },
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "results_dir = f\"{save_dir}/regression_results\"\n",
    "os.makedirs(results_dir, exist_ok= True)\n",
    "save_plots = True\n",
    "\n",
    "dataset = pd.read_csv(f\"{save_dir}/Vmax_info_masked_both_filtered.csv\")\n",
    "\n",
    "feature = \"Vmax_ws_masked\"\n",
    "X = dataset.loc[:, feature].values\n",
    "y = dataset.loc[:, \"category\"].str.extract('(\\d+)').astype(int).values.flatten()\n",
    "#X = X/X.max()\n",
    "#print(X)\n",
    "\n",
    "# Split the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "#print(X_test, y_test)\n",
    "\n",
    "# One-hot encode outputs\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Count number of classes\n",
    "nb_classes = y_test.shape[1]\n",
    "print(\"Number of classes:\", nb_classes)\n",
    "\n",
    "X_train = X_train.reshape(-1, 1)\n",
    "X_test = X_test.reshape(-1, 1)\n",
    "\n",
    "# Feature scaling\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "#print(X_train, X_test)\n",
    "\n",
    "# Create model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation = 'relu', input_dim = 1))    # adding the input layer and the first hidden layer\n",
    "model.add(Dense(units = 32, activation = 'relu'))   # second hidden layer\n",
    "model.add(Dense(units = 32, activation = 'relu'))   # third hidden layer \n",
    "model.add(Dense(nb_classes, activation='softmax'))   # output layer\n",
    "#model.add(Dense(units = 1))\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer = 'adam', \n",
    "              loss = 'categorical_crossentropy', #loss = 'mean_squared_error'\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# Fit model to data\n",
    "callback = [\n",
    "    ModelCheckpoint(f\"{results_dir}/{feature}_best_model.h5\", verbose=1, save_best_only=True)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split = 0.2,\n",
    "    batch_size = 10,\n",
    "    epochs = 50,\n",
    "    callbacks = callback\n",
    ")\n",
    "\n",
    "# Load best model\n",
    "time.sleep(5) # guarantees enough time for weights to be saved and loaded afterwards, otherwise gives concurrency problems\n",
    "print(\"Loaded best weights of the training\")\n",
    "model.load_weights(f\"{results_dir}/{feature}_best_model.h5\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "#print(y_pred)\n",
    "predictions = [np.argmax(t) for t in y_pred]\n",
    "#print(predictions)\n",
    "y_test_non_category = [np.argmax(t) for t in y_test]\n",
    "\n",
    "# Plot confusion matrix\n",
    "conf_mat = confusion_matrix(y_test_non_category, predictions, labels = [0,1,2,3,4,5])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = conf_mat, display_labels = [0,1,2,3,4,5])\n",
    "conf_mat_display = disp.plot()\n",
    "if save_plots:\n",
    "    plt.savefig(f\"{results_dir}/{feature}_confusion_matrix.jpg\", bbox_inches='tight')\n",
    "\n",
    "# Plot train/validation losses\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(10,10))\n",
    "ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "ax1.grid(True)\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.set(ylabel=\"Accuracy\",\n",
    "        title='Training and Validation Accuracy')\n",
    "\n",
    "ax2.plot(history.history['loss'], label='Training Loss')\n",
    "ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
    "ax2.grid(True)\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.set(xlabel='Epoch',\n",
    "        ylabel='Categorical Cross Entropy',\n",
    "        title='Training and Validation Loss')\n",
    "plt.show()\n",
    "if save_plots:\n",
    "    fig.savefig(f\"{results_dir}/{feature}_learning_curves.jpg\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs labels\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.plot(y_test_non_category, '.', color = 'red', label = 'Real data')\n",
    "ax.plot(predictions, '.', color = 'blue', label = 'Predicted data')\n",
    "ax.set(title = 'Prediction')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2s1hi1pjPBf"
   },
   "source": [
    "### 3.1. Test different regression model structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bm5QP3kGKPXa"
   },
   "outputs": [],
   "source": [
    "# model = tf.keras.Sequential([\n",
    "#   tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(1,)),  # input shape required\n",
    "#   tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "#   tf.keras.layers.Dense(3)\n",
    "# ])\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(10, activation='relu', input_dim=1))\n",
    "# model.add(Dense(10, activation='relu'))\n",
    "# model.add(Dense(5, activation='softmax'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MhLZr2Y-vwkk",
    "8X91rnaZE2f0"
   ],
   "name": "TC_Vmax_Regression.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "tropical_cyclones",
   "language": "python",
   "name": "tropical_cyclones"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
