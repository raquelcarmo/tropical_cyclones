{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/raquelcarmo/tropical_cyclones/blob/import-py-files/src/code/TC_Category_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDvPT6mlexeA"
   },
   "source": [
    "# Tropical Cyclones Categorization\n",
    "Script to train Deep Learning models to categorize tropical cyclones based on their topology patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EG-uvLsngAPX"
   },
   "source": [
    "## Imports and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jnxs1L1PWjeL"
   },
   "outputs": [],
   "source": [
    "# Insert your desired path to work on\n",
    "import os\n",
    "os.chdir('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell once per session. This cell links the code folder to the python exectution path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNUcV31BIHhF"
   },
   "outputs": [],
   "source": [
    "# Path where the modules are stored\n",
    "import sys\n",
    "sys.path.append('../code')\n",
    "\n",
    "# Import modules\n",
    "import utils\n",
    "from models import CategorizationCNN\n",
    "from data_process import DataProcessor\n",
    "from visualization import plot_history, plot_history_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell allows Jupyter Notebooks to detect changes in external code and to automatically update it without restarting the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.applications import resnet50, mobilenet_v2, vgg16\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import concatenate, Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy, TopKCategoricalAccuracy, Precision, Recall, TruePositives, FalsePositives, TrueNegatives, FalseNegatives\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUEMhiDjZ9bi"
   },
   "source": [
    "## 1. Train on data according to csv split into train, val and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Define settings (arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'main_dir':        \"VV_VH_WS\",\n",
    "    'cnn':             \"ResNet\",     # choices: [\"ResNet\", \"Mobile\"]\n",
    "    'loss':            \"categorical_crossentropy\",\n",
    "    'height':          700,\n",
    "    'width':           400,\n",
    "    'eye_only':        True,\n",
    "    'numerical_vars':  False,\n",
    "    'normalise':       True,\n",
    "    'norm_mode':       \"model\",     # choices=['z-norm', 'model', 'simple', 'none']\n",
    "    'rotate':          False,\n",
    "    'crop':            True,\n",
    "    'crop_mode':       \"uniform\",   # choices=['uniform', 'weighted']\n",
    "    'nb_crops':        1,\n",
    "    'data_aug':        False,\n",
    "    'batch_size':      8,\n",
    "    'buffer_size':     100,\n",
    "    'epochs':          10,\n",
    "    'learning_rate':   0.0001,\n",
    "    'nb_splits':       5,\n",
    "    'dropout':         True,\n",
    "    'drop_rate':       0.5,\n",
    "    'finetune':        False\n",
    "}\n",
    "args['save_dir'] = os.path.join(args['main_dir'], \"results\", \"cat\", \"R_700x400_nM_bs8_bf100_e10_lr0001_dr0.5\")\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Prepare the tf.data.Dataset instances to be fed to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySpk4sOVemVz"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "main_dir = args['main_dir']\n",
    "train_images, train_labels, train_bbox = utils.load_data(args, f\"{main_dir}/csv/training.csv\")\n",
    "val_images, val_labels, val_bbox = utils.load_data(args, f\"{main_dir}/csv/val.csv\")\n",
    "test_images, test_labels, test_bbox = utils.load_data(args, f\"{main_dir}/csv/test.csv\")\n",
    "\n",
    "class_weights = utils.compute_class_weights(f\"{main_dir}/csv/full_dataset.csv\", args)\n",
    "\n",
    "# Create an instance of the DataProcessor\n",
    "p = DataProcessor(args,\n",
    "                  plot_light = False,          # plot only select_crop() images\n",
    "                  plot_extensive = False,      # plot extensively all images\n",
    "                  show_prints = False\n",
    "                 )\n",
    "\n",
    "# Generate datasets\n",
    "# NOTE: utils.create_dataset() allows for further data augmentation\n",
    "train_ds = utils.prepare_dataset(p, train_images, train_labels, train_bbox)\n",
    "val_ds = utils.prepare_dataset(p, val_images, val_labels, val_bbox)\n",
    "test_ds = utils.prepare_dataset(p, test_images, test_labels, test_bbox)\n",
    "\n",
    "# Perform normalization\n",
    "train_ds_norm, val_ds_norm = utils.normalisation(train_ds, val_ds, args)\n",
    "_, test_ds_norm = utils.normalisation(train_ds, test_ds, args)\n",
    "\n",
    "# Configure for performance\n",
    "train_dataset = utils.config_performance(train_ds_norm, args, shuffle=True)\n",
    "val_dataset = utils.config_performance(val_ds_norm, args, flag=True)\n",
    "test_dataset = utils.config_performance(test_ds_norm, args, flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9-tkgGrid0-"
   },
   "source": [
    "### 1.3. Perform end-to-end training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0eTiiKghBZg"
   },
   "outputs": [],
   "source": [
    "# Directory to save results\n",
    "save_dir = args['save_dir']\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Create model\n",
    "model = CategorizationCNN(args)\n",
    "\n",
    "# Create callbacks\n",
    "dt = datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\")\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint(os.path.join(save_dir, \"best_model.h5\"), verbose=1, save_best_only=True),\n",
    "    TensorBoard(log_dir=f\"{save_dir}/logs/{dt}\")\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x = train_dataset,\n",
    "    steps_per_epoch = len(train_dataset),\n",
    "    validation_data = val_dataset,\n",
    "    validation_steps = len(val_dataset),\n",
    "    epochs = args['epochs'],\n",
    "    callbacks = callbacks,\n",
    "    verbose = 1,\n",
    "    class_weight = class_weights,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the TensorBoard notebook extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0wRibZ1YUh-"
   },
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir VV_VH_WS/results/cat/R_700x400_nM_bs8_bf100_e10_lr0001_dr0.5/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBRY3vfnhF-D"
   },
   "outputs": [],
   "source": [
    "print(\"Loading best weights from training...\")\n",
    "model.load_weights(os.path.join(save_dir, \"best_model.h5\"))\n",
    "\n",
    "results = model.evaluate(\n",
    "    test_dataset,\n",
    "    steps = len(test_dataset),\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kGg9tMhEhKut"
   },
   "outputs": [],
   "source": [
    "# Retrieve a batch of images from the test set\n",
    "predictions = model.predict(test_dataset)\n",
    "predictions = tf.where(predictions < 0.5, 0, 1)\n",
    "\n",
    "print('Predictions:\\n', predictions.numpy())\n",
    "print('Labels:\\n')\n",
    "test_labels = test_dataset.map(lambda x, y: y)\n",
    "for label in test_labels:\n",
    "    print(label)\n",
    "# class_names = {0: \"No eye\", 1: \"Eye\"}\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for i in range(9):\n",
    "#     ax = plt.subplot(3, 3, i + 1)\n",
    "#     plt.imshow(image_batch[i].astype(\"uint8\"))\n",
    "#     plt.title(class_names[predictions[i]])\n",
    "#     plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-V1lTSzbDUyS"
   },
   "source": [
    "## 2. Train on data using the Stratified K-Fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Perform Stratified 5-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NxvI67fsr2zD"
   },
   "outputs": [],
   "source": [
    "def stratified_cv(args):\n",
    "    dataset_path = f\"{main_dir}/csv/full_dataset.csv\"\n",
    "    # Compute class weights\n",
    "    Y, df, class_weights = utils.compute_class_weights(dataset_path, args)\n",
    "\n",
    "    # Create an instance of the DataProcessor\n",
    "    p = DataProcessor(args,\n",
    "                      plot_light = False,              # plot only select_crop() images\n",
    "                      plot_extensive = False,          # plot extensively all images\n",
    "                      show_prints = False\n",
    "                     )\n",
    "\n",
    "    # Create and build model\n",
    "    catNet = CategorizationCNN(args)\n",
    "\n",
    "    print(\"Entering in K-fold Cross Validation...\")\n",
    "    stratified_k_fold = StratifiedKFold(n_splits=args['nb_splits'], random_state=42, shuffle=False)\n",
    "    fold_var = 1\n",
    "\n",
    "    for train_index, val_index in stratified_k_fold.split(np.zeros(len(df)), Y):\n",
    "        training_data = df.iloc[train_index]\n",
    "        validation_data = df.iloc[val_index]\n",
    "\n",
    "        # Load data\n",
    "        train_images, train_labels, train_bbox = utils.load_data(args, df=training_data)\n",
    "        val_images, val_labels, val_bbox = utils.load_data(args, df=validation_data)\n",
    "\n",
    "        # Generate datasets\n",
    "        #train_ds = utils.prepare_dataset(p, train_images, train_labels, train_bbox)\n",
    "        #val_ds = utils.prepare_dataset(p, val_images, val_labels, val_bbox)\n",
    "        train_ds = utils.create_dataset(p, train_images, train_labels, train_bbox, args)\n",
    "        val_ds = utils.create_dataset(p, val_images, val_labels, val_bbox, args, flag=True)\n",
    "\n",
    "        # Perform normalisation\n",
    "        train_ds_norm, val_ds_norm = utils.normalisation(train_ds, val_ds, args)\n",
    "\n",
    "        # Configure for performance\n",
    "        train_ds_perf = utils.config_performance(train_ds_norm, args, shuffle=True)\n",
    "        val_ds_perf = utils.config_performance(val_ds_norm, args, flag=True)\n",
    "\n",
    "        # Train the model\n",
    "        history = catNet.train(train_ds_perf, val_ds_perf, class_weights, fold_var)\n",
    "        plot_history(history, fold_var, save_dir)\n",
    "        #print(history)\n",
    "        \n",
    "        # Guarantee time for weights to be saved and loaded again\n",
    "        time.sleep(10) \n",
    "        \n",
    "        # Load best model & predict\n",
    "        print(\"Loading best weights from training...\")\n",
    "        catNet.get_eval(val_ds_perf, fold_var)\n",
    "        catNet.get_preds(val_ds_perf, val_ds.map(lambda x, y: y), fold_var)\n",
    "\n",
    "        ### Grad-CAM analysis ###\n",
    "        # Train dataset\n",
    "        train_gradcam_path = f\"{save_dir}/train_gradcam_heatmaps_{fold_var}\"\n",
    "        os.makedirs(train_gradcam_path, exist_ok=True)\n",
    "        gradcam_train_ds = utils.config_performance(train_ds_norm, args, flag=True)\n",
    "        utils.grad_cam(catNet.model, train_ds, train_ds_norm, catNet.model.predict(gradcam_train_ds), train_gradcam_path, args)\n",
    "        # Validation dataset\n",
    "        gradcam_path = f\"{save_dir}/gradcam_heatmaps_{fold_var}\"\n",
    "        os.makedirs(gradcam_path, exist_ok=True)\n",
    "        utils.grad_cam(catNet.model, val_ds, val_ds_norm, catNet.model.predict(val_ds_perf), gradcam_path, args)\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        catNet.__reset()\n",
    "        fold_var += 1\n",
    "\n",
    "    # Save the values of each fold\n",
    "    catNet.save_metrics()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_cv(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yl9NJGQ2jb-g"
   },
   "source": [
    "## 3. Perform fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Define settings (arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'main_dir':        \"VV_VH_WS\",\n",
    "    'cnn':             \"ResNet\",     # choices: [\"ResNet\", \"Mobile\"]\n",
    "    'loss':            \"categorical_crossentropy\",\n",
    "    'height':          700,\n",
    "    'width':           400,\n",
    "    'eye_only':        True,\n",
    "    'numerical_vars':  False,\n",
    "    'normalise':       True,\n",
    "    'norm_mode':       \"model\",     # choices=['z-norm', 'model', 'simple', 'none']\n",
    "    'rotate':          False,\n",
    "    'crop':            True,\n",
    "    'crop_mode':       \"uniform\",   # choices=['uniform', 'weighted']\n",
    "    'nb_crops':        1,\n",
    "    'data_aug':        False,\n",
    "    'batch_size':      8,\n",
    "    'buffer_size':     100,\n",
    "    'epochs':          10,\n",
    "    'learning_rate':   0.0001,\n",
    "    'nb_splits':       5,\n",
    "    'dropout':         True,\n",
    "    'drop_rate':       0.5,\n",
    "    'finetune':        True,\n",
    "    'finetune_at':     -5,\n",
    "    'initial_epochs':  20,\n",
    "    'finetune_epochs': 10\n",
    "}\n",
    "args['save_dir'] = os.path.join(args['main_dir'], \"results\", \"cat\", \"R_700x400_nM_bs8_bf100_lr0001_dr0.5_ft-5_ie20_fe10\")\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPP7a2XY3Atf"
   },
   "source": [
    "### 3.2. Using the csv split into train, val and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "main_dir = args['main_dir']\n",
    "train_images, train_labels, train_bbox = utils.load_data(args, f\"{main_dir}/csv/training.csv\")\n",
    "val_images, val_labels, val_bbox = utils.load_data(args, f\"{main_dir}/csv/val.csv\")\n",
    "test_images, test_labels, test_bbox = utils.load_data(args, f\"{main_dir}/csv/test.csv\")\n",
    "\n",
    "class_weights = utils.compute_class_weights(f\"{main_dir}/csv/full_dataset.csv\", args)\n",
    "\n",
    "# Create an instance of the DataProcessor\n",
    "p = DataProcessor(args,\n",
    "                  plot_light = False,          # plot only select_crop() images\n",
    "                  plot_extensive = False,      # plot extensively all images\n",
    "                  show_prints = False\n",
    "                 )\n",
    "\n",
    "# Generate datasets\n",
    "# NOTE: utils.create_dataset() allows for further data augmentation\n",
    "train_ds = utils.prepare_dataset(p, train_images, train_labels, train_bbox)\n",
    "val_ds = utils.prepare_dataset(p, val_images, val_labels, val_bbox)\n",
    "test_ds = utils.prepare_dataset(p, test_images, test_labels, test_bbox)\n",
    "\n",
    "# Perform normalization\n",
    "train_ds_norm, val_ds_norm = utils.normalisation(train_ds, val_ds, args)\n",
    "_, test_ds_norm = utils.normalisation(train_ds, test_ds, args)\n",
    "\n",
    "# Configure for performance\n",
    "train_dataset = utils.config_performance(train_ds_norm, args, shuffle=True)\n",
    "val_dataset = utils.config_performance(val_ds_norm, args, flag=True)\n",
    "test_dataset = utils.config_performance(test_ds_norm, args, flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save results\n",
    "save_dir = args['save_dir']\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPNL_CdNjdpK"
   },
   "outputs": [],
   "source": [
    "if args['cnn'] == \"ResNet\":\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(args['width'], args['height'], 3))\n",
    "elif args['cnn'] == \"Mobile\":\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(args['width'], args['height'], 3))\n",
    "elif args['cnn'] == \"VGG\":\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(args['width'], args['height'], 3))\n",
    "else:\n",
    "    sys.exit(\"Incert valid cnn model. Options: Mobile, ResNet or VGG (case sensitive)\")\n",
    "\n",
    "# Freeze the base_model\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = Input(shape=(args['width'], args['height'], 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(args['drop_rate'])(x) if args['dropout'] else x\n",
    "\n",
    "classes = 6 if not args['eye_only'] else 5\n",
    "outputs = Dense(classes, activation=\"softmax\")(x)\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Compile the model (should be done *after* setting layers to non-trainable)\n",
    "eval_metrics = [CategoricalAccuracy(name=\"accuracy\"), TopKCategoricalAccuracy(k=2, name=\"top2_accuracy\"),\n",
    "                Precision(name=\"precision\"), Recall(name=\"recall\"), \n",
    "                TruePositives(name='tp'), FalsePositives(name='fp'),\n",
    "                TrueNegatives(name='tn'), FalseNegatives(name='fn')]\n",
    "\n",
    "model.compile(\n",
    "    optimizer = Adam(learning_rate=args['learning_rate']), \n",
    "    loss = args['loss'],\n",
    "    metrics = eval_metrics\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YcGj-neEjnwo"
   },
   "outputs": [],
   "source": [
    "# Create callbacks\n",
    "dt = datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
    "callbacks = [\n",
    "    ModelCheckpoint(f\"{save_dir}/best_model_frozen.h5\", verbose=1, save_best_only=True),\n",
    "    TensorBoard(log_dir = f\"{save_dir}/logs/{dt}\")\n",
    "]\n",
    "\n",
    "# Train the top layer of the model on the dataset, the weights\n",
    "# of the pre-trained network will not be updated during training\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch = len(train_dataset),\n",
    "    validation_data = val_dataset,\n",
    "    validation_steps = len(val_dataset),\n",
    "    epochs = args['initial_epochs'],\n",
    "    callbacks = callbacks,\n",
    "    class_weight = class_weights,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot train/val losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HTQb0boPjqll"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(10,10))\n",
    "ax1.plot(acc, label='Training Accuracy')\n",
    "ax1.plot(val_acc, label='Validation Accuracy')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.set(ylabel = \"Accuracy\",\n",
    "        title = 'Training and Validation Accuracy')\n",
    "#plt.ylim([min(plt.ylim()),1])\n",
    "\n",
    "ax2.plot(loss, label='Training Loss')\n",
    "ax2.plot(val_loss, label='Validation Loss')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.set(xlabel = 'epoch', \n",
    "        ylabel = 'Cross Entropy',\n",
    "        title = 'Training and Validation Loss')\n",
    "#plt.ylim([0,max(plt.ylim())])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Dn0K4WwjtF6"
   },
   "outputs": [],
   "source": [
    "# Unfreeze the whole base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Try to fine-tune a small number of top layers rather than the whole model\n",
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "if args['finetune_at'] > 0:\n",
    "    # fine-tune from this layer onwards, freezing all layers before\n",
    "    for layer in base_model.layers[:args['finetune_at']]:\n",
    "        layer.trainable = False\n",
    "else:\n",
    "    # input is given as number of FINAL layers to finetune\n",
    "    nb_layers2ft = abs(args['finetune_at'])\n",
    "    total2freeze = len(base_model.layers) - nb_layers2ft\n",
    "    for layer in base_model.layers[:total2freeze]:\n",
    "        layer.trainable = False\n",
    "\n",
    "# Recompile the model for the modifications to take effect, with a low learning rate\n",
    "model.compile(\n",
    "    optimizer = Adam(learning_rate = 1e-5),\n",
    "    loss = args['loss'],\n",
    "    metrics = eval_metrics\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2vnrD8Rj4p-"
   },
   "outputs": [],
   "source": [
    "# Adjust callbacks\n",
    "dt = datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
    "callbacks = [\n",
    "    #EarlyStopping(patience=5, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.000001, verbose=1),\n",
    "    ModelCheckpoint(f\"{save_dir}/best_model_fine_tuned.h5\", verbose=1, save_best_only=True),\n",
    "    TensorBoard(log_dir = f\"{save_dir}/logs/{dt}\")\n",
    "]\n",
    "\n",
    "total_epochs = args['initial_epochs'] + args['finetune_epochs']\n",
    "\n",
    "# Train the entire model end-to-end\n",
    "history_fine = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch = len(train_dataset),\n",
    "    validation_data = val_dataset,\n",
    "    validation_steps = len(val_dataset),\n",
    "    epochs = total_epochs,\n",
    "    initial_epoch = history.epoch[-1],\n",
    "    callbacks = callbacks,\n",
    "    class_weight = class_weights,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add history of the fine-tuning step to the previous history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0FwWQH1j7aW"
   },
   "outputs": [],
   "source": [
    "acc += history_fine.history['accuracy']\n",
    "val_acc += history_fine.history['val_accuracy']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot train/val losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows = 2, ncols = 1, figsize=(10, 10))\n",
    "ax1.plot(acc, label='Training Accuracy')\n",
    "ax1.plot(val_acc, label='Validation Accuracy')\n",
    "#plt.ylim([min(plt.ylim()),1])\n",
    "ax1.plot([args['initial_epochs']-1, args['initial_epochs']-1],\n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.set(ylabel = \"Accuracy\",\n",
    "        title = 'Training and Validation Accuracy')\n",
    "\n",
    "ax2.plot(loss, label='Training Loss')\n",
    "ax2.plot(val_loss, label='Validation Loss')\n",
    "#plt.ylim([0, 1.0])\n",
    "ax2.plot([args['initial_epochs']-1, args['initial_epochs']-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.set(xlabel = 'epoch', \n",
    "      ylabel = 'Cross Entropy',\n",
    "      title = 'Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(f'{save_dir}/learning_curves.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gP4_1B_sj_Jy"
   },
   "outputs": [],
   "source": [
    "print(\"Loading best weights from training...\")\n",
    "model.load_weights(f\"{save_dir}/best_model_fine_tuned.h5\")\n",
    "\n",
    "results = model.evaluate(\n",
    "    test_dataset, \n",
    "    steps = len(test_dataset),\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3EqPAWN3Kbl"
   },
   "source": [
    "### 3.3. Using Stratified-K Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hOERchi3OA3"
   },
   "outputs": [],
   "source": [
    "def stratified_cv_ft(args):\n",
    "    dataset_path = f\"{main_dir}/csv/full_dataset.csv\"\n",
    "    # Compute class weights\n",
    "    Y, df, class_weights = utils.compute_class_weights(dataset_path, args)\n",
    "\n",
    "    # Create an instance of the DataProcessor\n",
    "    p = DataProcessor(args,\n",
    "                      plot_light = False,              # plot only select_crop() images\n",
    "                      plot_extensive = False,          # plot extensively all images\n",
    "                      show_prints = False\n",
    "                     )\n",
    "\n",
    "    # Create and build model\n",
    "    catNet = CategorizationCNN(args)\n",
    "\n",
    "    print(\"Entering in K-fold Cross Validation...\")\n",
    "    stratified_k_fold = StratifiedKFold(n_splits=args['nb_splits'], random_state=42, shuffle=False)\n",
    "    fold_var = 1\n",
    "\n",
    "    for train_index, val_index in stratified_k_fold.split(np.zeros(len(df)), Y):\n",
    "        training_data = df.iloc[train_index]\n",
    "        validation_data = df.iloc[val_index]\n",
    "\n",
    "        # Load data\n",
    "        train_images, train_labels, train_bbox = utils.load_data(args, df=training_data)\n",
    "        val_images, val_labels, val_bbox = utils.load_data(args, df=validation_data)\n",
    "\n",
    "        # Generate datasets\n",
    "        #train_ds = utils.prepare_dataset(p, train_images, train_labels, train_bbox)\n",
    "        #val_ds = utils.prepare_dataset(p, val_images, val_labels, val_bbox)\n",
    "        train_ds = utils.create_dataset(p, train_images, train_labels, train_bbox, args)\n",
    "        val_ds = utils.create_dataset(p, val_images, val_labels, val_bbox, args, flag=True)\n",
    "\n",
    "        # Perform normalisation\n",
    "        train_ds_norm, val_ds_norm = utils.normalisation(train_ds, val_ds, args)\n",
    "\n",
    "        # Configure for performance\n",
    "        train_ds_perf = utils.config_performance(train_ds_norm, args, shuffle=True)\n",
    "        val_ds_perf = utils.config_performance(val_ds_norm, args, flag=True)\n",
    "\n",
    "        # Train classifier with frozen base model\n",
    "        history1 = catNet.trainftStage1(train_ds_perf, val_ds_perf, class_weights, fold_var)\n",
    "\n",
    "        acc = history1.history['accuracy']\n",
    "        validation_acc = history1.history['val_accuracy']\n",
    "        loss = history1.history['loss']\n",
    "        validation_loss = history1.history['val_loss']\n",
    "\n",
    "        # Build and train unfrozen model\n",
    "        catNet.__buildftStage2()\n",
    "        history2 = catNet.trainftStage2(train_ds_perf, val_ds_perf, class_weights, history1, fold_var)\n",
    "\n",
    "        acc += history2.history['accuracy']\n",
    "        validation_acc += history2.history['val_accuracy']\n",
    "        loss += history2.history['loss']\n",
    "        validation_loss += history2.history['val_loss']\n",
    "\n",
    "        # Plot train/val losses\n",
    "        hDict = {'acc':acc, 'validation_acc':validation_acc, \n",
    "                'loss':loss, 'validation_loss':validation_loss}\n",
    "        plot_history_ft(args, hDict, fold_var, save_dir)\n",
    "\n",
    "        # Guarantee time for weights to be saved and loaded again\n",
    "        time.sleep(10)\n",
    "\n",
    "        print(\"Loading best weights from training...\")\n",
    "        catNet.get_eval(val_ds_perf, fold_var)\n",
    "        catNet.get_preds(val_ds_perf, val_ds.map(lambda x, y: y), fold_var)\n",
    "\n",
    "        ### Grad-CAM analysis ###\n",
    "        # Train dataset\n",
    "        train_gradcam_path = f\"{save_dir}/train_gradcam_heatmaps_{fold_var}\"\n",
    "        os.makedirs(train_gradcam_path, exist_ok=True)\n",
    "        gradcam_train_ds_norm = utils.config_performance(train_ds_norm, args, flag=True)\n",
    "        utils.grad_cam(catNet.model, train_ds, train_ds_norm, catNet.model.predict(gradcam_train_ds_norm), train_gradcam_path, args)\n",
    "        # Validation dataset\n",
    "        gradcam_path = f\"{save_dir}/gradcam_heatmaps_{fold_var}\"\n",
    "        os.makedirs(gradcam_path, exist_ok=True)\n",
    "        utils.grad_cam(catNet.model, val_ds, val_ds_norm, catNet.model.predict(val_ds_perf), gradcam_path, args)\n",
    "\n",
    "        # Reset model and clear session\n",
    "        tf.keras.backend.clear_session()\n",
    "        catNet.__reset()\n",
    "        fold_var += 1\n",
    "\n",
    "    # Save the values of each fold\n",
    "    catNet.save_metrics()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_cv_ft(args)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "EG-uvLsngAPX",
    "sUEMhiDjZ9bi",
    "-V1lTSzbDUyS",
    "ajM8QKP5EIMU",
    "GPP7a2XY3Atf",
    "9ht1Z-KOohzD"
   ],
   "name": "TC_Category_Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
