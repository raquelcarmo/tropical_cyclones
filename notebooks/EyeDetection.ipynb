{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVxTZjRBer5W"
   },
   "source": [
    "# Tropical Cyclones Eye Detection\n",
    "Script to train Deep Learning models to identify the presence/abscence of the TC eye in an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "He7AKLXqgKlx"
   },
   "source": [
    "## Imports and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your desired path to work on\n",
    "import os\n",
    "os.chdir('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell once per session. This cell links the code folder to the python exectution path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where the modules are stored\n",
    "import sys\n",
    "sys.path.append('../code')\n",
    "\n",
    "# Import modules\n",
    "import utils\n",
    "from models import DetectionCNN\n",
    "from data_process import DataProcessor\n",
    "from visualization import plot_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell allows Jupyter Notebooks to detect changes in external code and to automatically update it without restarting the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZu5fkuXWxdC"
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.applications import resnet50, mobilenet_v2\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import concatenate, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, Precision, Recall, TruePositives, FalsePositives, TrueNegatives, FalseNegatives\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUEMhiDjZ9bi",
    "tags": []
   },
   "source": [
    "## 1. Train on data according to csv split into train, validation and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.1. Define settings (arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'main_dir': 'VV_VH_WS',\n",
       " 'cnn': 'ResNet',\n",
       " 'loss': 'binary_crossentropy',\n",
       " 'height': 700,\n",
       " 'width': 400,\n",
       " 'numerical_vars': False,\n",
       " 'normalise': True,\n",
       " 'norm_mode': 'model',\n",
       " 'rotate': False,\n",
       " 'crop': True,\n",
       " 'crop_mode': 'uniform',\n",
       " 'nb_crops': 1,\n",
       " 'batch_size': 8,\n",
       " 'buffer_size': 100,\n",
       " 'epochs': 10,\n",
       " 'learning_rate': 0.0001,\n",
       " 'save_dir': 'VV_VH_WS\\\\results\\\\id\\\\R_700x400_nM_bs8_bf100_e10_lr0001'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = {\n",
    "    'main_dir':       \"VV_VH_WS\",\n",
    "    'cnn':            \"ResNet\",     # choices: [\"ResNet\", \"Mobile\"]\n",
    "    'loss':           \"binary_crossentropy\",\n",
    "    'height':         700,\n",
    "    'width':          400,\n",
    "    'numerical_vars': False,\n",
    "    'normalise':      True,\n",
    "    'norm_mode':      \"model\",     # choices=['z-norm', 'model', 'simple', 'none']\n",
    "    'rotate':         False,\n",
    "    'crop':           True,\n",
    "    'crop_mode':      \"uniform\",   # choices=['uniform', 'weighted']\n",
    "    'nb_crops':       1,\n",
    "    'batch_size':     8,\n",
    "    'buffer_size':    100,\n",
    "    'epochs':         10,\n",
    "    'learning_rate':  0.0001,\n",
    "    'nb_splits':      5\n",
    "}\n",
    "args['save_dir'] = os.path.join(args['main_dir'], \"results\", \"id\", \"R_700x400_nM_bs8_bf100_e10_lr0001\")\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.2. Prepare the tf.data.Dataset instances to be fed to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySpk4sOVemVz"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "main_dir = args['main_dir']\n",
    "train_images, train_labels, train_bbox = utils.load_data(args, f\"{main_dir}/csv/training.csv\")\n",
    "val_images, val_labels, val_bbox = utils.load_data(args, f\"{main_dir}/csv/val.csv\")\n",
    "test_images, test_labels, test_bbox = utils.load_data(args, f\"{main_dir}/csv/test.csv\")\n",
    "\n",
    "# Create an instance of the DataProcessor\n",
    "p = DataProcessor(args,\n",
    "                  plot_light = False,          # plot only select_crop() images\n",
    "                  plot_extensive = False,      # plot extensively all images\n",
    "                  show_prints = False\n",
    "                 )\n",
    "\n",
    "# Generate datasets\n",
    "# NOTE: utils.create_dataset() allows for further data augmentation\n",
    "train_ds = utils.prepare_dataset(p, train_images, train_labels, train_bbox)\n",
    "val_ds = utils.prepare_dataset(p, val_images, val_labels, val_bbox)\n",
    "test_ds = utils.prepare_dataset(p, test_images, test_labels, test_bbox)\n",
    "\n",
    "# Perform normalization\n",
    "train_ds_norm, val_ds_norm = utils.normalisation(train_ds, val_ds, args)\n",
    "_, test_ds_norm = utils.normalisation(train_ds, test_ds, args)\n",
    "\n",
    "# Configure for performance\n",
    "train_dataset = utils.config_performance(train_ds_norm, args, shuffle=True)\n",
    "val_dataset = utils.config_performance(val_ds_norm, args, flag=True)\n",
    "test_dataset = utils.config_performance(test_ds_norm, args, flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9-tkgGrid0-",
    "tags": []
   },
   "source": [
    "### 1.3. Perform end-to-end training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0eTiiKghBZg"
   },
   "outputs": [],
   "source": [
    "# Directory to save results\n",
    "save_dir = args['save_dir']\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Create model\n",
    "model = DetectionCNN(args)\n",
    "\n",
    "# Create callbacks\n",
    "dt = datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\")\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint(os.path.join(save_dir, \"best_model.h5\"), verbose=1, save_best_only=True),\n",
    "    TensorBoard(log_dir=f\"{save_dir}/logs/{dt}\")\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x = train_dataset,\n",
    "    steps_per_epoch = len(train_dataset),\n",
    "    validation_data = val_dataset,\n",
    "    validation_steps = len(val_dataset),\n",
    "    epochs = args['epochs'],\n",
    "    callbacks = callbacks,\n",
    "    verbose = 1,\n",
    "    class_weight = {0:1.6, 1:1},\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the TensorBoard notebook extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0wRibZ1YUh-"
   },
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir VV_VH_WS/results/id/R_700x400_nM_bs8_bf100_e10_lr0001/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBRY3vfnhF-D"
   },
   "outputs": [],
   "source": [
    "print(\"Loading best weights from training...\")\n",
    "model.load_weights(os.path.join(save_dir, \"best_model.h5\"))\n",
    "\n",
    "results = model.evaluate(\n",
    "    test_dataset, \n",
    "    steps = len(test_dataset),\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kGg9tMhEhKut"
   },
   "outputs": [],
   "source": [
    "# Retrieve a batch of images from the test set\n",
    "predictions = model.predict(test_dataset)\n",
    "predictions = tf.where(predictions < 0.5, 0, 1)\n",
    "\n",
    "print('Predictions:\\n', predictions.numpy())\n",
    "print('Labels:\\n')\n",
    "test_labels = test_dataset.map(lambda x, y: y)\n",
    "for label in test_labels:\n",
    "    print(label)\n",
    "# class_names = {0: \"No eye\", 1: \"Eye\"}\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for i in range(9):\n",
    "#     ax = plt.subplot(3, 3, i + 1)\n",
    "#     plt.imshow(image_batch[i].astype(\"uint8\"))\n",
    "#     plt.title(class_names[predictions[i]])\n",
    "#     plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-V1lTSzbDUyS"
   },
   "source": [
    "## 2. Train on data using the Stratified K-Fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Perform Stratified 5-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NxvI67fsr2zD"
   },
   "outputs": [],
   "source": [
    "def stratified_cv(args):\n",
    "    dataset_path = f\"{args['main_dir']}/csv/full_dataset.csv\"\n",
    "    df = pd.read_csv(dataset_path, converters={'bbox_shape': eval}).dropna()\n",
    "    #print(f\"Dataset dimension: {len(df)}\")\n",
    "    Y = df[\"label\"]\n",
    "\n",
    "    # Create an instance of the DataProcessor\n",
    "    p = DataProcessor(args,\n",
    "                      plot_light = False,              # plot only select_crop() images\n",
    "                      plot_extensive = False,          # plot extensively all images\n",
    "                      show_prints = False\n",
    "                     )\n",
    "    \n",
    "    # Create an instance of the model\n",
    "    model = DetectionCNN(args)\n",
    "    \n",
    "    print(\"Entering in K-fold Cross Validation...\")\n",
    "    stratified_k_fold = StratifiedKFold(n_splits=args['nb_splits'], random_state=42, shuffle=False)\n",
    "    fold_var = 1\n",
    "    \n",
    "    for train_index, val_index in stratified_k_fold.split(np.zeros(len(df)), Y):\n",
    "        training_data = df.iloc[train_index]\n",
    "        validation_data = df.iloc[val_index]\n",
    "\n",
    "        # Load data\n",
    "        train_images, train_labels, train_bbox = utils.load_data(args, df=training_data)\n",
    "        val_images, val_labels, val_bbox = utils.load_data(args, df=validation_data)\n",
    "\n",
    "        # Generate datasets\n",
    "        train_ds = utils.create_dataset(p, train_images, train_labels, train_bbox, args)\n",
    "        val_ds = utils.create_dataset(p, val_images, val_labels, val_bbox, args, flag=True)\n",
    "\n",
    "        # Perform normalisation\n",
    "        train_ds_norm, val_ds_norm = utils.normalisation(train_ds, val_ds, args)\n",
    "        \n",
    "        # Configure for performance\n",
    "        train_ds_perf = utils.config_performance(train_ds_norm, args, shuffle=True)\n",
    "        val_ds_perf = utils.config_performance(val_ds_norm, args, flag=True)\n",
    "\n",
    "        # Train the model\n",
    "        \"\"\"\n",
    "        # multi GPU strategy\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "        with strategy.scope():\n",
    "            model = DetectionCNN(args)\n",
    "        \"\"\"\n",
    "        history = model.train(train_ds_perf, val_ds_perf, fold_var)\n",
    "        plot_history(history, fold_var, save_dir)\n",
    "        #print(history)\n",
    "\n",
    "        # Guarantee time for weights to be saved and loaded again\n",
    "        time.sleep(10)\n",
    "\n",
    "        # Load best model & predict\n",
    "        print(\"Loading best weights from training...\")\n",
    "        model.get_eval(val_ds_perf, fold_var)\n",
    "        model.get_preds(val_ds_perf, val_labels, fold_var)\n",
    "\n",
    "        # Reset model and clear session\n",
    "        tf.keras.backend.clear_session()\n",
    "        model.__reset()\n",
    "        fold_var += 1\n",
    "\n",
    "    # Save the values of each fold\n",
    "    model.save_metrics()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_cv(args)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "sUEMhiDjZ9bi"
   ],
   "name": "TC_Eye_Detection.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
