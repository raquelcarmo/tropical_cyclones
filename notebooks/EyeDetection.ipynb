{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVxTZjRBer5W"
   },
   "source": [
    "# Tropical Cyclones Eye Detection\n",
    "Script to train Deep Learning models to identify the presence/abscence of the TC eye in an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "He7AKLXqgKlx"
   },
   "source": [
    "## Imports and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jnxs1L1PWjeL"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your desired path to work on\n",
    "import os\n",
    "os.chdir('/content/drive/MyDrive/ESRIN_PhiLab/Tropical_Cyclones/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell once per session. This cell links the code folder to the python exectution path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where the modules are stored\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/ESRIN_PhiLab/Tropical_Cyclones/tropical_cyclones/src/code')\n",
    "\n",
    "# Import modules\n",
    "import utils\n",
    "from models import DetectionCNN\n",
    "from data_process import DataProcessor\n",
    "from visualization import plot_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell allows Google Colab/Jupyter Notebooks to detect changes in external code and to automatically update it without restarting the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZu5fkuXWxdC"
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.applications import resnet50, mobilenet_v2\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import concatenate, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, Precision, Recall, TruePositives, FalsePositives, TrueNegatives, FalseNegatives\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUEMhiDjZ9bi",
    "tags": []
   },
   "source": [
    "## 1. Train on data according to csv split into train, validation and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.1. Define settings (arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'main_dir':       \"SAR_swath_images_VV+VH+WS\",\n",
    "    'save_dir':       os.path.join(main_dir, \"id_results\", \"R_700x400_nM_bs8_bf100_e10_lr0001\"),\n",
    "    'cnn':            \"ResNet\",     # choices: [\"ResNet\", \"Mobile\"]\n",
    "    'loss':           \"binary_crossentropy\",\n",
    "    'height':         700,\n",
    "    'width':          400,\n",
    "    'numerical_vars': False,\n",
    "    'normalise':      True,\n",
    "    'norm_mode':      \"model\",     # choices=['z-norm', 'model', 'simple', 'none']\n",
    "    'rotate':         False,\n",
    "    'crop':           True,\n",
    "    'crop_mode':      \"uniform\",   # choices=['uniform', 'weighted']\n",
    "    'nb_crops':       1,\n",
    "    'batch_size':     8,\n",
    "    'buffer_size':    100,\n",
    "    'epochs':         10,\n",
    "    'learning_rate':  0.0001\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.2. Prepare the tf.data.Dataset instances to be fed to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySpk4sOVemVz"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "main_dir = args['main_dir']\n",
    "train_images, train_labels, train_bbox = utils.load_data(\"{}/csv/training.csv\".format(main_dir), args)\n",
    "val_images, val_labels, val_bbox = utils.load_data(\"{}/csv/val.csv\".format(main_dir), args)\n",
    "test_images, test_labels, test_bbox = utils.load_data(\"{}/csv/test.csv\".format(main_dir), args)\n",
    "\n",
    "# Create an instance of the DataProcessor\n",
    "p = DataProcessor(args,\n",
    "                  plot_light = False,          # plot only select_crop() images\n",
    "                  plot_extensive = False,      # plot extensively all images\n",
    "                  show_prints = False\n",
    "                 )\n",
    "\n",
    "# Generate datasets\n",
    "train_ds = utils.prepare_dataset(p, train_images, train_labels, train_bbox)\n",
    "val_ds = utils.prepare_dataset(p, val_images, val_labels, val_bbox)\n",
    "test_ds = utils.prepare_dataset(p, test_images, test_labels, test_bbox)\n",
    "\n",
    "# Perform normalization\n",
    "train_ds_norm, val_ds_norm = utils.normalisation(train_ds, val_ds, args)\n",
    "_, test_ds_norm = utils.normalisation(train_ds, test_ds, args)\n",
    "\n",
    "# Configure for performance\n",
    "train_dataset = utils.config_performance(train_ds_norm, args, shuffle=True)\n",
    "val_dataset = utils.config_performance(val_ds_norm, args)\n",
    "test_dataset = utils.config_performance(test_ds_norm, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9-tkgGrid0-",
    "tags": []
   },
   "source": [
    "### 1.3. Perform end-to-end training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0eTiiKghBZg"
   },
   "outputs": [],
   "source": [
    "# Directory to save results\n",
    "save_dir = args['save_dir']\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Create model\n",
    "model = DetectionCNN(args)\n",
    "\n",
    "# Create callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint(os.path.join(save_dir, \"best_model.h5\"), verbose=1, save_best_only=True),\n",
    "    TensorBoard(log_dir=os.path.join(save_dir, \"logs\", datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\")))\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x = train_dataset,\n",
    "    steps_per_epoch = len(train_dataset),\n",
    "    validation_data = val_dataset,\n",
    "    validation_steps = len(val_dataset),\n",
    "    epochs = args['epochs'],\n",
    "    callbacks = callbacks,\n",
    "    verbose = 1,\n",
    "    class_weight = {0:1.6, 1:1},\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0wRibZ1YUh-"
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "#%load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir SAR_swath_images_VV+VH+WS/id_results/R_700x400_nM_bs8_bf100_e10_lr0001/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBRY3vfnhF-D"
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "print(\"Loaded best weights of the training\")\n",
    "model.load_weights(os.path.join(save_dir, \"best_model.h5\"))\n",
    "\n",
    "results = model.evaluate(\n",
    "    test_dataset, \n",
    "    steps = len(test_dataset),\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kGg9tMhEhKut"
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "# Retrieve a batch of images from the test set\n",
    "predictions = model.predict(test_dataset)\n",
    "predictions = tf.where(predictions < 0.5, 0, 1)\n",
    "\n",
    "print('Predictions:\\n', predictions.numpy())\n",
    "print('Labels:\\n')\n",
    "for label in test_labels_dataset:\n",
    "    print(label)\n",
    "#class_names = {0: \"No eye\", 1: \"Eye\"}\n",
    "#plt.figure(figsize=(10, 10))\n",
    "#for i in range(9):\n",
    "#  ax = plt.subplot(3, 3, i + 1)\n",
    "#  plt.imshow(image_batch[i].astype(\"uint8\"))\n",
    "#  plt.title(class_names[predictions[i]])\n",
    "#  plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-V1lTSzbDUyS"
   },
   "source": [
    "## 2. Train on data using the Stratified K-Fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1. Define settings (arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'main_dir':       \"SAR_swath_images_VV+VH+WS\",\n",
    "    'save_dir':       os.path.join(main_dir, \"id_results\", \"R_700x400_nM_bs8_bf100_e10_lr0001\"),\n",
    "    'cnn':            \"ResNet\",     # choices: [\"ResNet\", \"Mobile\"]\n",
    "    'loss':           \"binary_crossentropy\",\n",
    "    'height':         700,\n",
    "    'width':          400,\n",
    "    'numerical_vars': False,\n",
    "    'normalise':      True,\n",
    "    'norm_mode':      \"model\",     # choices=['z-norm', 'model', 'simple', 'none']\n",
    "    'rotate':         False,\n",
    "    'crop':           True,\n",
    "    'crop_mode':      \"uniform\",   # choices=['uniform', 'weighted']\n",
    "    'nb_crops':       1,\n",
    "    'batch_size':     8,\n",
    "    'buffer_size':    100,\n",
    "    'epochs':         20,\n",
    "    'learning_rate':  0.0001,\n",
    "    'nb_splits':      5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Perform Stratified 5-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NxvI67fsr2zD"
   },
   "outputs": [],
   "source": [
    "def stratified_cv(args):\n",
    "    dataset_path = \"{}/csv/full_dataset.csv\".format(args['main_dir'])\n",
    "    df = pd.read_csv(dataset_path, converters={'bbox_shape': eval}).dropna()\n",
    "    #print(\"Dataset dimension: {}\".format(len(df)))\n",
    "    Y = df[\"label\"]\n",
    "\n",
    "    # Create an instance of the DataProcessor\n",
    "    p = DataProcessor(args,\n",
    "                      plot_light = False,              # plot only select_crop() images\n",
    "                      plot_extensive = False,          # plot extensively all images\n",
    "                      show_prints = False\n",
    "                     )\n",
    "    \n",
    "    # Create an instance of the model\n",
    "    model = DetectionCNN(args)\n",
    "    \n",
    "    print(\"Entering in K-fold Cross Validation...\")\n",
    "    stratified_k_fold = StratifiedKFold(n_splits=args['nb_splits'], random_state=42, shuffle=False)\n",
    "    fold_var = 1\n",
    "    \n",
    "    for train_index, val_index in stratified_k_fold.split(np.zeros(len(df)), Y):\n",
    "        training_data = df.iloc[train_index]\n",
    "        validation_data = df.iloc[val_index]\n",
    "\n",
    "        # Load data\n",
    "        train_images, train_labels, train_bbox = utils.load_data(df = training_data, args)\n",
    "        val_images, val_labels, val_bbox = utils.load_data(df = validation_data, args)\n",
    "\n",
    "        # Generate datasets\n",
    "        train_ds = utils.create_dataset(p, train_images, train_labels, train_bbox, args)\n",
    "        val_ds = utils.create_dataset(p, val_images, val_labels, val_bbox, args)\n",
    "\n",
    "        # Perform normalisation\n",
    "        train_ds_norm, val_ds_norm = utils.normalisation(train_ds, val_ds, args)\n",
    "        \n",
    "        # Configure for performance\n",
    "        train_dataset = utils.config_performance(train_ds_norm, args, shuffle=True)\n",
    "        val_dataset = utils.config_performance(val_ds_norm, args)\n",
    "\n",
    "        # Train the model\n",
    "        \"\"\"\n",
    "        # multi GPU strategy\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "        with strategy.scope():\n",
    "            model = DetectionCNN(args)\n",
    "        \"\"\"\n",
    "        history = model.train(train_ds_perf, val_ds_perf, fold_var)\n",
    "\n",
    "        # Plot history\n",
    "        plot_history(history, fold_var, save_dir)\n",
    "        #print(history)\n",
    "\n",
    "        # Guarantee time for weights to be saved and loaded again\n",
    "        time.sleep(10)\n",
    "\n",
    "        # Load best model\n",
    "        print(\"Loading best weights from training...\")\n",
    "        model.get_eval(val_ds_perf, fold_var)\n",
    "        model.get_preds(val_ds_perf, val_labels, fold_var)\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        model.__reset()\n",
    "        fold_var += 1\n",
    "\n",
    "    # Save the values of each fold\n",
    "    model.save_metrics()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_cv(args)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "sUEMhiDjZ9bi"
   ],
   "name": "TC_Eye_Detection.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
