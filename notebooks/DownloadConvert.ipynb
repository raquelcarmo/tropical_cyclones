{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0uQMbru4Gy9"
   },
   "source": [
    "# Tropical Cyclones Data Download and Convertion\n",
    "\n",
    "This script downloads the SMOS/SMAP/SAR data from the [Cyclobs API](https://cyclobs.ifremer.fr/app/docs/) and converts the resulting .nc files into .png files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PaVuZ_RgZmY"
   },
   "source": [
    "## Imports and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7oC6PTgdZX2"
   },
   "outputs": [],
   "source": [
    "# Insert your desired path to work on\n",
    "import os\n",
    "os.chdir('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NjRUdQbjica"
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import netCDF4\n",
    "from glob import glob\n",
    "import cv2\n",
    "#from google.colab.patches import cv2_imshow\n",
    "from datetime import datetime\n",
    "import re\n",
    "import random\n",
    "\n",
    "# import rasterio as rio\n",
    "# import rioxarray # geospatial extension for xarray\n",
    "# import bokeh.io\n",
    "# import cartopy\n",
    "# import cartopy.crs as ccrs\n",
    "# import geoviews as gv\n",
    "# import geoviews.feature as gf\n",
    "\n",
    "# bokeh.io.output_notebook()\n",
    "# gv.extension('bokeh','matplotlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set folder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'in_nc': 'nc',\n",
    "    # Define the SAR features to stack along the 3rd-axis and call the folder according to their siglas\n",
    "    'out_feats': 'VV_VH_WS',\n",
    "    'out_archer': 'link_ARCHER_IFREMER'\n",
    "}\n",
    "\n",
    "# List comprehension for the folder structure code\n",
    "[os.makedirs(val) for key, val in config.items() if not os.path.exists(val)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzs88t_ymkwf",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. Download the .nc products from the Cyclobs API\n",
    "The \"request_url\" has to be changed according to the category, mission (Sentinel-1 A and B / SMOS / SMAP) and product type (swath) to be downloaded, as follows:\n",
    "* **Category 1**: request_url=\"https://cyclobs.ifremer.fr/app/api/getData?cat_min=cat-1&cat_max=cat-2&mission=S1B,S1A&product_type=swath&include_cols=all\"\n",
    "* **Category 2**: request_url=\"https://cyclobs.ifremer.fr/app/api/getData?cat_min=cat-2&cat_max=cat-3&mission=S1B,S1A&product_type=swath&include_cols=all\"\n",
    "* **Category 3**: request_url=\"https://cyclobs.ifremer.fr/app/api/getData?cat_min=cat-3&cat_max=cat-4&mission=S1B,S1A&product_type=swath&include_cols=all\"\n",
    "* **Category 4**: request_url=\"https://cyclobs.ifremer.fr/app/api/getData?cat_min=cat-4&cat_max=cat-5&mission=S1B,S1A&product_type=swath&include_cols=all\"\n",
    "* **Category 5**: request_url=\"https://cyclobs.ifremer.fr/app/api/getData?cat_min=cat-5&mission=S1B,S1A&product_type=swath&include_cols=all\"\n",
    "\n",
    "Please refer to https://cyclobs.ifremer.fr/app/docs/getData.html for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlFB4WXadK9X"
   },
   "outputs": [],
   "source": [
    "# Download path\n",
    "download_path = f\"{config['in_nc']}/category5\"\n",
    "os.makedirs(download_path, exist_ok=True)\n",
    "\n",
    "# Make the resquest using cyclobs API and store the result in a pandas dataframe\n",
    "#request_url=\"https://cyclobs.ifremer.fr/app/api/getData?cat_min=cat-4&cat_max=cat-5&mission=S1B,S1A&product_type=swath&include_cols=all\"\n",
    "request_url=\"https://cyclobs.ifremer.fr/app/api/getData?cat_min=cat-5&mission=S1B,S1A&product_type=swath&include_cols=all\"\n",
    "df_request = pd.read_csv(request_url)\n",
    "\n",
    "# Add download path\n",
    "df_request['path'] = df_request['data_url'].map(lambda x : os.path.join(download_path,os.path.basename(x)))\n",
    "#df_request\n",
    "\n",
    "# Download 'data_url' to 'path' with wget, and read files\n",
    "projection=ccrs.Mercator()\n",
    "datasets = []\n",
    "for idx,entry in tqdm(df_request.iterrows(), total=df_request.shape[0]):\n",
    "    ret = os.system('cd %s ; wget -N  %s' % (os.path.dirname(entry['path']),entry['data_url']))\n",
    "\n",
    "    if ret == 0 : \n",
    "        ds = xr.open_dataset(entry['path'])\n",
    "        datasets.append(ds)\n",
    "        #datasets.append(ds.rio.reproject(projection.proj4_params))\n",
    "    else:\n",
    "        datasets.append(None) # error fetching file\n",
    "\n",
    "#print(datasets)\n",
    "df_request['dataset'] = datasets\n",
    "\n",
    "\"\"\"\n",
    "df_request['dataset'].iloc[0]['wind_speed']\n",
    "\n",
    "gv_list=[gf.coastline.opts(projection=projection)]\n",
    "for ds in df_request['dataset']:\n",
    "    print(ds)\n",
    "    gv_list.append(gv.Image(ds['wind_speed'].squeeze()[::5,::5],crs=projection).opts(cmap='jet',tools=['hover']))\n",
    "    \n",
    "gv.Overlay(gv_list).options(width=800, height=500)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-bUNkg-Zn2C",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.1. Download data_url, sid and name of each Tropical Cyclone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akpdtj2JZjy6"
   },
   "outputs": [],
   "source": [
    "download_path = \"best_track\"\n",
    "os.makedirs(download_path, exist_ok=True)\n",
    "\n",
    "request_url = \"https://cyclobs.ifremer.fr/app/api/getData?cat_min=cat-5&mission=SMAP,SMOS,S1B,S1A&include_cols=all\"\n",
    "df_request = pd.read_csv(request_url)\n",
    "\n",
    "# Add download path\n",
    "df_request['path'] = df_request['data_url'].map(lambda x : os.path.join(download_path,os.path.basename(x)))\n",
    "\n",
    "info = pd.DataFrame(data = {'data_url': df_request['data_url'],\n",
    "                            'sid': df_request[\"sid\"],\n",
    "                            'TC_name': df_request[\"cyclone_name\"]}\n",
    "                                     )\n",
    "info[\"data_url\"] = info[\"data_url\"].apply(lambda x: os.path.basename(x))\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.to_csv(f'{download_path}/Cyclobs_info_names.csv', index=False, header=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScgK25TlVd2-"
   },
   "source": [
    "## 2. Save the features in the .nc product as an image (3 channels = 3 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeature(infoImage, feat, mask=None, mask_dilated=None):\n",
    "    feature = infoImage.variables[feat][:]\n",
    "\n",
    "    if mask is not None:\n",
    "        # Mask out the land values\n",
    "        if mask_dilated is None:\n",
    "            feature[0][mask[0] != 0] = 0\n",
    "            #feature[0][mask[0] == 1] = 1  # if we want land==1\n",
    "        else:\n",
    "            feature[0][mask_dilated != 0] = 0\n",
    "            \n",
    "    feature = (feature[0] - np.min(feature[0]))/(np.max(feature[0]) - np.min(feature[0]))\n",
    "    #print(\"Feature '{}' normalised: max={}, min={}\".format(feat, np.max(feature), np.min(feature)))\n",
    "    #plt.imshow(feature)\n",
    "    #plt.show()\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDV-bjPaVaoL"
   },
   "outputs": [],
   "source": [
    "with_land_mask = True\n",
    "dilate_mask = False\n",
    "categories = [\"category1\", \"category2\", \"category3\", \"category4\", \"category5\"]\n",
    "\n",
    "for category in categories:\n",
    "    # Directory where .nc files are\n",
    "    saved_dir = f\"{config['in_nc']}/{category}\"\n",
    "    nc_list = glob(f\"{saved_dir}/*.nc\")\n",
    "    \n",
    "    # Directory where corresponding images will be saved\n",
    "    images_dir = f\"{config['out_feats']}/{category}\"\n",
    "    #images_dir = \"Vmax/{}\".format(category)\n",
    "    #images_dir = \"masks/{}\".format(category)\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "    #lon_dir = f\"{images_dir}/lon\"\n",
    "    #os.makedirs(lon_dir, exist_ok=True)\n",
    "    #lat_dir = f\"{images_dir}/lat\"\n",
    "    #os.makedirs(lat_dir, exist_ok=True)\n",
    "\n",
    "    count=0\n",
    "    for nc_image in nc_list:\n",
    "        # Read .nc product\n",
    "        full_info_image = netCDF4.Dataset(nc_image, mode='r') \n",
    "\n",
    "        try:\n",
    "            count += 1\n",
    "            # Extract the information of the features wanted\n",
    "            if with_land_mask:\n",
    "                # Extract mask flag\n",
    "                mask = full_info_image.variables[\"mask_flag\"][:]\n",
    "                #print(np.unique(mask[0]))\n",
    "                #plt.imshow(mask[0])\n",
    "                #plt.show()\n",
    "                \n",
    "                if dilate_mask:\n",
    "                    mask[mask != 0] = 1\n",
    "                    kernel = np.ones((11, 11), np.int8)\n",
    "                    mask_dilated = cv2.dilate(mask[0], kernel, iterations=1)\n",
    "                else:\n",
    "                    mask_dilated = None\n",
    "\n",
    "            # Co-polarization (VV) \n",
    "            feature_co = extractFeature(full_info_image, \"nrcs_detrend_co\", mask, mask_dilated)\n",
    "\n",
    "            # Cross-polarization (VH)\n",
    "            feature_cross = extractFeature(full_info_image, \"nrcs_detrend_cross\", mask, mask_dilated)\n",
    "\n",
    "            # Wind Speed (WS)\n",
    "            feature_wind = extractFeature(full_info_image, \"wind_speed\", mask, mask_dilated)\n",
    "\n",
    "            # Wind Streaks Orientation (WSO) (sinWSO and cosWSO)\n",
    "            feature_wso = full_info_image.variables[\"wind_streaks_orientation\"][:]\n",
    "            #print(np.nanmax(feature_wso[0]), np.nanmin(feature_wso[0]))\n",
    "            if np.isnan(feature_wso).any():\n",
    "                count +=1\n",
    "            feature_wso = np.nan_to_num(feature_wso[0])\n",
    "            #print(\"feature_wso\")\n",
    "            #plt.imshow(feature_wso[0])\n",
    "            #plt.show()            \n",
    "            \n",
    "            sin_feature_wso = np.sin(feature_wso * np.pi/180.)\n",
    "            # Scale values from -1 to 1\n",
    "            sin_feature_wso = (sin_feature_wso - np.min(sin_feature_wso))/(np.max(sin_feature_wso) - np.min(sin_feature_wso))\n",
    "            \n",
    "            cos_feature_wso = np.cos(feature_wso * np.pi/180.)\n",
    "            # Scale values from -1 to 1\n",
    "            cos_feature_wso = (cos_feature_wso - np.min(cos_feature_wso))/(np.max(cos_feature_wso) - np.min(cos_feature_wso))\n",
    "\n",
    "            # Extract longitude and latitude features\n",
    "            #feature_lon = full_info_image.variables[\"lon\"][:]\n",
    "            #feature_lat = full_info_image.variables[\"lat\"][:]\n",
    "\n",
    "            # Extract time registered\n",
    "            tmax_units = datetime.strptime(full_info_image.measurementDate, '%Y-%m-%dT%H:%M:%SZ')\n",
    "            tmax_units = tmax_units.strftime('%Y-%m-%d %H_%M_%S')\n",
    "\n",
    "            # Stack matrices along 3rd axis (depth-wise)\n",
    "            output_image = np.dstack((feature_co, feature_cross, feature_wind))  # VV+VH+WS\n",
    "            #output_image = np.dstack((feature_wind[0], np.sin(feature_wso[0] * np.pi/180.), np.cos(feature_wso[0] * np.pi/180.)))\n",
    "\n",
    "            if (output_image < 0).any():\n",
    "                output_image = np.clip(output_image, a_min=0, a_max=None)\n",
    "            #print(\"First output image:\")\n",
    "            #print('Shape:', output_image.shape)\n",
    "            #print(\"Normalised: max={}, min={}\".format(np.max(output_image), np.min(output_image)))\n",
    "            #print(\"Output without NaNs: max={}, min={}\".format(np.nanmax(output_image), np.nanmin(output_image)))\n",
    "            #plt.imshow(output_image)\n",
    "            #plt.show()\n",
    "\n",
    "            full_info_image.close()\n",
    "        except KeyError as err:\n",
    "            # Creating KeyError instance for book keeping\n",
    "            print(\"Error:\", err)\n",
    "            full_info_image.close()\n",
    "            continue\n",
    "\n",
    "        # Save information extracted\n",
    "        plt.imsave(\"{}/{}.png\".format(images_dir, tmax_units), output_image, format='png')\n",
    "        #np.save(\"{}/{}_mask.npy\".format(images_dir, tmax_units), np.array(mask[0]))\n",
    "        #np.save(\"{}/{}_Vmax.npy\".format(images_dir, tmax_units), np.max(feature_wind[0]))\n",
    "        #np.save(\"{}/{}.npy\".format(lon_dir, tmax_units), np.mean(feature_lon[0]))\n",
    "        #np.save(\"{}/{}.npy\".format(lat_dir, tmax_units), np.mean(feature_lat[0]))\n",
    "    print('Number of products with NaNs in WSO:', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRfjWIAmi3cK"
   },
   "source": [
    "### 2.2. Delete doubled files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "982q-y86fr4z"
   },
   "outputs": [],
   "source": [
    "for cat in range(5, 1, -1):\n",
    "    print(f\"Category {cat-1}:\")\n",
    "    files_toKeep = glob(f\"{config['out_feats']}/category{cat}/*.png\")\n",
    "\n",
    "    folder_toDel = f\"{config['out_feats']}/category{cat-1}\"\n",
    "    files_toDel = glob(f\"{config['out_feats']}/category{cat-1}/*.png\")\n",
    "\n",
    "    for f in files_toKeep:\n",
    "        f = f.split(os.sep)[-1]\n",
    "        for s in files_toDel:\n",
    "            s = s.split(os.sep)[-1]\n",
    "            if f == s:\n",
    "              print('File removed:', s)\n",
    "              os.remove(f\"{config['out_feats']}/category{cat-1}/{s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEHpWS_ztZdb"
   },
   "outputs": [],
   "source": [
    "source_files = glob(f\"{config['out_feats']}/category5/*.png\")\n",
    "len(source_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygyOuX_Abug0"
   },
   "source": [
    "## 3. Split data between train, validation and test csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VDlmC0MOCITr"
   },
   "outputs": [],
   "source": [
    "category = [\"category1\", \"category2\", \"category3\", \"category4\", \"category5\"]\n",
    "labels_path = \"labels/via_project_11Jan2021_12h22m_csv.csv\"\n",
    "\n",
    "df = pd.read_csv(labels_path)\n",
    "cols = df.columns.tolist()\n",
    "for idx, row in df.iterrows():\n",
    "    # Retrieve the coordinates of the bounding boxes\n",
    "    df['region_shape_attributes'][idx] = re.findall(r'\\d+', df['region_shape_attributes'][idx])\n",
    "\n",
    "# Use a seed so to obtain the same splitting every time\n",
    "random.seed(20)\n",
    "\n",
    "df_train, df_val, df_test, df_full = [], [], [], []\n",
    "for cat in category:\n",
    "    dir = f\"{config['out_feats']}/{cat}\"\n",
    "    lon_dir = f\"{dir}/lon\"\n",
    "    lat_dir = f\"{dir}/lat\"\n",
    "\n",
    "    image_list = glob(f'{dir}/*.png')\n",
    "\n",
    "    for image_path in image_list:\n",
    "        #print(image_path)\n",
    "        image_name = os.path.basename(image_path)\n",
    "\n",
    "        lon_list = glob('{}/{}.npy'.format(lon_dir, image_name.split('.')[0]))\n",
    "        lat_list = glob('{}/{}.npy'.format(lat_dir, image_name.split('.')[0]))\n",
    "\n",
    "        lon = lon_list[0] if lon_list != [] else None\n",
    "        lat = lat_list[0] if lat_list != [] else None\n",
    "\n",
    "        aux = [image_path]\n",
    "        aux.extend([lat, lon])\n",
    "        #print(df[df.filename == image_name])\n",
    "        index = df[df.filename == image_name].index[0]\n",
    "        bb_present = 1 if df['region_count'][index] == 1 else 0\n",
    "        aux.append(bb_present)\n",
    "        aux.append(df['region_shape_attributes'][index])\n",
    "\n",
    "        df_full.append(aux)\n",
    "        rand = random.randint(0, 100)\n",
    "        # Split used: 60, 20, 20\n",
    "        if rand < 60:\n",
    "            df_train.append(aux)\n",
    "        elif rand < 80:\n",
    "            df_val.append(aux)\n",
    "        else:\n",
    "            df_test.append(aux) \n",
    "\n",
    "col_names = ['image', 'lat', 'lon', 'label', 'bbox_shape']\n",
    "df_train = pd.DataFrame(df_train, columns = col_names)\n",
    "df_val = pd.DataFrame(df_val, columns = col_names)\n",
    "df_test = pd.DataFrame(df_test, columns = col_names)\n",
    "df_full = pd.DataFrame(df_full, columns = col_names)\n",
    "\n",
    "if (df_full['lat'].values == None).all() and (df_full['lon'].values == None).all():\n",
    "    #print(\"Warning: lat and lon variables not considered.\")\n",
    "    df_full.drop(columns=['lat', 'lon'], inplace=True)\n",
    "\n",
    "# Create path to save the csv files\n",
    "CSV_PATH = f\"{config['out_feats']}/csv\"\n",
    "os.makedirs(CSV_PATH, exist_ok=True)\n",
    "\n",
    "if df_train.empty == False:\n",
    "    df_train.to_csv(f'{CSV_PATH}/training.csv', index=False)\n",
    "\n",
    "if df_val.empty == False:\n",
    "    df_val.to_csv(f'{CSV_PATH}/val.csv', index=False)\n",
    "\n",
    "if df_test.empty == False:\n",
    "    df_test.to_csv(f'{CSV_PATH}/test.csv', index=False)\n",
    "\n",
    "if df_full.empty == False:\n",
    "    df_full.to_csv(f'{CSV_PATH}/full_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSyR90FIb3tY"
   },
   "source": [
    "## 4. Test the location of the bounding boxes around the eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mWfx5ZVHTYm"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{config['out_feats']}/csv/full_dataset.csv\", converters={'bbox_shape': eval})\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    print(\"Image:\", df['image'][idx], \"label:\", df['label'][idx])\n",
    "    img = cv2.imread(df['image'][idx])\n",
    "\n",
    "    b = img/np.max(img)\n",
    "    fig, ax = plt.subplots(figsize=(5,8))\n",
    "    plt.imshow(b)\n",
    "\n",
    "    if df['bbox_shape'][idx] != []:\n",
    "        bbox = df['bbox_shape'][idx]\n",
    "\n",
    "        # Read dimensions of bbox_eye\n",
    "        cX = (int)(bbox[0])\n",
    "        cY = (int)(bbox[1])\n",
    "        bb_width = (int)(bbox[2])\n",
    "        bb_height = (int)(bbox[3])\n",
    "\n",
    "        ax.add_patch(matplotlib.patches.Rectangle((cX, cY), bb_width, bb_height, color ='green', fc='none'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MniuThrHiD6s"
   },
   "source": [
    "## 5. Link .nc files to ARCHER products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uf9dcHNYbpGi"
   },
   "source": [
    "### 5.1. Read all txt files from ARCHER and convert them to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kB4u7if4n4eV"
   },
   "outputs": [],
   "source": [
    "# Directory where ARCHER products are stored\n",
    "archer_dir = f\"{config['out_archer']}/ARCHER_products\"\n",
    "folders = glob(f\"{archer_dir}/*\")\n",
    "cnt = 0\n",
    "\n",
    "for folder in folders:\n",
    "    #print(folder)\n",
    "    files = glob(f\"{folder}/*\")\n",
    "    file = os.path.join(folder, \"archerTrackTable.txt\")\n",
    "\n",
    "    if file in files:\n",
    "        cnt+=1\n",
    "        try:\n",
    "            # Read txt file as pandas df\n",
    "            cols = [\"Date/Time (UTC)\", \"Source Sensor\", \"Vmax (kts)\", \"t_off (hrs)\", \"Geo-ref Lat\",\n",
    "                    \"Lon\", \"Opr Ctr Lat\", \"Lon2\", \"50% cert rad\", \"95% cert rad\", \"Eye diam (deg)\", \"% cert eye\", \"50% cert fxa\"]\n",
    "            df = pd.read_csv(file, sep=\"\\s{2,}\", engine='python', skiprows=1, names = cols, index_col=False)\n",
    "            df[\"Date/Time (UTC)\"] = pd.to_datetime(df[\"Date/Time (UTC)\"], format='%Y-%m-%d %H:%M:%S')\n",
    "            df.to_csv(f\"{folder}/archerTrackTable.csv\", index=False)\n",
    "\n",
    "        except ValueError as err:\n",
    "            print(\"Error:\", err)\n",
    "            file = os.path.join(folder, \"summaryTable.txt\")\n",
    "            if file in files:\n",
    "                try:\n",
    "                    # read .txt file as pandas df\n",
    "                    cols = [\"Date/Time (UTC)\", \"Source Sensor\", \"Vmax (kts)\", \"ARCHER Lat\", \"Lon\", \"Geo-ref Lat\",\n",
    "                            \"Lon2\", \"50% cert rad\", \"95% cert rad\", \"Eye diam (deg)\", \"% cert eye\", \"50% cert fxa\"]\n",
    "                    df = pd.read_csv(file, sep=\"\\s{2,}\", engine='python', skiprows=1, names = cols, index_col=False)\n",
    "                    df[\"Date/Time (UTC)\"] = df[\"Date/Time (UTC)\"].apply(lambda date: date if \" *\" not in date else date.replace(\" *\", \"\"))\n",
    "                    df[\"Date/Time (UTC)\"] = pd.to_datetime(df[\"Date/Time (UTC)\"], format='%Y-%m-%d %H:%M:%S')\n",
    "                    df.to_csv(f\"{folder}/summaryTable.csv\", index=False)\n",
    "                \n",
    "                except ValueError as err:\n",
    "                    print(\"Second Error:\", err)\n",
    "                    continue\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yn0ED6AEbyL-"
   },
   "source": [
    "### 5.2. Link each .nc file to an ARCHER csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KURETo41rBMG"
   },
   "source": [
    "#### 5.2.1. Gather info from .nc files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVu0FRlQiGsc"
   },
   "outputs": [],
   "source": [
    "categories = [\"category1\", \"category2\", \"category3\", \"category4\", \"category5\"]\n",
    "\n",
    "nc_files = []\n",
    "date_time = []\n",
    "extreme_coords = []\n",
    "for category in categories:\n",
    "    print(category)\n",
    "    # Directory where .nc files are\n",
    "    saved_dir = f\"{config['in_nc']}/{category}\"\n",
    "\n",
    "    # Directory where corresponding images will be saved\n",
    "    images_dir = f\"{config['out_feats']}/{category}\"\n",
    "\n",
    "    if category == 'category5':\n",
    "        # Get .nc files\n",
    "        nc_list = glob(f\"{saved_dir}/*.nc\")\n",
    "    else:\n",
    "        cat = int(re.findall(\"\\d\", category)[0])\n",
    "        categories_higher = [i for i in range(cat+1, 6)]\n",
    "        nc_list_higher_all = []\n",
    "        for cat_h in categories_higher:\n",
    "            nc_list_higher_all.extend(glob(f\"{config['in_nc']}/category{cat_h}/*.nc\"))\n",
    "\n",
    "        nc_list_higher = [os.path.basename(x) for x in nc_list_higher_all]\n",
    "        nc_list = [file for file in glob(f\"{saved_dir}/*.nc\") if os.path.basename(file) not in nc_list_higher]\n",
    "        \n",
    "    cnt = 0\n",
    "    for nc_image in nc_list:\n",
    "        # Read .nc product\n",
    "        full_info_image = netCDF4.Dataset(nc_image, mode='r') \n",
    "\n",
    "        # Getting the information of the feature wanted\n",
    "        try:\n",
    "            # Try to read this feature as it usually gives problems\n",
    "            feature_cross = full_info_image.variables[\"nrcs_detrend_cross\"][:]\n",
    "\n",
    "            # extract time registered\n",
    "            tmax_units = datetime.strptime(full_info_image.measurementDate, '%Y-%m-%dT%H:%M:%SZ')\n",
    "            #tmax_units = tmax_units.strftime('%Y-%m-%d %H_%M_%S')\n",
    "\n",
    "            # extract longitude and latitude features\n",
    "            feature_lon = full_info_image.variables[\"lon\"][:]\n",
    "            #print(\"feature_lon:\", np.max(feature_lon[0]), np.min(feature_lon[0]))\n",
    "            feature_lat = full_info_image.variables[\"lat\"][:]\n",
    "            #print(\"feature_lat:\", np.max(feature_lat[0]), np.min(feature_lat[0]))\n",
    "\n",
    "            coords = [np.min(feature_lat[0]), np.max(feature_lat[0]), np.min(feature_lon[0]), np.max(feature_lon[0])]\n",
    "\n",
    "            nc_files.append(nc_image)\n",
    "            date_time.append(tmax_units)\n",
    "            extreme_coords.append(coords)\n",
    "\n",
    "            full_info_image.close()\n",
    "        except KeyError as err:\n",
    "            # Creating KeyError instance for book keeping\n",
    "            print(\"Error:\", err)\n",
    "            full_info_image.close()\n",
    "            continue\n",
    "        cnt +=1\n",
    "    print(cnt)\n",
    "\n",
    "df = pd.DataFrame({\"nc_files\": nc_files, \"date_time\": date_time, \"extreme_coords\": extreme_coords})\n",
    "df[\"image\"] = df[\"date_time\"].apply(lambda i: i.strftime('%Y-%m-%d %H_%M_%S'+'.png'))\n",
    "df[\"ARCHER product\"] = np.empty((len(df), 0)).tolist()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0KbhnQeegjE"
   },
   "outputs": [],
   "source": [
    "df.to_csv(f\"{config['out_archer']}/IFREMER_info.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0D33UPLrHIt"
   },
   "source": [
    "#### 5.2.2. Link .nc files to ARCHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sTUfJEJj6Rcv"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{config['out_archer']}/IFREMER_info.csv\", converters={'ARCHER product': eval, 'extreme_coords': eval})\n",
    "df[\"date_time\"] = pd.to_datetime(df[\"date_time\"])\n",
    "\n",
    "# directory where ARCHER products are stored\n",
    "archer_dir = f\"{config['out_archer']}/ARCHER_products\"\n",
    "folders = glob(f\"{archer_dir}/*\")\n",
    "cnt = 0\n",
    "\n",
    "def link_ARCHER_IFREMER(x, EndDate, StartDate, lat_arr, lon_arr):\n",
    "    lat_ok = False\n",
    "    lon_ok = False\n",
    "    x['extreme_coords'] = np.array(x['extreme_coords'], dtype=np.float)\n",
    "    print(\"StartDate:\", StartDate, \", EndDate:\", EndDate, \", SAR datetime:\", x['date_time'])\n",
    "\n",
    "    if StartDate <= x['date_time'] <= EndDate:\n",
    "        print(\"Inside datetime!\")\n",
    "        print(x['extreme_coords'])\n",
    "\n",
    "        print(lat_arr)\n",
    "        if any((x['extreme_coords'][0] <= lat_arr) & (lat_arr <= x['extreme_coords'][1])):\n",
    "            print(\"LAT OK!\")\n",
    "            lat_ok = True\n",
    "        #for item in lat_arr:\n",
    "        #    print(type(item), type(x['extreme_coords'][0]), type(x['extreme_coords'][1]))\n",
    "        #    if x['extreme_coords'][0] <= item <= x['extreme_coords'][1]:\n",
    "        #        print(\"LAT OK! Value: {}\".format(item))\n",
    "        #        lat_ok = True\n",
    "        #        break\n",
    "\n",
    "        print(lon_arr)\n",
    "        if any((x['extreme_coords'][2] <= lon_arr) & (lon_arr <= x['extreme_coords'][3])):\n",
    "            print(\"LON OK!\")\n",
    "            lon_ok = True\n",
    "        #for item in lon_arr:\n",
    "        #    if x['extreme_coords'][2] <= item <= x['extreme_coords'][3]:\n",
    "        #        print(\"LON OK! Value: {}\".format(item))\n",
    "        #        lon_ok = True\n",
    "        #        break\n",
    "\n",
    "        if lat_ok and lon_ok:\n",
    "            #sys.exit()\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "for folder in folders:\n",
    "    cnt +=1\n",
    "    #print(folder)\n",
    "    csv_files = glob(f\"{folder}/*.csv\")\n",
    "\n",
    "    if len(csv_files) > 1:\n",
    "        csv_files = [s for s in csv_files if \"new\" in s]\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        archer_df = pd.read_csv(csv_file, header=0)\n",
    "        archer_df[\"Date/Time (UTC)\"] = pd.to_datetime(archer_df[\"Date/Time (UTC)\"])\n",
    "        archer_df = archer_df.dropna()\n",
    "\n",
    "        StartDate = np.min(archer_df[\"Date/Time (UTC)\"])\n",
    "        EndDate = np.max(archer_df[\"Date/Time (UTC)\"])\n",
    "        if (EndDate - StartDate ) / np.timedelta64(1, 'D') > 30:    # time difference of more than 30 days\n",
    "            #print(folder, \"- StartDate:\", StartDate, \"EndDate:\", EndDate)\n",
    "        \n",
    "        inTime_df = df[(df.date_time >= StartDate) & (df.date_time <= EndDate)]\n",
    "\n",
    "        if not inTime_df.empty:\n",
    "            lat = archer_df[\"Geo-ref Lat\"].dropna().to_numpy()\n",
    "            lat_arr = np.delete(lat, np.argwhere(lat == \"***\"))\n",
    "            lat_arr = lat_arr.astype(np.float)\n",
    "\n",
    "            lon_col = \"Lon\" if \"archerTrackTable\" in os.path.basename(csv_file) else \"Lon2\"\n",
    "            lon = archer_df[lon_col].dropna().to_numpy()\n",
    "            lon_arr = np.delete(lon, np.argwhere(lon == \"***\"))\n",
    "            lon_arr = lon_arr.astype(np.float)\n",
    "\n",
    "            #df[\"ARCHER product\"] = df[\"ARCHER product\"].apply(lambda i: i if not (df.date_time < EndDate & df.date_time > StartDate) else i.append(folder)\n",
    "            df.apply(lambda x: x[\"ARCHER product\"].append(csv_file) if link_ARCHER_IFREMER(x, EndDate, StartDate, lat_arr, lon_arr) else x[\"ARCHER product\"], axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRhBx1BkYRCV"
   },
   "outputs": [],
   "source": [
    "df.to_csv(f\"{config['out_archer']}/ARCHER_IFREMER_link_by_date_and_coords.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dok9QDdTY_tX"
   },
   "source": [
    "#### 5.2.3. Deal with problematic ARCHER products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ywGnPNELsU9"
   },
   "outputs": [],
   "source": [
    "# Folders with time_diffs between StartDate and EndDate higher than 30 days (problematic)\n",
    "# link_ANCHER_IFREMER/ARCHER_products/2016_30W - StartDate: 2016-12-21 05:30:00 EndDate: 2017-11-15 11:30:00\n",
    "# link_ANCHER_IFREMER/ARCHER_products/2018_03S - StartDate: 2018-01-02 17:30:00 EndDate: 2018-11-11 17:30:00\n",
    "# link_ANCHER_IFREMER/ARCHER_products/2018_09P - StartDate: 2018-02-13 06:00:00 EndDate: 2019-01-07 21:00:00\n",
    "# link_ANCHER_IFREMER/ARCHER_products/2020_07S - StartDate: 2020-01-11 11:30:00 EndDate: 2020-12-30 17:30:00\n",
    "# link_ANCHER_IFREMER/ARCHER_products/2020_10S - StartDate: 2020-01-26 17:30:00 EndDate: 2021-01-19 11:30:00\n",
    "\n",
    "folder = f\"{config['out_archer']}/ARCHER_products/2020_10S\"\n",
    "csv_files = glob(f\"{folder}/*.csv\")\n",
    "\n",
    "for csv_file in csv_files:\n",
    "\n",
    "    archer_df = pd.read_csv(csv_file, header=0)\n",
    "    archer_df[\"Date/Time (UTC)\"] = pd.to_datetime(archer_df[\"Date/Time (UTC)\"])\n",
    "    archer_df = archer_df.sort_values(by=[\"Date/Time (UTC)\"], ignore_index=True).dropna()\n",
    "\n",
    "    # check time differences between consecutive obervations\n",
    "    time_diffs = archer_df['Date/Time (UTC)'].diff().dropna()\n",
    "    print(time_diffs)\n",
    "    # get index for when time diff is above 2 days\n",
    "    index = np.where(time_diffs / np.timedelta64(1, 'D') > 5)[0]\n",
    "\n",
    "    if index.size > 0:\n",
    "        df1 = archer_df.iloc[:index[0]+1]\n",
    "        df2 = archer_df.iloc[index[0]+1:]\n",
    "        print(df1)\n",
    "        print(df2)\n",
    "\n",
    "        input(\"Press Enter to continue...\")\n",
    "        df1.to_csv(\"{}/{}_new1.csv\".format(folder, os.path.basename(csv_file)[:-4]), index=False)\n",
    "        df2.to_csv(\"{}/{}_new2.csv\".format(folder, os.path.basename(csv_file)[:-4]), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkQ99UuaeH01"
   },
   "source": [
    "### 5.3. Test location of eye according to ARCHER and best track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4_T7Bbd6aYt"
   },
   "outputs": [],
   "source": [
    "def find_nearest(array, value):\n",
    "    #array = np.asarray(array)\n",
    "    X = np.abs(array - value)\n",
    "    idx = np.where(X == X.min())\n",
    "    print(\"Index:\", idx)\n",
    "    #print(\"Value wanted: {} and closest value in X: {}\".format(value, array[idx[0], idx[1]]))\n",
    "    #return array[idx[0], idx[1]], idx\n",
    "    flat_index = np.argmin(np.abs(array - value))\n",
    "    alt_idx = np.unravel_index(flat_index, array.shape)\n",
    "    print(\"Alternatively:\", alt_idx)\n",
    "    print(\"Value wanted: {} and closest value in X: {}\".format(value, array[alt_idx[0], alt_idx[1]]))\n",
    "    return array[alt_idx[0], alt_idx[1]], alt_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XiNnvIdpHD06"
   },
   "outputs": [],
   "source": [
    "request_url = \"https://cyclobs.ifremer.fr/app/api/getData?cat_min=cat-1&cat_max=cat-5&mission=S1B,S1A&product_type=swath&include_cols=all\"\n",
    "df_request = pd.read_csv(request_url)\n",
    "\n",
    "nc_CyclObs_info = pd.DataFrame(data = {'data_url': df_request['data_url'], 'sid': df_request[\"sid\"], 'TC_name': df_request[\"cyclone_name\"]})\n",
    "nc_CyclObs_info[\"data_url\"] = nc_CyclObs_info[\"data_url\"].apply(lambda x: os.path.basename(x))\n",
    "nc_CyclObs_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05-V8N05Dmdk"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{config['out_archer']}/ARCHER_IFREMER_link_by_date_and_coords.csv\", converters={'ARCHER product': eval})\n",
    "df[\"date_time\"] = pd.to_datetime(df[\"date_time\"])\n",
    "\n",
    "best_track_df = pd.read_csv(\"best_track/ibtracs.since1980.list.v04r00.csv\", header=0, skiprows=range(1, 2))\n",
    "best_track_df[\"ISO_TIME\"] = pd.to_datetime(best_track_df[\"ISO_TIME\"])\n",
    "best_track_df['ISO_TIME_str'] = best_track_df['ISO_TIME'].apply(lambda i: datetime.strftime(i, \"%Y-%m-%d\"))\n",
    "\n",
    "time_diffs = []\n",
    "\n",
    "cnt = 0\n",
    "for index, row in df.iterrows():\n",
    "    print(row)\n",
    "    cnt+=1\n",
    "\n",
    "    full_info_image = netCDF4.Dataset(row[\"nc_files\"], mode='r') \n",
    "\n",
    "    mask = full_info_image.variables[\"mask_flag\"][:]\n",
    "    mask[mask != 0] = 1\n",
    "    # dilate mask\n",
    "    kernel = np.ones((11, 11), np.int8)\n",
    "    mask_dilated = cv2.dilate(mask[0], kernel, iterations = 1)\n",
    "\n",
    "    feature_co = full_info_image.variables[\"nrcs_detrend_co\"][:]\n",
    "    feature_co[0][mask_dilated != 0] = 0 # mask out the land values\n",
    "    feature_co = (feature_co[0] - np.min(feature_co[0]))/(np.max(feature_co[0]) - np.min(feature_co[0]))\n",
    "\n",
    "    feature_cross = full_info_image.variables[\"nrcs_detrend_cross\"][:]\n",
    "    feature_cross[0][mask_dilated != 0] = 0 # mask out the land values\n",
    "    feature_cross = (feature_cross[0] - np.min(feature_cross[0]))/(np.max(feature_cross[0]) - np.min(feature_cross[0]))\n",
    "\n",
    "    feature_wind = full_info_image.variables[\"wind_speed\"][:]\n",
    "    feature_wind[0][mask_dilated != 0] = 0 # mask out the land values\n",
    "    feature_wind = (feature_wind[0] - np.min(feature_wind[0]))/(np.max(feature_wind[0]) - np.min(feature_wind[0]))\n",
    "\n",
    "    # stack matrices along 3rd axis (depth-wise)\n",
    "    output_image = np.dstack((feature_co, feature_cross, feature_wind))\n",
    "\n",
    "    # extract longitude and latitude features\n",
    "    feature_lon = full_info_image.variables[\"lon\"][:]\n",
    "    #feature_lon = feature_lon[0]\n",
    "    feature_lat = full_info_image.variables[\"lat\"][:]\n",
    "    #feature_lat = feature_lat[0]\n",
    "\n",
    "    nc_info = nc_CyclObs_info.loc[nc_CyclObs_info[\"data_url\"] == os.path.basename(row[\"nc_files\"])]\n",
    "    nc_sid = nc_info[\"sid\"].values[0].upper()\n",
    "    print(nc_sid)\n",
    "\n",
    "    # prepare plot of coordinates\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(output_image)    \n",
    "\n",
    "    # filter best_track_df by date\n",
    "    aux = best_track_df.loc[best_track_df['ISO_TIME_str'] == datetime.strftime(row[\"date_time\"], \"%Y-%m-%d\")][[\"USA_ATCF_ID\", \"ISO_TIME\", \"LAT\", \"LON\"]]\n",
    "    #print(aux)\n",
    "    if not aux.empty:\n",
    "        best_track = aux.loc[aux['USA_ATCF_ID'] == nc_sid] if len(np.unique(aux['USA_ATCF_ID'])) > 1 else aux\n",
    "\n",
    "        if not best_track.empty:\n",
    "            best_track = best_track.append({\"ISO_TIME\": row[\"date_time\"], \"LAT\": np.nan, \"LON\": np.nan}, ignore_index=True)\n",
    "            best_track = best_track.sort_values(by=[\"ISO_TIME\"], ignore_index=True)\n",
    "            interp_best_track = best_track.interpolate(method='linear')\n",
    "            bt_lat_value = interp_best_track.loc[interp_best_track[\"ISO_TIME\"] == row[\"date_time\"]][\"LAT\"]\n",
    "            bt_lon_value = interp_best_track.loc[interp_best_track[\"ISO_TIME\"] == row[\"date_time\"]][\"LON\"]\n",
    "            #print(interp_best_track)\n",
    "            #print(bt_lat_value.values[0], bt_lon_value.values[0])\n",
    "\n",
    "            _, bt_idx_lat = find_nearest(feature_lat[0], bt_lat_value.values[0])\n",
    "            _, bt_idx_lon = find_nearest(feature_lon[0], bt_lon_value.values[0])\n",
    "            bt_x = bt_idx_lon[1]\n",
    "            bt_y = bt_idx_lat[0]\n",
    "            #print(\"BT Center:\", bt_x, bt_y)\n",
    "\n",
    "            ax.plot(bt_x, bt_y, '^', mfc='none', label = \"Best track\")\n",
    "\n",
    "    for item in row['ARCHER product']:\n",
    "        if row['ARCHER product'] == []:\n",
    "            continue\n",
    "        print(item)\n",
    "        archer_df = pd.read_csv(item)\n",
    "        archer_df[\"Date/Time (UTC)\"] = pd.to_datetime(archer_df[\"Date/Time (UTC)\"])\n",
    "\n",
    "        lon_col = \"Lon\" if \"archerTrackTable\" in os.path.basename(item) else \"Lon2\"\n",
    "        coords = archer_df[[\"Date/Time (UTC)\", \"Geo-ref Lat\", lon_col]].dropna()\n",
    "        coords_df = coords[coords[\"Geo-ref Lat\"] != \"***\"]\n",
    "        coords_df[\"Geo-ref Lat\"] = coords_df[\"Geo-ref Lat\"].astype(float)\n",
    "        coords_df[lon_col] = coords_df[lon_col].astype(float)\n",
    "        coords_df = coords_df.append({\"Date/Time (UTC)\": row[\"date_time\"], \"Geo-ref Lat\": np.nan, lon_col: np.nan}, ignore_index=True)\n",
    "        coords_df = coords_df.sort_values(by=[\"Date/Time (UTC)\"], ignore_index=True)\n",
    "        print(coords_df)\n",
    "\n",
    "        # Check time difference between ARCHER records and SAR measuremente Date\n",
    "        index = coords_df.index[coords_df[\"Date/Time (UTC)\"] == row[\"date_time\"]].tolist()[0]\n",
    "        test_diff = coords_df.iloc[index-1:index+2] if index > 0 else coords_df.iloc[index:index+2]\n",
    "        print(min(test_diff['Date/Time (UTC)'].diff().dropna().tolist()))\n",
    "        time_diffs.append(min(test_diff['Date/Time (UTC)'].diff().dropna().tolist()))\n",
    "\n",
    "        interp_coords_df = coords_df.interpolate(method='linear')\n",
    "        lat_value = interp_coords_df.loc[interp_coords_df[\"Date/Time (UTC)\"] == row[\"date_time\"]][\"Geo-ref Lat\"]\n",
    "        lon_value = interp_coords_df.loc[interp_coords_df[\"Date/Time (UTC)\"] == row[\"date_time\"]][lon_col]\n",
    "        #print(interp_coords_df.loc[interp_coords_df[\"Date/Time (UTC)\"] == row[\"date_time\"]])\n",
    "    \n",
    "        lat, idx_lat = find_nearest(feature_lat[0], lat_value.values[0])\n",
    "        lon, idx_lon = find_nearest(feature_lon[0], lon_value.values[0])\n",
    "        x = idx_lon[1]\n",
    "        y = idx_lat[0]\n",
    "        print(\"ARCHER Center:\", x, y)\n",
    "\n",
    "        ax.plot(x, y, 'o', mfc='none', label = \"ARCHER\")\n",
    "        \n",
    "    ax.legend(title = \"Source\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    #plots_dir = f\"{config['out_archer']}/plot_coords\"\n",
    "    #os.makedirs(plots_dir, exist_ok=True)\n",
    "    #fig.savefig(\"{}/TC_center_by_source_{}.jpg\".format(plots_dir, cnt), bbox_inches='tight')\n",
    "\n",
    "#time_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-O68dWgThnV"
   },
   "outputs": [],
   "source": [
    "time_diff_df = pd.DataFrame(time_diffs, columns=['Date'])\n",
    "#print(time_diff_df)\n",
    "\n",
    "hours = time_diff_df.Date / np.timedelta64(1, 'h')\n",
    "#hours = hours[hours < 48]\n",
    "\n",
    "fig,ax = plt.subplots(1, 1, figsize=(15,8))\n",
    "ax.set(xlabel = 'Time difference (hours)', title='Time difference between SAR and ARCHER products')\n",
    "hours.plot.hist(bins=100)\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()\n",
    "\n",
    "#fig.savefig(f\"{config['out_archer']}/time_diff_hist.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYSJQrLgHugS"
   },
   "source": [
    "## 6. Test black pixels in image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HT8lqJwrHxok"
   },
   "outputs": [],
   "source": [
    "# Read the image path\n",
    "image_path = f\"{config['out_feats']}/category2/2018-10-24 01_06_05.png\"\n",
    "im = cv2.imread(image_path) # loads images as BGR in float32\n",
    "image = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)   # BGR -> RGB\n",
    "plt.imshow(image)\n",
    "print(image.size)\n",
    "print(image.shape)\n",
    "print(image.shape[0]*image.shape[1])\n",
    "print(cv2.countNonZero(image[:,:,0]))\n",
    "print(cv2.countNonZero(image[:,:,1]))\n",
    "print(cv2.countNonZero(image[:,:,2]))\n",
    "percentage = (image.shape[0]*image.shape[1] - cv2.countNonZero(image[:,:,0]))/(image.shape[0]*image.shape[1]) * 100\n",
    "percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vd-td7MaJjEK"
   },
   "outputs": [],
   "source": [
    "# Read the image path\n",
    "image_path = f\"{config['out_feats']}/category2/2020-09-22 10_17_34.png\"\n",
    "im = cv2.imread(image_path) # loads images as BGR in float32\n",
    "image = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)   # BGR -> RGB\n",
    "plt.imshow(image)\n",
    "print(image.size)\n",
    "print(image.shape)\n",
    "print(image.shape[0]*image.shape[1])\n",
    "print(cv2.countNonZero(image[:,:,0]))\n",
    "percentage = (image.shape[0]*image.shape[1] - cv2.countNonZero(image[:,:,0]))/(image.shape[0]*image.shape[1]) * 100\n",
    "percentage"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_PaVuZ_RgZmY",
    "gzs88t_ymkwf",
    "ScgK25TlVd2-",
    "QzJBBp6HZdAh",
    "xRfjWIAmi3cK",
    "ygyOuX_Abug0",
    "zSyR90FIb3tY",
    "xYSJQrLgHugS"
   ],
   "name": "TC_downloader_&_converter.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "tropical_cyclones",
   "language": "python",
   "name": "tropical_cyclones"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
