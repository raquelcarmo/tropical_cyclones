{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/raquelcarmo/tropical_cyclones/blob/main/src/code/TC_model_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJn-0pS7STzM"
   },
   "source": [
    "# Model Comparison\n",
    "Script to execute performance comparison between trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1006,
     "status": "ok",
     "timestamp": 1625214820192,
     "user": {
      "displayName": "Raquel Carmo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgUUTZlzR-089p-AQlc4K-YNrw3DWlq1fWmuzLsdfI=s64",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "3d95OmlLCDSj",
    "outputId": "6a59f6d6-fc65-47a1-d0e3-9f399c2f3e4b"
   },
   "outputs": [],
   "source": [
    "# Insert your desired path to work on\n",
    "import os\n",
    "os.chdir('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3719,
     "status": "ok",
     "timestamp": 1625214826408,
     "user": {
      "displayName": "Raquel Carmo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgUUTZlzR-089p-AQlc4K-YNrw3DWlq1fWmuzLsdfI=s64",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "a8tbWAaHayf0"
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pickle\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Uk0cd0xa-Dw"
   },
   "source": [
    "## 1. Comparison between X models in one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bksBnI0LgwnY"
   },
   "outputs": [],
   "source": [
    "main_dir = \"VV_VH_WS_dilated\"\n",
    "sub_dir = f\"{main_dir}/results/categorization/test_eye_only\"\n",
    "\n",
    "# note that the result (in this case the recall) could be null if the classifier is broken \n",
    "# (all recall = 1 because is always classified as positive class)\n",
    "MODEL_1_DIRECTORY = f\"{sub_dir}/ResNet_nu-F_bs-8_416x416_lr-1e-05_ep-30_sp-5_no-zT_cr-u1_ag-T_drp-F/csv\"\n",
    "MODEL_2_DIRECTORY = f\"{sub_dir}/test_new_weights/ResNet_nu-F_bs-8_416x416_lr-1e-05_ep-30_sp-5_no-zT_cr-u1_ag-T_drp-F/csv\"\n",
    "#MODEL_3_DIRECTORY = f\"{sub_dir}/test_new_weights/ResNet_nu-F_bs-8_416x416_lr-1e-05_ep-30_sp-5_no-zT_cr-u1_ag-T_drp-F/csv\"\n",
    "#MODEL_4_DIRECTORY = f\"{sub_dir}/test_rmse/ResNet_nu-F_bs-8_416x416_lr-1e-05_ep-30_sp-5_no-mT_cr-u1_ag-T_drp-F/csv\"\n",
    "#MODEL_5_DIRECTORY = f\"{sub_dir}/ResNet_nu-F_bs-8_416x416_lr-0.0001_ep-35_sp-5_no-zT_cr-u1_ag-T_drp-F_ft-last50/csv\"\n",
    "#MODEL_6_DIRECTORY = f\"{sub_dir}/ResNet_nu-F_bs-8_416x416_lr-0.0001_ep-35_sp-5_no-zT_cr-u1_ag-T_drp-F_ft-last75/csv\"\n",
    "#MODEL_7_DIRECTORY = f\"{sub_dir}/ResNet_nu-F_bs-8_416x416_lr-0.0001_ep-35_sp-5_no-zT_cr-u1_ag-T_drp-F_ft-last125/csv\"\n",
    "#MODEL_8_DIRECTORY = f\"{sub_dir}/ResNet_nu-F_bs-8_416x416_lr-0.0001_ep-35_sp-5_no-zT_cr-u1_ag-T_drp-F_ft-last175/csv\"\n",
    "#MODEL_9_DIRECTORY = f\"{sub_dir}/ResNet_nu-F_bs-8_416x416_lr-1e-05_ep-30_sp-5_no-zT_cr-u1_ag-T_drp-F/csv\"\n",
    "\n",
    "#dir_dict = {1: MODEL_1_DIRECTORY, 2: MODEL_2_DIRECTORY, 3: MODEL_3_DIRECTORY,\\\n",
    "#            4: MODEL_4_DIRECTORY, 5: MODEL_5_DIRECTORY, 6: MODEL_6_DIRECTORY,\\\n",
    "#            7: MODEL_7_DIRECTORY, 8: MODEL_8_DIRECTORY, 9: MODEL_9_DIRECTORY}\n",
    "#dir_dict = {1: MODEL_1_DIRECTORY, 2: MODEL_2_DIRECTORY, 3: MODEL_3_DIRECTORY, 4: MODEL_4_DIRECTORY, 5: MODEL_5_DIRECTORY}\n",
    "#dir_dict = {1: MODEL_1_DIRECTORY, 2: MODEL_2_DIRECTORY, 3: MODEL_3_DIRECTORY, 4: MODEL_4_DIRECTORY}\n",
    "#dir_dict = {1: MODEL_1_DIRECTORY, 2: MODEL_2_DIRECTORY, 3: MODEL_3_DIRECTORY}\n",
    "dir_dict = {1: MODEL_1_DIRECTORY, 2: MODEL_2_DIRECTORY}\n",
    "\n",
    "# Settings\n",
    "models = len(dir_dict)\n",
    "save_plot = True\n",
    "#title = \"Comparison between dropout rates - ResNet_nu-F_bs-8_384x384_lr-1e-05_ep-30_sp-5_no-mT_cr-u1_ag-T_drp-XXX\"\n",
    "#title = \"Comparison between input sizes - ResNet_nu-F_bs-8_XXX_lr-0.0001_ep-20_sp-5_no-zT_cr-u1_ag-T\"\n",
    "#title = \"Comparison between normalizations\" - ResNet_nu-F_bs-8_384x384_lr-1e-05_ep-30_sp-5_no-XXX_cr-u1_ag-T_drp-F\"\n",
    "#title = \"Comparison between nb crops\"# - ResNet_nu-F_bs-8_XXX_lr-1e-05_ep-30_sp-5_no-mT_cr-uXXX_ag-T_drp-F\"\n",
    "#title = \"Comparison between nb epochs - ResNet_nu-F_bs-8_288x288_lr-0.0001_ep-XXX_sp-5_no-mT_cr-u1_ag-T_ft-165\"\n",
    "#title = \"Comparison between learning rates - ResNet_nu-F_bs-8_384x384_lr-XXX_ep-XXX_sp-5_no-zT_cr-u1_ag-T\"\n",
    "#title = \"Comparison between nb frozen layers - ResNet_nu-F_bs-8_416x416_lr-XXX_ep-XXX_sp-5_no-zT_cr-u1_ag-T_drp-F_ft-XXX\"\n",
    "#title = \"Comparison between CNNs - XXX_nu-F_bs-8_288x288_lr-0.0001_ep-20_sp-5_no-mT_cr-u1_ag-T\"\n",
    "#title = \"Comparison between nb of classes - ResNet_nu-F_bs-8_416x416_lr-XXX_ep-XXX_sp-5_no-XXXT_cr-u1_ag-T\"\n",
    "#title = \"Comparison between rotations - Model 1 has two more augmentation techniques (rot180 and rot270)\"\n",
    "#title = 'Comparison between zeroed land (Model 1) and land (Model 2) - ResNet_nu-F_bs-8_416x416_lr-1e-05_ep-30_sp-5_no-mT_cr-u1_ag-T_drp-F'\n",
    "#title = 'CrossEntropy Loss (Models 1 and 2) and Combined CrossEntropy and RMSE Loss (Models 3 and 4)'\n",
    "title = 'Normal Class weights (Model 1) and Class weights with prior precision (Model 2) - ResNet_nu-F_bs-8_416x416_lr-1e-05_ep-30_sp-5_no-zT_cr-u1_ag-T_drp-F'\n",
    "file_name = title.replace(\" \", \"_\")\n",
    "\n",
    "# Critical t-value to see the difference in two distribution with 5 samples\n",
    "criticalTvalue = 1.533      # 80% confidence\n",
    "#criticalTvalue = 2.132     # 90% confidence\n",
    "#criticalTvalue = 2.776     # 95% confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJHkL9rpnuUJ"
   },
   "source": [
    "### 1.1. Load necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 590,
     "status": "ok",
     "timestamp": 1625214834504,
     "user": {
      "displayName": "Raquel Carmo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgUUTZlzR-089p-AQlc4K-YNrw3DWlq1fWmuzLsdfI=s64",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "jFN-keiZlUGx"
   },
   "outputs": [],
   "source": [
    "def mean_confidence_interval(data, confidence=0.80):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, m-h, m+h\n",
    "\n",
    "\n",
    "def compute_f1(dir):\n",
    "    precision = np.asarray(pickle.load(open(f\"{dir}/test_precision.pkl\", \"rb\")))\n",
    "    recall = np.asarray(pickle.load(open(f\"{dir}/test_recall.pkl\", \"rb\")))\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    f1 = np.nan_to_num(f1)\n",
    "    with open(f\"{dir}/test_f1-score.pkl\", 'wb') as handle:\n",
    "        pickle.dump(f1.tolist(), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def compute_stats(nb_models, dir_dict, metrics):\n",
    "    inter_dict = {}\n",
    "    mean_dict = {}\n",
    "    for metric in metrics:\n",
    "        metric_dict = {}\n",
    "        for i in range(1, nb_models+1):\n",
    "            load_metric = pickle.load(open(f\"{dir_dict[i]}/test_{metric}.pkl\", \"rb\"))\n",
    "            metric_dict['model%d_%s' %(i, metric)] = load_metric\n",
    "\n",
    "            mean, b, u = mean_confidence_interval(load_metric)\n",
    "            #print(mean, b, u)\n",
    "            #conf_dict['model%d_%s' %(i, metric)] = [mean, b, u]\n",
    "\n",
    "            if 'model%d' %(i) in inter_dict:\n",
    "                inter_dict['model%d' %(i)].append(u - mean)\n",
    "                mean_dict['model%d' %(i)].append(mean)\n",
    "            else:\n",
    "                inter_dict['model%d' %(i)] = [u - mean]\n",
    "                mean_dict['model%d' %(i)] = [mean]\n",
    "\n",
    "        #print(inter_dict)\n",
    "        #print(mean_dict)\n",
    "        print(f\"---------------- {metric.upper()} analysis ----------------\")\n",
    "        for x, y in itertools.combinations(metric_dict.keys(), 2):\n",
    "            t, p = scipy.stats.ttest_ind(metric_dict[x], metric_dict[y])\n",
    "            print(f\"t value {metric} between {x} and {y}: {t}\")\n",
    "            if t > criticalTvalue or t < -criticalTvalue :\n",
    "                print(f\"Null hypothesis rejected -> there is a significant difference in the two {metric} distributions.\")\n",
    "            else:\n",
    "                print(f\"Null hypothesis not rejected -> I assume that each difference in {metric} is due to the randomness.\")\n",
    "        print(\"-------------------------------------------------\")\n",
    "    return mean_dict, inter_dict\n",
    "\n",
    "\n",
    "def plot_model_comparisons(nb_models, metrics, dir_dict, mean_dict, inter_dict, sub_dir, title, save_plot):\n",
    "    # Get colors from palette\n",
    "    colors = sns.color_palette(\"hls\", nb_models)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15,8))\n",
    "    index = np.arange(len(metrics))\n",
    "    bar_width = 0.1\n",
    "    opacity = 0.8\n",
    "\n",
    "    for i in range(1, nb_models+1):\n",
    "        rects = plt.bar(index + (i-1)*bar_width, mean_dict['model%d' %(i)], bar_width, yerr = inter_dict['model%d' %(i)],\n",
    "                        capsize = 5, alpha = opacity, color = colors[i-1],\n",
    "                        label = dir_dict[i].split(sep='/')[-2])\n",
    "\n",
    "    font_size = 15\n",
    "    font_size_ticks = 13\n",
    "    plt.ylabel('Scores', fontsize = font_size)\n",
    "    plt.title(title, fontsize = font_size)\n",
    "    plt.xticks(index + (nb_models/2-0.5)*bar_width, tuple([name.capitalize() for name in metrics]), fontsize = font_size)\n",
    "    plt.yticks(fontsize = font_size_ticks)\n",
    "    #plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True, ncol=2, prop={'size': font_size-3})\n",
    "    plt.legend(loc = 'lower left', prop={'size': font_size})\n",
    "    plt.grid(True)\n",
    "    #ax.set_ylim(ymin=0.75)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Save plot\n",
    "    if save_plot:\n",
    "        dir = f\"{sub_dir}/model_comparisons\"\n",
    "        os.makedirs(dir, exist_ok=True)\n",
    "        fig.savefig(\"{}/{}.jpg\".format(dir, title.replace(\" \", \"_\")), bbox_inches='tight')\n",
    "\n",
    "\n",
    "def plot_frozen_layers(nb_models, metrics, dir_dict, mean_dict, inter_dict, sub_dir, title, save_plot):\n",
    "    # Get colors from palette\n",
    "    colors = sns.color_palette(\"hls\", nb_models)\n",
    "    a = np.arange(nb_models)\n",
    "    x = [0, 165, 170, 172, 174]\n",
    "    assert nb_models == len(x)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15,8))\n",
    "\n",
    "    for j in range(len(metrics)):\n",
    "        metric_mean = []\n",
    "        metric_yerr = []\n",
    "        for i in range(1, nb_models+1):\n",
    "            #ax.plot(a, mean_dict['model%d' %(i)], color = colors[i-1], label = dir_dict[i].split(sep='/')[-2])\n",
    "            metric_mean.append(mean_dict['model%d' %(i)][j])\n",
    "            metric_yerr.append(inter_dict['model%d' %(i)][j])\n",
    "\n",
    "        ax.errorbar(a, metric_mean, yerr = metric_yerr, capsize=5, capthick=2,\n",
    "                    fmt='-o', color = colors[j], label = metrics[j].capitalize())\n",
    "        bellow = np.asarray(metric_mean)-np.asarray(metric_yerr)\n",
    "        up = np.asarray(metric_mean)+np.asarray(metric_yerr)\n",
    "        ax.fill_between(a, bellow.tolist(), up.tolist(), color=colors[j], alpha=.1)\n",
    "    \n",
    "    font_size = 15\n",
    "    font_size_ticks = 13\n",
    "    plt.xlabel('Number of frozen layers in backbone', fontsize = font_size)\n",
    "    plt.ylabel('Scores', fontsize = font_size)\n",
    "    plt.title(title, fontsize = font_size)\n",
    "    ax.xaxis.set_ticks(a) #set the ticks to be a\n",
    "    ax.xaxis.set_ticklabels(x, fontsize = font_size_ticks) # change the ticks' names to x\n",
    "    #ax.set_ylim(ymin=0.25)\n",
    "    #plt.xticks(x, fontsize = font_size_ticks)\n",
    "    start, end = ax.get_ylim()\n",
    "    ax.yaxis.set_ticks(np.arange(np.round(start, 1), np.round(end, 1), 0.05))\n",
    "    plt.yticks(fontsize = font_size_ticks)\n",
    "    plt.legend(loc = 'lower left', prop={'size': font_size})\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Save plot\n",
    "    if save_plot:\n",
    "        dir = f\"{sub_dir}/model_comparisons\"\n",
    "        os.makedirs(dir, exist_ok=True)\n",
    "        fig.savefig(\"{}/{}.jpg\".format(dir, title.replace(\" \", \"_\")), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZ417Sp1nyaE"
   },
   "source": [
    "### 1.2. Metric analysis and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5132,
     "status": "ok",
     "timestamp": 1625217488927,
     "user": {
      "displayName": "Raquel Carmo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgUUTZlzR-089p-AQlc4K-YNrw3DWlq1fWmuzLsdfI=s64",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "27PGdZGuszTa",
    "outputId": "b2b6b336-bd8b-4713-a39e-91c839a4fe56"
   },
   "outputs": [],
   "source": [
    "dirs = [\"VV_VH_WS/results/identification/ResNet_Numeric-F_BatchSize-8_lr-0.0001_Epochs-20_Folds-5_Norm-T/csv\", \n",
    "        \"WS_WS_WS/results/identification/ResNet_Numeric-F_BatchSize-8_lr-0.0001_Epochs-20_Folds-5_Norm-T/csv\",\n",
    "        \"VV_VH_VH/results/identification/ResNet_Numeric-F_BatchSize-8_lr-0.0001_Epochs-20_Folds-5_Norm-T/csv\",\n",
    "        \"WS_sWSO_cWSO/results/identification/ResNet_Numeric-F_BatchSize-8_lr-0.0001_Epochs-20_Folds-5_Norm-T/csv\"]\n",
    "\n",
    "for check_dir in dirs:\n",
    "    print(check_dir.split('/')[0])\n",
    "    file = glob(f'{check_dir}/test_f1-score.pkl')\n",
    "\n",
    "    if file == []:\n",
    "        compute_f1(check_dir)\n",
    "    #check_dir = dir_dict[9]\n",
    "\n",
    "    acc = np.asarray(pickle.load(open(f\"{check_dir}/test_accuracy.pkl\", \"rb\")))\n",
    "    prec = np.asarray(pickle.load(open(f\"{check_dir}/test_precision.pkl\", \"rb\")))\n",
    "    rec = np.asarray(pickle.load(open(f\"{check_dir}/test_recall.pkl\", \"rb\")))\n",
    "    f1 = np.asarray(pickle.load(open(f\"{check_dir}/test_f1-score.pkl\", \"rb\")))\n",
    "    \n",
    "    print(\"Accuracy:\", acc, np.mean(acc), np.std(acc))\n",
    "    print(\"Precision:\", prec, np.mean(prec), np.std(prec))\n",
    "    print(\"Recall:\", rec, np.mean(rec), np.std(rec))\n",
    "    print(\"F1-score:\", f1, np.mean(f1), np.std(f1))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNja6m1MX6cx"
   },
   "outputs": [],
   "source": [
    "# Compute F1-score if not already computed previously\n",
    "for i in range(1, models+1):\n",
    "    dir = dir_dict[i]\n",
    "    file = glob(f'{dir}/test_f1-score.pkl')\n",
    "    if file == []:\n",
    "        compute_f1(dir)\n",
    "\n",
    "# Metrics to use for analysis\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"f1-score\"]\n",
    "\n",
    "# Compute mean and confidence intervals for each metric and each model\n",
    "mean_dict, inter_dict = compute_stats(models, dir_dict, metrics)\n",
    "\n",
    "# Plot model comparison\n",
    "plot_model_comparisons(models, metrics, dir_dict, mean_dict, inter_dict, sub_dir, title, save_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9695aBjozMD"
   },
   "source": [
    "### 1.3. Plot Scores vs. Number of frozen layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QcXw8M2ks9d5"
   },
   "outputs": [],
   "source": [
    "# Compute F1-score if not already computed previously\n",
    "for i in range(1, models+1):\n",
    "    dir = dir_dict[i]\n",
    "    file = glob(f'{dir}/test_f1-score.pkl')\n",
    "    if file == []:\n",
    "        print(\"No f1-score file found.\")\n",
    "        compute_f1(dir)\n",
    "\n",
    "# Metrics to use for analysis\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"f1-score\"]\n",
    "\n",
    "# Compute mean and confidence intervals for each metric and each model\n",
    "mean_dict, inter_dict = compute_stats(models, dir_dict, metrics)\n",
    "\n",
    "# Plot model comparison\n",
    "plot_frozen_layers(models, metrics, dir_dict, mean_dict, inter_dict, sub_dir, title, save_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U90HRPq1bC9p"
   },
   "source": [
    "## 2. Comparison between 6 models across datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9AOLrxYbFQM"
   },
   "outputs": [],
   "source": [
    "NETWORK = \"Mobile\"\n",
    "models = 6\n",
    "MODEL_1_DIRECTORY = f\"VV_VH_WS/results/categorization/{NETWORK}_Numeric-F_BatchSize-8_lr-0.0001_Epochs-20_Folds-5_Norm-T_Aug-T/csv\"\n",
    "MODEL_2_DIRECTORY = f\"VV_VH_VH/results/categorization/{NETWORK}_Numeric-F_BatchSize-8_lr-0.0001_Epochs-20_Folds-5_Norm-T_Aug-T/csv\"\n",
    "MODEL_3_DIRECTORY = f\"WS_WS_WS/results/categorization/{NETWORK}_Numeric-F_BatchSize-8_lr-0.0001_Epochs-20_Folds-5_Norm-T_Aug-T/csv\"\n",
    "MODEL_4_DIRECTORY = f\"WS_sWSO_cWSO/results/categorization/{NETWORK}_Numeric-F_BatchSize-8_lr-0.0001_Epochs-20_Folds-5_Norm-T_Aug-T/csv\"\n",
    "MODEL_5_DIRECTORY = f\"VV_VV_VV/results/categorization/{NETWORK}_Numeric-F_BatchSize-8_700x400_lr-0.0001_Epochs-20_Folds-5_Norm-T_Aug-T/csv\"\n",
    "MODEL_6_DIRECTORY = f\"VH_VH_VH/results/categorization/{NETWORK}_Numeric-F_BatchSize-8_700x400_lr-0.0001_Epochs-20_Folds-5_Norm-T_Aug-T/csv\"\n",
    "\n",
    "# Critical t-value to see the difference in two distribution with 5 samples\n",
    "criticalTvalue = 1.533   # 80% confidence\n",
    "#criticalTvalue = 2.132  # 90% confidence\n",
    "#criticalTvalue = 2.776  # 95% confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9nfHkfGOfiY"
   },
   "outputs": [],
   "source": [
    "# ACCURACY analysis\n",
    "model1_accuracy = pickle.load(open(MODEL_1_DIRECTORY + \"/test_accuracy.pkl\",  \"rb\" ))\n",
    "model2_accuracy = pickle.load(open(MODEL_2_DIRECTORY + \"/test_accuracy.pkl\",  \"rb\" ))\n",
    "model3_accuracy = pickle.load(open(MODEL_3_DIRECTORY + \"/test_accuracy.pkl\",  \"rb\" ))\n",
    "model4_accuracy = pickle.load(open(MODEL_4_DIRECTORY + \"/test_accuracy.pkl\",  \"rb\" ))\n",
    "model5_accuracy = pickle.load(open(MODEL_5_DIRECTORY + \"/test_accuracy.pkl\",  \"rb\" ))\n",
    "model6_accuracy = pickle.load(open(MODEL_6_DIRECTORY + \"/test_accuracy.pkl\",  \"rb\" ))\n",
    "\n",
    "model1_accuracy_mean, b_model1_acc, u_model1_acc = mean_confidence_interval(model1_accuracy)\n",
    "model2_accuracy_mean, b_model2_acc, u_model2_acc = mean_confidence_interval(model2_accuracy)\n",
    "model3_accuracy_mean, b_model3_acc, u_model3_acc = mean_confidence_interval(model3_accuracy)\n",
    "model4_accuracy_mean, b_model4_acc, u_model4_acc = mean_confidence_interval(model4_accuracy)\n",
    "model5_accuracy_mean, b_model5_acc, u_model5_acc = mean_confidence_interval(model5_accuracy)\n",
    "model6_accuracy_mean, b_model6_acc, u_model6_acc = mean_confidence_interval(model6_accuracy)\n",
    "\n",
    "acc_dict = {\"model1_accuracy\": model1_accuracy, \"model2_accuracy\": model2_accuracy, \"model3_accuracy\": model3_accuracy,\n",
    "            \"model4_accuracy\": model4_accuracy, \"model5_accuracy\": model5_accuracy, \"model6_accuracy\": model6_accuracy}\n",
    "print(\"---------------- ACCURACY analysis ----------------\")\n",
    "for x, y in itertools.combinations(acc_dict.keys(), 2):\n",
    "    t, p = scipy.stats.ttest_ind(acc_dict[x], acc_dict[y])\n",
    "    print(\"t value accuracy between {} and {}: {}\".format(x, y, t))\n",
    "    if t > criticalTvalue or t < -criticalTvalue :\n",
    "        print(\"Null hypothesis rejected -> there is a significant difference in the two accuracy distributions\")\n",
    "    else:\n",
    "        print(\"Null hypothesis not rejected -> I assume that each difference in accuracy is due to the randomness\")\n",
    "print(\"-------------------------------------------------\")\n",
    "\n",
    "\n",
    "# PRECISION analysis\n",
    "model1_precision = pickle.load(open(MODEL_1_DIRECTORY + \"/test_precision.pkl\",  \"rb\" ))\n",
    "model2_precision = pickle.load(open(MODEL_2_DIRECTORY + \"/test_precision.pkl\",  \"rb\" ))\n",
    "model3_precision = pickle.load(open(MODEL_3_DIRECTORY + \"/test_precision.pkl\",  \"rb\" ))\n",
    "model4_precision = pickle.load(open(MODEL_4_DIRECTORY + \"/test_precision.pkl\",  \"rb\" ))\n",
    "model5_precision = pickle.load(open(MODEL_5_DIRECTORY + \"/test_precision.pkl\",  \"rb\" ))\n",
    "model6_precision = pickle.load(open(MODEL_6_DIRECTORY + \"/test_precision.pkl\",  \"rb\" ))\n",
    "\n",
    "model1_precision_mean, b_model1_p, u_model1_p = mean_confidence_interval(model1_precision)\n",
    "model2_precision_mean, b_model2_p, u_model2_p = mean_confidence_interval(model2_precision)\n",
    "model3_precision_mean, b_model3_p, u_model3_p = mean_confidence_interval(model3_precision)\n",
    "model4_precision_mean, b_model4_p, u_model4_p = mean_confidence_interval(model4_precision)\n",
    "model5_precision_mean, b_model5_p, u_model5_p = mean_confidence_interval(model5_precision)\n",
    "model6_precision_mean, b_model6_p, u_model6_p = mean_confidence_interval(model6_precision)\n",
    "\n",
    "prec_dict = {\"model1_precision\": model1_precision, \"model2_precision\": model2_precision, \"model3_precision\": model3_precision,\n",
    "            \"model4_precision\": model4_precision, \"model5_precision\": model5_precision, \"model6_precision\": model6_precision}\n",
    "print(\"---------------- PRECISION analysis ----------------\")\n",
    "for x, y in itertools.combinations(prec_dict.keys(), 2):\n",
    "    t, p = scipy.stats.ttest_ind(prec_dict[x], prec_dict[y])\n",
    "    print(\"t value precision between {} and {}: {}\".format(x, y, t))\n",
    "    if t > criticalTvalue or t < -criticalTvalue :\n",
    "        print(\"Null hypothesis rejected -> there is a significant difference in the two precision distributions\")\n",
    "    else:\n",
    "        print(\"Null hypothesis not rejected -> I assume that each difference in precision is due to the randomness\")\n",
    "print(\"-------------------------------------------------\")\n",
    "\n",
    "\n",
    "# RECALL analysis\n",
    "model1_recall = pickle.load(open(MODEL_1_DIRECTORY + \"/test_recall.pkl\",  \"rb\" ))\n",
    "model2_recall = pickle.load(open(MODEL_2_DIRECTORY + \"/test_recall.pkl\",  \"rb\" ))\n",
    "model3_recall = pickle.load(open(MODEL_3_DIRECTORY + \"/test_recall.pkl\",  \"rb\" ))\n",
    "model4_recall = pickle.load(open(MODEL_4_DIRECTORY + \"/test_recall.pkl\",  \"rb\" ))\n",
    "model5_recall = pickle.load(open(MODEL_5_DIRECTORY + \"/test_recall.pkl\",  \"rb\" ))\n",
    "model6_recall = pickle.load(open(MODEL_6_DIRECTORY + \"/test_recall.pkl\",  \"rb\" ))\n",
    "\n",
    "model1_recall_mean, b_model1_r, u_model1_r = mean_confidence_interval(model1_recall)\n",
    "model2_recall_mean, b_model2_r, u_model2_r = mean_confidence_interval(model2_recall)\n",
    "model3_recall_mean, b_model3_r, u_model3_r = mean_confidence_interval(model3_recall)\n",
    "model4_recall_mean, b_model4_r, u_model4_r = mean_confidence_interval(model4_recall)\n",
    "model5_recall_mean, b_model5_r, u_model5_r = mean_confidence_interval(model5_recall)\n",
    "model6_recall_mean, b_model6_r, u_model6_r = mean_confidence_interval(model6_recall)\n",
    "\n",
    "rec_dict = {\"model1_recall\": model1_recall, \"model2_recall\": model2_recall, \"model3_recall\": model3_recall,\n",
    "            \"model4_recall\": model4_recall, \"model5_recall\": model5_recall, \"model6_recall\": model6_recall}\n",
    "print(\"---------------- RECALL analysis ----------------\")\n",
    "for x, y in itertools.combinations(rec_dict.keys(), 2):\n",
    "    t, p = scipy.stats.ttest_ind(rec_dict[x], rec_dict[y])\n",
    "    print(\"t value recall between {} and {}: {}\".format(x, y, t))\n",
    "    if t > criticalTvalue or t < -criticalTvalue :\n",
    "        print(\"Null hypothesis rejected -> there is a significant difference in the two recall distributions\")\n",
    "    else:\n",
    "        print(\"Null hypothesis not rejected -> I assume that each difference in recall is due to the randomness\")\n",
    "print(\"-------------------------------------------------\")\n",
    "\n",
    "\n",
    "# Compute confidence intervals\n",
    "confidence_model1 = [u_model1_acc - b_model1_acc, u_model1_p - b_model1_p, u_model1_r - b_model1_r]\n",
    "confidence_model2 = [u_model2_acc - b_model2_acc, u_model2_p - b_model2_p, u_model2_r - b_model2_r]\n",
    "confidence_model3 = [u_model3_acc - b_model3_acc, u_model3_p - b_model3_p, u_model3_r - b_model3_r]\n",
    "confidence_model4 = [u_model4_acc - b_model4_acc, u_model4_p - b_model4_p, u_model4_r - b_model4_r]\n",
    "confidence_model5 = [u_model5_acc - b_model5_acc, u_model5_p - b_model5_p, u_model5_r - b_model5_r]\n",
    "confidence_model6 = [u_model6_acc - b_model6_acc, u_model6_p - b_model6_p, u_model6_r - b_model6_r]\n",
    "\n",
    "# Data to plot\n",
    "n_groups = 3\n",
    "means_model1 = (model1_accuracy_mean, model1_precision_mean, model1_recall_mean)\n",
    "means_model2 = (model2_accuracy_mean, model2_precision_mean, model2_recall_mean)\n",
    "means_model3 = (model3_accuracy_mean, model3_precision_mean, model3_recall_mean)\n",
    "means_model4 = (model4_accuracy_mean, model4_precision_mean, model4_recall_mean)\n",
    "means_model5 = (model5_accuracy_mean, model5_precision_mean, model5_recall_mean)\n",
    "means_model6 = (model6_accuracy_mean, model6_precision_mean, model6_recall_mean)\n",
    "\n",
    "# Create plot\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.1\n",
    "opacity = 0.8\n",
    "\n",
    "rects1 = plt.bar(index, means_model1, bar_width, yerr = confidence_model1, capsize = 5, alpha = opacity, color = 'g',\n",
    "                 #label = MODEL_1_DIRECTORY.split(sep='/')[0]\n",
    "                 label = \"Dataset VV+VH+WS\")\n",
    "\n",
    "rects2 = plt.bar(index + bar_width, means_model2, bar_width, yerr = confidence_model2, capsize = 5, alpha = opacity, color = 'r',\n",
    "                 #label = MODEL_2_DIRECTORY.split(sep='/')[0]\n",
    "                 label = \"Dataset VV+VH+VH\")\n",
    "\n",
    "rects3 = plt.bar(index + 2 * bar_width, means_model3, bar_width, yerr = confidence_model3, capsize = 5, alpha = opacity, color = 'b',\n",
    "                 #label = MODEL_3_DIRECTORY.split(sep='/')[0]\n",
    "                 label = \"Dataset WS+WS+WS\")\n",
    "\n",
    "rects4 = plt.bar(index + 3 * bar_width, means_model4, bar_width, yerr = confidence_model4, capsize = 5, alpha = opacity, color = 'y',\n",
    "                 #label = MODEL_4_DIRECTORY.split(sep='/')[0]\n",
    "                 label = \"Dataset WS+sWSO+cWSO\")\n",
    "\n",
    "rects5 = plt.bar(index + 4 * bar_width, means_model5, bar_width, yerr = confidence_model5, capsize = 5, alpha = opacity, color = 'c',\n",
    "                 #label = MODEL_5_DIRECTORY.split(sep='/')[0]\n",
    "                 label = \"Dataset VV+VV+VV\")\n",
    "\n",
    "rects6 = plt.bar(index + 5 * bar_width, means_model6, bar_width, yerr = confidence_model6, capsize = 5, alpha = opacity, color = 'm',\n",
    "                 #label = MODEL_6_DIRECTORY.split(sep='/')[0]\n",
    "                 label = \"Dataset VH+VH+VH\")\n",
    "\n",
    "font_size = 15\n",
    "font_size_ticks = 15\n",
    "plt.ylabel('Scores', fontsize = font_size)\n",
    "plt.title(MODEL_1_DIRECTORY.split(sep='/')[-2], fontsize = font_size)\n",
    "#plt.title(\"{} across datasets\".format(NETWORK), fontsize = font_size)\n",
    "plt.xticks(index + 1.5*bar_width, ('Accuracy', 'Precision', 'Recall'), fontsize = font_size_ticks)\n",
    "plt.yticks(fontsize = font_size_ticks)\n",
    "plt.legend(loc = 'lower left', prop={'size': font_size})\n",
    "plt.grid(True)\n",
    "#ax.set_ylim(ymin=0.75)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save plot\n",
    "save_plot = True\n",
    "if save_plot:\n",
    "    dir = \"model_comparisons_between_datasets/categorization/\"\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    fig.savefig(dir + \"{}_across_{}_datasets.jpg\".format(MODEL_1_DIRECTORY.split(sep='/')[-2], models), bbox_inches='tight')\n",
    "    #fig.savefig(dir + \"{}_across_{}_datasets.jpg\".format(NETWORK, models), bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "SJHkL9rpnuUJ",
    "K9695aBjozMD",
    "4EkR1bLbnfoT",
    "UtrFW51bpY4h",
    "U90HRPq1bC9p"
   ],
   "name": "TC_model_comparison.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
